<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://github.com/nervosys/AutonomySim/pages/autonomysim/reinforcement_learning/" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>Reinforcement Learning in AutonomySim - AutonomySim</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Reinforcement Learning in AutonomySim";
        var mkdocs_page_input_path = "pages\\autonomysim\\reinforcement_learning.md";
        var mkdocs_page_url = "/nervosys/AutonomySim/pages/autonomysim/reinforcement_learning/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> AutonomySim
        </a>
        <div class="version">
          master
        </div><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Home</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../../README.md">Home</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="" href="../../../CHANGELOG.md">Changelog</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Building AutonomySim</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../../use_precompiled.md">Download Binaries</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../build_windows.md">Build on Windows</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../build_linux.md">Build on Linux</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../build_macos.md">Build on macOS</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../docker_ubuntu.md">Docker on Linux</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../azure.md">AutonomySim on Azure</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../unreal_custenv.md">Custom Unreal Environment</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../Unity.md">AutonomySim with Unity</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../custom_unity_environments.md">Custom Unity Environment</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../unity_api_support.md">Unity APIs</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../build_faq.md">FAQ</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Using AutonomySim</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../../apis.md">Core APIs</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../image_apis.md">Image APIs</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../apis_cpp.md">C++ APIs</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../api_docs/html/index.html">API Reference Docs</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../dev_workflow.md">Development Workflow</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../settings.md">Settings</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../camera_views.md">Camera Views</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../using_car.md">Car Mode</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../remote_control.md">Remote Control</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../xbox_controller.md">XBox Controller</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../steering_wheel_installation.md">Steering Wheel</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../multi_vehicle.md">Multiple Vehicles</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">Sensors</a>
    <ul>
                <li class="toctree-l2"><a class="" href="../../../sensors.md">Sensors</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../lidar.md">LIDAR</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../distance_sensor.md">Distance Sensor</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../InfraredCamera.md">Infrared Camera</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../AutonomySim_ros_pkgs.md">ROS: AutonomySim ROS Wrapper</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../AutonomySim_tutorial_pkgs.md">ROS: AutonomySim Tutorial Packages</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../gazebo_drone.md">Import Gazebo models</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../retexturing.md">Domain Randomization</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../meshes.md">Mesh Vertex Buffers</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../playback.md">Playing Logs</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../voxel_grid.md">Voxel Grid Generator</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../event_sim.md">Event camera</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Design</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../../design.md">Architecture</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../code_structure.md">Code Structure</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../adding_new_apis.md">Adding new APIs</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../modify_recording_data.md">Modifying Recording Data</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../coding_guidelines.md">Coding Guidelines</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../robot_controller.md">Flight Controller</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../simple_flight.md">Simple Flight</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../hello_drone.md">Hello Drone</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">External Flight Controllers</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="#">MavLink and PX4</a>
    <ul>
                <li class="toctree-l2"><a class="" href="../../../px4_setup.md">PX4 Setup for AutonomySim</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../px4_sitl.md">PX4 in SITL</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../px4_sitl_wsl2.md">PX4 SITL with WSL 2</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../px4_lockstep.md">PX4 Lockstep</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../px4_multi_vehicle.md">PX4 Multi-vehicle in SITL</a>
                </li>
                <li class="toctree-l2"><a class="" href="https://youtu.be/1oY8Qu5maQQ">AutonomySim with Pixhawk</a>
                </li>
                <li class="toctree-l2"><a class="" href="https://youtu.be/HNWdYrtw3f0">PX4 Setup with AutonomySim</a>
                </li>
                <li class="toctree-l2"><a class="" href="https://www.youtube.com/watch?v=d_FyjKDWQfc&feature=youtu.be">Debugging Attitude Estimation</a>
                </li>
                <li class="toctree-l2"><a class="" href="https://github.com/nervosys/AutonomySim/wiki/Intercepting-MavLink-messages">Intercepting MavLink Messages</a>
                </li>
                <li class="toctree-l2"><a class="" href="https://github.com/nervosys/AutonomySim/wiki/Rapid-Descent-on-PX4-drones">Rapid Descent on PX4 Drones</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../px4_build.md">Building PX4</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../px4_logging.md">PX4/MavLink Logging</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../log_viewer.md">MavLink LogViewer</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../mavlinkcom.md">MavLinkCom</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../mavlinkcom_mocap.md">MavLink MoCap</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">ArduPilot</a>
    <ul>
                <li class="toctree-l2"><a class="" href="https://ardupilot.org/dev/docs/building-the-code.html">ArduPilot SITL Setup</a>
                </li>
                <li class="toctree-l2"><a class="" href="https://ardupilot.org/dev/docs/sitl-with-AirSim.html">AutonomySim & ArduPilot</a>
                </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Upgrading</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../../unreal_upgrade.md">Upgrading Unreal</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../upgrade_apis.md">Upgrading APIs</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../upgrade_settings.md">Upgrading Settings</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Contributed Tutorials</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../../reinforcement_learning.md">Reinforcement Learning</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://www.youtube.com/watch?v=y09VbdQWvQY">Using Environments from Marketplace</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://github.com/simondlevy/AirSimTensorFlow">Simple Collision Avoidance</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://aka.ms/AutonomousDrivingCookbook">Autonomous Driving on Azure</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://github.com/nervosys/AutonomySim/wiki/hexacopter">Building Hexacopter</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://github.com/nervosys/AutonomySim/wiki/moveOnPath-demo">Moving on Path Demo</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../point_clouds.md">Building Point Clouds</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../drone_survey.md">Surveying Using Drone</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../orbit.md">Orbit Trajectory</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://youtu.be/Bp86WiLUC80">Importing a custom multirotor mesh</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../object_detection.md">Object Detection</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://youtu.be/ZonkdMcwXH4">AutonomySim with MAVROS and PX4</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Misc</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../../custom_drone.md">AutonomySim on Real Drones</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../cmake_linux.md">Installing cmake on Linux</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../hard_drive.md">Tips for Busy HDD</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../pfm.md">pfm format</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../unreal_proj.md">Setting up Unreal Environment</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../unreal_blocks.md">Blocks Environment</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../who_is_using.md">Who is Using AutonomySim</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../working_with_plugin_contents.md">Working with UE Plugin Contents</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://github.com/nervosys/AutonomySim/wiki/technion">Formula Student Technion Self-drive</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Support</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../../faq.md">FAQ</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../SUPPORT.md">Support</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../create_issue.md">Create Issue</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../CONTRIBUTING.md">Contribute</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">AutonomySim</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Reinforcement Learning in AutonomySim</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/nervosys/AutonomySim/edit/master/docs/pages/autonomysim/reinforcement_learning.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="reinforcement-learning-in-autonomysim">Reinforcement Learning in AutonomySim<a class="headerlink" href="#reinforcement-learning-in-autonomysim" title="Permanent link">#</a></h1>
<p>We below describe how we can implement DQN in AutonomySim using an OpenAI gym wrapper around AutonomySim API, and using stable baselines implementations of standard RL algorithms. We recommend installing stable-baselines3 in order to run these examples (please see https://github.com/DLR-RM/stable-baselines3)</p>
<h4 id="disclaimer">Disclaimer<a class="headerlink" href="#disclaimer" title="Permanent link">#</a></h4>
<p>This is still in active development. What we share below is a framework that can be extended and tweaked to obtain better performance.</p>
<h4 id="gym-wrapper">Gym wrapper<a class="headerlink" href="#gym-wrapper" title="Permanent link">#</a></h4>
<p>In order to use AutonomySim as a gym environment, we extend and reimplement the base methods such as <code>step</code>, <code>_get_obs</code>, <code>_compute_reward</code> and <code>reset</code> specific to AutonomySim and the task of interest. The sample environments used in these examples for car and drone can be seen in <code>PythonClient/reinforcement_learning/*_env.py</code></p>
<h2 id="rl-with-car">RL with Car<a class="headerlink" href="#rl-with-car" title="Permanent link">#</a></h2>
<p><a href="https://github.com/nervosys/AutonomySim/tree/main/PythonClient/reinforcement_learning">Source code</a></p>
<p>This example works with AutonomySimNeighborhood environment available in <a href="https://github.com/nervosys/AutonomySim/releases">releases</a>.</p>
<p>First, we need to get the images from simulation and transform them appropriately. Below, we show how a depth image can be obtained from the ego camera and transformed to an 84X84 input to the network. (you can use other sensor modalities, and sensor inputs as well – of course you’ll have to modify the code accordingly).</p>
<div class="highlight"><pre><span></span><code><span class="n">responses</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">simGetImages</span><span class="p">([</span><span class="n">ImageRequest</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">AutonomySimImageType</span><span class="o">.</span><span class="n">DepthPerspective</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">)])</span>
<span class="n">current_state</span> <span class="o">=</span> <span class="n">transform_input</span><span class="p">(</span><span class="n">responses</span><span class="p">)</span>
</code></pre></div>
<p>We further define the six actions (brake, straight with throttle, full-left with throttle, full-right with throttle, half-left with throttle, half-right with throttle) that an agent can execute. This is done via the function <code>interpret_action</code>:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">interpret_action</span><span class="p">(</span><span class="n">action</span><span class="p">):</span>
    <span class="n">car_controls</span><span class="o">.</span><span class="n">brake</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">car_controls</span><span class="o">.</span><span class="n">throttle</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">car_controls</span><span class="o">.</span><span class="n">throttle</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">car_controls</span><span class="o">.</span><span class="n">brake</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">car_controls</span><span class="o">.</span><span class="n">steering</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">car_controls</span><span class="o">.</span><span class="n">steering</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">car_controls</span><span class="o">.</span><span class="n">steering</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">car_controls</span><span class="o">.</span><span class="n">steering</span> <span class="o">=</span> <span class="mf">0.25</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">car_controls</span><span class="o">.</span><span class="n">steering</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.25</span>
    <span class="k">return</span> <span class="n">car_controls</span>
</code></pre></div>
<p>We then define the reward function in <code>_compute_reward</code> as a convex combination of how fast the vehicle is travelling and how much it deviates from the center line. The agent gets a high reward when its moving fast and staying in the center of the lane.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">_compute_reward</span><span class="p">(</span><span class="n">car_state</span><span class="p">):</span>
    <span class="n">MAX_SPEED</span> <span class="o">=</span> <span class="mi">300</span>
    <span class="n">MIN_SPEED</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">thresh_dist</span> <span class="o">=</span> <span class="mf">3.5</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mi">3</span>

    <span class="n">z</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">pts</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">z</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">130</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">z</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">130</span><span class="p">,</span> <span class="mi">125</span><span class="p">,</span> <span class="n">z</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">125</span><span class="p">,</span> <span class="n">z</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">z</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">130</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">z</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">130</span><span class="p">,</span> <span class="o">-</span><span class="mi">128</span><span class="p">,</span> <span class="n">z</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">128</span><span class="p">,</span> <span class="n">z</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">z</span><span class="p">])]</span>
    <span class="n">pd</span> <span class="o">=</span> <span class="n">car_state</span><span class="o">.</span><span class="n">position</span>
    <span class="n">car_pt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>

    <span class="n">dist</span> <span class="o">=</span> <span class="mi">10000000</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">pts</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cross</span><span class="p">((</span><span class="n">car_pt</span> <span class="o">-</span> <span class="n">pts</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="p">(</span><span class="n">car_pt</span> <span class="o">-</span> <span class="n">pts</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">pts</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">pts</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>

    <span class="c1">#print(dist)</span>
    <span class="k">if</span> <span class="n">dist</span> <span class="o">&gt;</span> <span class="n">thresh_dist</span><span class="p">:</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">3</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reward_dist</span> <span class="o">=</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">beta</span><span class="o">*</span><span class="n">dist</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">reward_speed</span> <span class="o">=</span> <span class="p">(((</span><span class="n">car_state</span><span class="o">.</span><span class="n">speed</span> <span class="o">-</span> <span class="n">MIN_SPEED</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">MAX_SPEED</span> <span class="o">-</span> <span class="n">MIN_SPEED</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">reward_dist</span> <span class="o">+</span> <span class="n">reward_speed</span>

    <span class="k">return</span> <span class="n">reward</span>
</code></pre></div>
<p>The compute reward function also subsequently determines if the episode has terminated (e.g. due to collision). We look at the speed of the vehicle and if it is less than a threshold than the episode is considered to be terminated.</p>
<div class="highlight"><pre><span></span><code><span class="n">done</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">if</span> <span class="n">reward</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
    <span class="n">done</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">if</span> <span class="n">car_controls</span><span class="o">.</span><span class="n">brake</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">car_state</span><span class="o">.</span><span class="n">speed</span> <span class="o">&lt;=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="n">done</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">return</span> <span class="n">done</span>
</code></pre></div>
<p>The main loop then sequences through obtaining the image, computing the action to take according to the current policy, getting a reward and so forth. If the episode terminates then we reset the vehicle to the original state via <code>reset()</code>:</p>
<div class="highlight"><pre><span></span><code><span class="n">client</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">client</span><span class="o">.</span><span class="n">enableApiControl</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">client</span><span class="o">.</span><span class="n">armDisarm</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">car_control</span> <span class="o">=</span> <span class="n">interpret_action</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">Reset</span> <span class="n">position</span> <span class="ow">and</span> <span class="n">drive</span> <span class="n">straight</span> <span class="k">for</span> <span class="n">one</span> <span class="n">second</span>
<span class="n">client</span><span class="o">.</span><span class="n">setCarControls</span><span class="p">(</span><span class="n">car_control</span><span class="p">)</span>
<span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<p>Once the gym-styled environment wrapper is defined as in <code>car_env.py</code>, we then make use of stable-baselines3 to run a DQN training loop. The DQN training can be configured as follows, seen in <code>dqn_car.py</code>.</p>
<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span>
    <span class="s2">&quot;CnnPolicy&quot;</span><span class="p">,</span>
    <span class="n">env</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.00025</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">train_freq</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">target_update_interval</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">learning_starts</span><span class="o">=</span><span class="mi">200000</span><span class="p">,</span>
    <span class="n">buffer_size</span><span class="o">=</span><span class="mi">500000</span><span class="p">,</span>
    <span class="n">max_grad_norm</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">exploration_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">exploration_final_eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span>
    <span class="n">tensorboard_log</span><span class="o">=</span><span class="s2">&quot;./tb_logs/&quot;</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<p>A training environment and an evaluation envrionment (see <code>EvalCallback</code> in <code>dqn_car.py</code>) can be defined. The evaluation environoment can be different from training, with different termination conditions/scene configuration. A tensorboard log directory is also defined as part of the DQN parameters. Finally, <code>model.learn()</code> starts the DQN training loop. Similarly, implementations of PPO, A3C etc. can be used from stable-baselines3.</p>
<p>Note that the simulation needs to be up and running before you execute <code>dqn_car.py</code>. The video below shows first few episodes of DQN training.</p>
<p><a href="https://youtu.be/fv-oFPAqSZ4"><img alt="Reinforcement Learning - Car" src="images/dqn_car.png" /></a></p>
<h2 id="rl-with-quadrotor">RL with Quadrotor<a class="headerlink" href="#rl-with-quadrotor" title="Permanent link">#</a></h2>
<p><a href="https://github.com/nervosys/AutonomySim/tree/main/PythonClient/reinforcement_learning">Source code</a></p>
<p>This example works with AutonomySimMountainLandscape environment available in <a href="https://github.com/nervosys/AutonomySim/releases">releases</a>.</p>
<p>We can similarly apply RL for various autonomous flight scenarios with quadrotors. Below is an example on how RL could be used to train quadrotors to follow high tension power lines (e.g. application for energy infrastructure inspection). There are seven discrete actions here that correspond to different directions in which the quadrotor can move in (six directions + one hovering action).</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">interpret_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">quad_offset</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_length</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">quad_offset</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_length</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">quad_offset</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_length</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">quad_offset</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">step_length</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">quad_offset</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">step_length</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="n">quad_offset</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">step_length</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">quad_offset</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<p>The reward again is a function how how fast the quad travels in conjunction with how far it gets from the known powerlines.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_reward</span><span class="p">(</span><span class="n">quad_state</span><span class="p">,</span> <span class="n">quad_vel</span><span class="p">,</span> <span class="n">collision_info</span><span class="p">):</span>
    <span class="n">thresh_dist</span> <span class="o">=</span> <span class="mi">7</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">z</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10</span>
    <span class="n">pts</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.55265</span><span class="p">,</span> <span class="o">-</span><span class="mf">31.9786</span><span class="p">,</span> <span class="o">-</span><span class="mf">19.0225</span><span class="p">]),</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">48.59735</span><span class="p">,</span> <span class="o">-</span><span class="mf">63.3286</span><span class="p">,</span> <span class="o">-</span><span class="mf">60.07256</span><span class="p">]),</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">193.5974</span><span class="p">,</span> <span class="o">-</span><span class="mf">55.0786</span><span class="p">,</span> <span class="o">-</span><span class="mf">46.32256</span><span class="p">]),</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">369.2474</span><span class="p">,</span> <span class="mf">35.32137</span><span class="p">,</span> <span class="o">-</span><span class="mf">62.5725</span><span class="p">]),</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">541.3474</span><span class="p">,</span> <span class="mf">143.6714</span><span class="p">,</span> <span class="o">-</span><span class="mf">32.07256</span><span class="p">]),]</span>

    <span class="n">quad_pt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">x_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">y_val</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;position&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">z_val</span><span class="p">,)))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;collision&quot;</span><span class="p">]:</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="mi">10000000</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">pts</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cross</span><span class="p">((</span><span class="n">quad_pt</span> <span class="o">-</span> <span class="n">pts</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="p">(</span><span class="n">quad_pt</span> <span class="o">-</span> <span class="n">pts</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">pts</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">pts</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>

        <span class="k">if</span> <span class="n">dist</span> <span class="o">&gt;</span> <span class="n">thresh_dist</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reward_dist</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">beta</span> <span class="o">*</span> <span class="n">dist</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>
            <span class="n">reward_speed</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;velocity&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">x_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;velocity&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">y_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;velocity&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">z_val</span><span class="p">,])</span><span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">reward_dist</span> <span class="o">+</span> <span class="n">reward_speed</span>
</code></pre></div>
<p>We consider an episode to terminate if it drifts too much away from the known power line coordinates, and then reset the drone to its starting point.</p>
<p>Once the gym-styled environment wrapper is defined as in <code>drone_env.py</code>, we then make use of stable-baselines3 to run a DQN training loop. The DQN training can be configured as follows, seen in <code>dqn_drone.py</code>.</p>
<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span>
    <span class="s2">&quot;CnnPolicy&quot;</span><span class="p">,</span>
    <span class="n">env</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.00025</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">train_freq</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">target_update_interval</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">learning_starts</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">buffer_size</span><span class="o">=</span><span class="mi">500000</span><span class="p">,</span>
    <span class="n">max_grad_norm</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">exploration_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">exploration_final_eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span>
    <span class="n">tensorboard_log</span><span class="o">=</span><span class="s2">&quot;./tb_logs/&quot;</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<p>A training environment and an evaluation envrionment (see <code>EvalCallback</code> in <code>dqn_drone.py</code>) can be defined. The evaluation environoment can be different from training, with different termination conditions/scene configuration. A tensorboard log directory is also defined as part of the DQN parameters. Finally, <code>model.learn()</code> starts the DQN training loop. Similarly, implementations of PPO, A3C etc. can be used from stable-baselines3.</p>
<p>Here is the video of first few episodes during the training.</p>
<p><a href="https://youtu.be/uKm15Y3M1Nk"><img alt="Reinforcement Learning - Quadrotor" src="images/dqn_quadcopter.png" /></a></p>
<h2 id="related">Related<a class="headerlink" href="#related" title="Permanent link">#</a></h2>
<p>Please also see <a href="https://aka.ms/AutonomousDrivingCookbook">The Autonomous Driving Cookbook</a> by Microsoft Deep Learning and Robotics Garage Chapter.</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
      <p>Copyright &copy; 2024 Nervosys, LLC</p>
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/nervosys/AutonomySim" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
