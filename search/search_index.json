{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"The simulation engine for autonomous systems"},{"location":"index.html#welcome","title":"Welcome","text":"<p>Welcome to the official documentation for <code>AutonomySim</code>, the simulation engine for autonomous systems.</p> <p>Please see the navigation sidebar to the left for more information.</p>"},{"location":"index.html#repository","title":"Repository","text":"<p>AutonomySim on GitHub</p> <p> \"Accelerating the development of robotic general intelligence\"    TM 2024 \u00a9 Nervosys, LLC </p>"},{"location":"CHANGELOG.html","title":"Change Log","text":"<p>Below is summarized list of important changes. This does not include minor/less important changes or bug fixes or documentation update. This list updated every few months. For complete detailed changes, please review the project commit history.</p>"},{"location":"CHANGELOG.html#february-2024","title":"February 2024","text":"<ul> <li>AutonomySim relaunched as clean-slate repository.</li> <li>Major documentation update.</li> <li>Trimmed the fat to go light and fast.</li> </ul>"},{"location":"CHANGELOG.html#october-2023","title":"October 2023","text":"<ul> <li>AutonomySim fork created.</li> <li>Migration from Batch/Command to PowerShell.</li> <li>Support dropped for Unity, Gazebo, and ROS1 to better focus on Unreal Engine 5.</li> <li>Major project reorganization begun.</li> </ul>"},{"location":"CHANGELOG.html#jan-2022","title":"Jan 2022","text":"<ul> <li>Latest release <code>v1.7.0</code> for Windows and Linux</li> </ul>"},{"location":"CHANGELOG.html#dec-2021","title":"Dec 2021","text":"<ul> <li>Cinematographic Camera</li> <li>ROS2 wrapper</li> <li>API to list all assets</li> <li>movetoGPS API</li> </ul>"},{"location":"CHANGELOG.html#nov-2021","title":"Nov 2021","text":"<ul> <li>Optical flow camera</li> <li>simSetKinematics API</li> <li>Dynamically set object textures from existing UE material or texture PNG</li> <li>Ability to spawn/destroy lights and control light parameters</li> </ul>"},{"location":"CHANGELOG.html#sep-2021","title":"Sep 2021","text":"<ul> <li>Support for multiple drones in Unity</li> </ul>"},{"location":"CHANGELOG.html#aug-2021","title":"Aug 2021","text":"<ul> <li>Control manual camera speed through the keyboard </li> <li>Latest release <code>v1.6.0</code> for Windows and Linux</li> <li>Fix: DepthPlanar capture</li> <li>Fix: compression bug in segmentation palette</li> </ul>"},{"location":"CHANGELOG.html#jul-2021","title":"Jul 2021","text":"<ul> <li>Fixed external cameras</li> <li>Fix: ROS topic names</li> <li>Fix: Weather API crash</li> </ul>"},{"location":"CHANGELOG.html#jun-2021","title":"Jun 2021","text":"<ul> <li>Object detection API</li> <li>GazeboDrone project added to connect a gazebo drone to the AutonomySim drone</li> <li>Control manual camera speed through the keyboard</li> <li>Octo X config</li> <li>API for list of vehicle names</li> <li>Fix: issue where no new scene is rendered after simContinueForTime</li> <li>Fix:Check for settings.json in current directory as well</li> </ul>"},{"location":"CHANGELOG.html#may-2021","title":"May 2021","text":"<ul> <li>Make falling leaves visible in depth and segmentation</li> <li>Fix: Unity Car API</li> <li>Latest release <code>v1.5.0</code> for Windows and Linux</li> <li>fix px4 connection for wsl 2.</li> </ul>"},{"location":"CHANGELOG.html#apr-2021","title":"Apr 2021","text":"<ul> <li>External physics engine</li> <li>ArduPilot Sensor Updates</li> <li>Add new build configuration \"--RelWithDebInfo\" which makes it easier to debug</li> <li>Add ApiServerPort to available AutonomySim settings</li> <li>ROS: Use the same settings as AutonomySim</li> </ul>"},{"location":"CHANGELOG.html#mar-2021","title":"Mar 2021","text":"<ul> <li>Add moveByVelocityZBodyFrame</li> <li>Spawn vehicles via RPC</li> <li>Unity weather parameters, weather HUD, and a visual effect for snow</li> <li>Rotor output API</li> <li>Extend Recording to multiple vehicles</li> <li>Combine Lidar Segmentation API into getLidarData</li> </ul>"},{"location":"CHANGELOG.html#feb-2021","title":"Feb 2021","text":"<ul> <li>Add Ubuntu 20.04 to Actions CI</li> <li>add tcp server support to MavLinkTest</li> </ul>"},{"location":"CHANGELOG.html#jan-2021","title":"Jan 2021","text":"<ul> <li>Added continueForFrames</li> <li>Latest release <code>v1.4.0</code> for Windows and Linux</li> </ul>"},{"location":"CHANGELOG.html#dec-2020","title":"Dec 2020","text":"<ul> <li>Add Actions script to build and deploy to gh-pages</li> <li>Gym environments and stable-baselines integration for RL</li> <li>Programmable camera distortion</li> <li>Voxel grid construction</li> <li>Event camera simulation</li> <li>Add Github Actions CI Checks</li> <li>Added moveByVelocityBodyFrame</li> </ul>"},{"location":"CHANGELOG.html#nov-2020","title":"Nov 2020","text":"<ul> <li>fix auto-detect of pixhawk 4 hardware</li> </ul>"},{"location":"CHANGELOG.html#oct-2020","title":"Oct 2020","text":"<ul> <li>[Travis] Add Ubuntu 20.04, OSX XCode 11.5 jobs</li> </ul>"},{"location":"CHANGELOG.html#sep-2020","title":"Sep 2020","text":"<ul> <li>Add Vehicle option for Subwindow settings</li> <li>Disable cameras after fetching images, projection matrix</li> <li>Add Wind simulation</li> <li>New <code>simRunConsoleCommand</code> API</li> <li>UE4: Fixes and improvements to World APIs</li> <li>UE4: Fix random crash with Plotting APIs</li> <li>Add backwards-compatibility layer for <code>simSetCameraPose</code></li> <li>Disable LogMessages if set to false</li> <li>ROS: Removed double inclusion of <code>static_transforms.launch</code></li> <li>Add retry loop when connecting PX4 SITL control channel</li> <li>Allow for enabling physics when spawning a new object</li> </ul>"},{"location":"CHANGELOG.html#july-2020","title":"July 2020","text":"<ul> <li>Add Python APIs for new Object functions</li> <li>UE4: Fix Broken List Level Option</li> <li>Linux build improvements</li> <li>Allow passing the settings.json file location via <code>--settings</code> argument</li> <li>Distance Sensor Upgrades and fixes</li> <li>Update to min CMake version required for VS 2019</li> <li>Fix: Non-linear bias corrupts SurfaceNormals, Segmentation image</li> <li>Fix: <code>simGetSegmentationObjectID</code> will always return -1</li> <li>Initial implementation of simLoadLevel, simGet/SetObjectScale, simSpawn|DestroyObject APIs</li> <li>Upgrade <code>setCameraOrientation</code> API to <code>setCameraPose</code></li> <li>ROS: All sensors and car support</li> <li>Get rid of potential div-0 errors so we can set dt = 0 for pausing</li> <li>ROS: Add mavros_msgs to build dependencies</li> <li>Move Wiki pages to docs</li> <li>Add Recording APIs</li> <li>Update Dockerfiles and documentation to Ubuntu 18.04</li> <li>Azure development environment and documentation</li> <li>ROS: Add autonomysim_node to install list</li> </ul>"},{"location":"CHANGELOG.html#may-2020","title":"May 2020","text":"<ul> <li>Fix more issues with PX4 master</li> <li>Reduce warnings level in Unity build</li> <li>Support for Unreal Engine 4.25</li> <li>Unity crash fix, upgrade to 2019.3.12, Linux build improvements</li> </ul>"},{"location":"CHANGELOG.html#april-2020","title":"April 2020","text":"<ul> <li>Fix issues with PX4 latest master branch</li> <li>Fix Lidar DrawDebugPoints causing crash</li> <li>Add docstrings for Python API</li> <li>Add missing noise, weather texture materials</li> <li>Update AutonomySim.uplugin version to 1.3.1</li> <li>Camera Roll angle control using Q,E keys in CV mode, manual camera</li> <li>Remove broken GCC build</li> <li>New API - <code>simSetTraceLine()</code></li> <li>ROS package compilation fixes and updates</li> <li>Latest release <code>v1.3.1</code> for Windows and Linux</li> <li>APIs added and fixed - <code>simSetCameraFov</code>, <code>rotateToYaw</code></li> <li>AutonomySim Python package update to <code>1.2.8</code></li> <li>NoDisplay ViewMode render state fix</li> </ul>"},{"location":"CHANGELOG.html#march-2020","title":"March 2020","text":"<ul> <li>Latest release <code>v1.3.0</code> for Windows and Linux</li> <li>Upgraded to Unreal Engine 4.24, Visual Studio 2019, Clang 8, C++ 17 standard</li> <li>Mac OSX Catalina support</li> <li>Updated AutonomySim Python package, with lots of new APIs</li> <li>Removed legacy API wrappers</li> <li>Support for latest PX4 stable release</li> <li>Support for ArduPilot - Copter, Rover vehicles</li> <li>Updated Unity support</li> <li>Removed simChar* APIs</li> <li>Plotting APIs for Debugging</li> <li>Low-level Multirotor APIs</li> <li>Updated Eigen version to 3.3.7</li> <li>Distance Sensor API fix</li> <li>Add <code>simSwapTextures</code> API</li> <li>Fix <code>simContinueForTime</code>, <code>simPause</code> APIs</li> <li>Lidar Sensor Trace Casting fix</li> <li>Fix rare <code>reset()</code> bug which causes Unreal crash</li> <li>Lidar sensor improvements, add <code>simGetLidarSegmentation</code> API</li> <li>Add RpcLibPort in settings</li> <li>Recording thread deadlock fix</li> <li>Prevent environment crash when Sun is not present</li> <li>Africa Tracking feautre, add <code>simListSceneObjects()</code> API, fix camera projection matrix</li> <li>ROS wrapper for multirotors is available. See ros_pkgs for the ROS API, and ros_pkgs_tutorial for tutorials.</li> <li>Added sensor APIs for Barometer, IMU, GPS, Magnetometer, Distance Sensor</li> <li>Added support for docker in ubuntu</li> </ul>"},{"location":"CHANGELOG.html#november-2018","title":"November, 2018","text":"<ul> <li>Added Weather Effects and APIs</li> <li>Added Time of Day API</li> <li>An experimental integration of AutonomySim on Unity is now available. Learn more in Unity blog post. </li> <li>New environments: Forest, Plains (windmill farm), TalkingHeads (human head simulation), TrapCam (animal detection via camera)</li> <li>Highly efficient NoDisplay view mode to turn off main screen rendering so you can capture images at high rate</li> <li>Enable/disable sensors via settings</li> <li>Lidar Sensor</li> <li>Support for Flysky FS-SM100 RC USB adapter</li> <li>Case Study: Formula Student Technion Driverless</li> <li>Multi-Vehicle Capability</li> <li>Custom speed units</li> <li>ROS publisher</li> <li>simSetObjectPose API</li> <li>Character Control APIs (works on TalkingHeads binaries in release)</li> <li>Arducopter Solo Support</li> <li>Linux install without sudo access</li> <li>Kinect like ROS publisher</li> </ul>"},{"location":"CHANGELOG.html#june-2018","title":"June, 2018","text":"<ul> <li>Development workflow doc</li> <li>Better Python 2 compatibility</li> <li>OSX setup fixes</li> <li>Almost complete rewrite of our APIs with new threading model, merging old APIs and creating few newer ones</li> </ul>"},{"location":"CHANGELOG.html#april-2018","title":"April, 2018","text":"<ul> <li>Upgraded to Unreal Engine 4.18 and Visual Studio 2017</li> <li>API framework refactoring to support world-level APIs</li> <li>Latest PX4 firmware now supported</li> <li>CarState with more information</li> <li>ThrustMaster wheel support</li> <li>pause and continueForTime APIs for drone as well as car</li> <li>Allow drone simulation run at higher clock rate without any degradation</li> <li>Forward-only mode fully functional for drone (do orbits while looking at center)</li> <li>Better PID tuning to reduce wobble for drones</li> <li>Ability to set arbitrary vehicle blueprint for drone as well as car</li> <li>gimbal stabilization via settings</li> <li>Ability to segment skinned and skeletal meshes by their name</li> <li>moveByAngleThrottle API</li> <li>Car physics tuning for better maneuverability</li> <li>Configure additional cameras via settings</li> <li>Time of day with geographically computed sun position</li> <li>Better car steering via keyboard</li> <li>Added MeshNamingMethod in segmentation setting </li> <li>gimbal API</li> <li>getCameraParameters API</li> <li>Ability turn off main rendering to save GPU resources</li> <li>Projection mode for capture settings</li> <li>getRCData, setRCData APIs</li> <li>Ability to turn off segmentation using negative IDs</li> <li>OSX build improvements</li> <li>Segmentation working for very large environments with initial IDs</li> <li>Better and extensible hash calculation for segmentation IDs</li> <li>Extensible PID controller for custom integration methods</li> <li>Sensor architecture now enables renderer specific features like ray casting</li> <li>Laser altimeter sensor</li> </ul>"},{"location":"CHANGELOG.html#jan-2018","title":"Jan 2018","text":"<ul> <li>Config system rewrite, enable flexible config we are targeting in future</li> <li>Multi-Vehicle support Phase 1, core infrastructure changes</li> <li>MacOS support</li> <li>Infrared view</li> <li>5 types of noise and interference for cameras</li> <li>WYSIWIG capture settings for cameras, preview recording settings in main view</li> <li>Azure support Phase 1, enable configurability of instances for headless mode</li> <li>Full kinematics APIs, ability to get pose, linear and angular velocities + accelerations via APIs</li> <li>Record multiple images from multiple cameras</li> <li>New segmentation APIs, ability to set configure object IDs, search via regex</li> <li>New object pose APIs, ability to get pose of objects (like animals) in environment</li> <li>Camera infrastructure enhancements, ability to add new image types like IR with just few lines</li> <li>Clock speed APIs for drone as well as car, simulation can be run with speed factor of 0 &lt; x &lt; infinity</li> <li>Support for Logitech G920 wheel</li> <li>Physics tuning of the car, Car doesn\u2019t roll over, responds to steering with better curve, releasing gas paddle behavior more realistic</li> <li>Debugging APIs</li> <li>Stress tested to 24+ hours of continuous runs</li> <li>Support for Landscape and sky segmentation</li> <li>Manual navigation with accelerated controls in CV mode, user can explore environment much more easily</li> <li>Collison APIs</li> <li>Recording enhancements, log several new data points including ground truth, multiple images, controls state</li> <li>Planner and Perspective Depth views</li> <li>Disparity view</li> <li>New Image APIs supports float, png or numpy formats</li> <li>6 config settings for image capture, ability to set auto-exposure, motion blur, gamma etc</li> <li>Full multi-camera support through out including sub-windows, recording, APIs etc</li> <li>Command line script to build all environments in one shot</li> <li>Remove submodules, use rpclib as download</li> </ul>"},{"location":"CHANGELOG.html#nov-2017","title":"Nov 2017","text":"<ul> <li>We now have the car model.</li> <li>No need to build the code. Just download binaries and you are good to go!</li> <li>The reinforcement learning example with AutonomySim</li> <li>New built-in flight controller called simple_flight that \"just works\" without any additional setup. It is also now default. </li> <li>AutonomySim now also generates depth as well as disparity images that are in camera plane. </li> <li>We also have official Linux build now!</li> </ul>"},{"location":"CHANGELOG.html#sep-2017","title":"Sep 2017","text":"<ul> <li>We have added car model!</li> </ul>"},{"location":"CHANGELOG.html#aug-2017","title":"Aug 2017","text":"<ul> <li>simple_flight is now default flight controller for drones. If you want to use PX4, you will need to modify settings.json as per PX4 setup doc.</li> <li>Linux build is official and currently uses Unreal 4.17 due to various bug fixes required</li> <li>ImageType enum has breaking changes with several new additions and clarifying existing ones</li> <li>SubWindows are now configurable from settings.json</li> <li>PythonClient is now complete and has parity with C++ APIs. Some of these would have breaking changes.</li> </ul>"},{"location":"CHANGELOG.html#feb-2017","title":"Feb 2017","text":"<ul> <li>First release!</li> </ul>"},{"location":"CONTRIBUTING.html","title":"Contributing","text":""},{"location":"CONTRIBUTING.html#quick-start","title":"Quick Start","text":"<ul> <li>Please read our short and sweet coding guidelines.</li> <li>For big changes such as adding new feature or refactoring, file an issue first.</li> <li>Use our recommended development workflow to make changes and test it.</li> <li>Use usual steps to make contributions just like other GitHub projects. If you are not familiar with Git Branch-Rebase-Merge workflow, please read this first.</li> </ul>"},{"location":"CONTRIBUTING.html#checklist","title":"Checklist","text":"<ul> <li>Use same style and formatting as rest of code even if it's not your preferred one.</li> <li>Change any documentation that goes with code changes.</li> <li>Do not include OS specific header files or new 3rd party dependencies.</li> <li>Keep your pull request small, ideally under 10 files.</li> <li>Make sure you don't include large binary files.</li> <li>When adding new includes, make dependency is absolutely necessary.</li> <li>Rebase your branch frequently with main (once every 2-3 days is ideal).</li> <li>Make sure your code would compile on Windows, Linux and OSX.</li> </ul>"},{"location":"SUPPORT.html","title":"Support","text":"<p>We highly recommend that user look at source code and contribute to the project. Due to the large number of incoming feature requests, we may not be able to get to your request in your desired timeframe. So, please contribute:</p> <ul> <li>File GitHub Issues</li> <li>Join the Discussions </li> <li>Join the Discord</li> </ul>"},{"location":"apis.html","title":"APIs","text":""},{"location":"apis.html#introduction","title":"Introduction","text":"<p><code>AutonomySim</code> exposes application programming interfaces (APIs) that enable you to interact with vehicle in the simulation programmatically. You can use these APIs to retrieve images, get state, control the vehicle, and so on.</p>"},{"location":"apis.html#python-quickstart","title":"Python Quickstart","text":"<p>If you want to use Python to call AutonomySim APIs, we recommend using Anaconda with Python 3.5 or later versions however some code may also work with Python 2.7 (help us improve compatibility!).</p> <p>First install this package:</p> <pre><code>pip install msgpack-rpc-python\n</code></pre> <p>You can either get AutonomySim binaries from releases or compile from the source (Windows, Linux). Once you can run AutonomySim, choose Car as vehicle and then navigate to <code>PythonClient\\car\\</code> folder and run:</p> <pre><code>python hello_car.py\n</code></pre> <p>If you are using Visual Studio 2019 then just open AutonomySim.sln, set PythonClient as startup project and choose <code>car\\hello_car.py</code> as your startup script.</p>"},{"location":"apis.html#installing-autonomysim-package","title":"Installing AutonomySim Package","text":"<p>You can also install <code>AutonomySim</code> package simply by,</p> <pre><code>pip install AutonomySim\n</code></pre> <p>You can find source code and samples for this package in <code>PythonClient</code> folder in your repo.</p> <p>Notes 1. You may notice a file <code>setup_path.py</code> in our example folders. This file has simple code to detect if <code>AutonomySim</code> package is available in parent folder and in that case we use that instead of pip installed package so you always use latest code. 2. AutonomySim is still under heavy development which means you might frequently need to update the package to use new APIs.</p>"},{"location":"apis.html#c-users","title":"C++ Users","text":"<p>If you want to use C++ APIs and examples, please see C++ APIs Guide.</p>"},{"location":"apis.html#hello-car","title":"Hello Car","text":"<p>Here's how to use AutonomySim APIs using Python to control simulated car (see also C++ example):</p> <pre><code># ready to run example: PythonClient/car/hello_car.py\nimport AutonomySim\nimport time\n\n# connect to the AutonomySim simulator\nclient = AutonomySim.CarClient()\nclient.confirmConnection()\nclient.enableApiControl(True)\ncar_controls = AutonomySim.CarControls()\n\nwhile True:\n    # get state of the car\n    car_state = client.getCarState()\n    print(\"Speed %d, Gear %d\" % (car_state.speed, car_state.gear))\n\n    # set the controls for car\n    car_controls.throttle = 1\n    car_controls.steering = 1\n    client.setCarControls(car_controls)\n\n    # let car drive a bit\n    time.sleep(1)\n\n    # get camera images from the car\n    responses = client.simGetImages([\n        AutonomySim.ImageRequest(0, AutonomySim.ImageType.DepthVis),\n        AutonomySim.ImageRequest(1, AutonomySim.ImageType.DepthPlanar, True)])\n    print('Retrieved images: %d', len(responses))\n\n    # do something with images\n    for response in responses:\n        if response.pixels_as_float:\n            print(\"Type %d, size %d\" % (response.image_type, len(response.image_data_float)))\n            AutonomySim.write_pfm('py1.pfm', AutonomySim.get_pfm_array(response))\n        else:\n            print(\"Type %d, size %d\" % (response.image_type, len(response.image_data_uint8)))\n            AutonomySim.write_file('py1.png', response.image_data_uint8)\n</code></pre>"},{"location":"apis.html#hello-drone","title":"Hello Drone","text":"<p>Here's how to use AutonomySim APIs using Python to control simulated quadrotor (see also C++ example):</p> <pre><code># ready to run example: PythonClient/multirotor/hello_drone.py\nimport AutonomySim\nimport os\n\n# connect to the AutonomySim simulator\nclient = AutonomySim.MultirotorClient()\nclient.confirmConnection()\nclient.enableApiControl(True)\nclient.armDisarm(True)\n\n# Async methods returns Future. Call join() to wait for task to complete.\nclient.takeoffAsync().join()\nclient.moveToPositionAsync(-10, 10, -10, 5).join()\n\n# take images\nresponses = client.simGetImages([\n    AutonomySim.ImageRequest(\"0\", AutonomySim.ImageType.DepthVis),\n    AutonomySim.ImageRequest(\"1\", AutonomySim.ImageType.DepthPlanar, True)])\nprint('Retrieved images: %d', len(responses))\n\n# do something with the images\nfor response in responses:\n    if response.pixels_as_float:\n        print(\"Type %d, size %d\" % (response.image_type, len(response.image_data_float)))\n        AutonomySim.write_pfm(os.path.normpath('/temp/py1.pfm'), AutonomySim.get_pfm_array(response))\n    else:\n        print(\"Type %d, size %d\" % (response.image_type, len(response.image_data_uint8)))\n        AutonomySim.write_file(os.path.normpath('/temp/py1.png'), response.image_data_uint8)\n</code></pre>"},{"location":"apis.html#common-apis","title":"Common APIs","text":"<ul> <li><code>reset</code>: This resets the vehicle to its original starting state. Note that you must call <code>enableApiControl</code> and <code>armDisarm</code> again after the call to <code>reset</code>.</li> <li><code>confirmConnection</code>: Checks state of connection every 1 sec and reports it in Console so user can see the progress for connection.</li> <li><code>enableApiControl</code>: For safety reasons, by default API control for autonomous vehicle is not enabled and human operator has full control (usually via RC or joystick in simulator). The client must make this call to request control via API. It is likely that human operator of vehicle might have disallowed API control which would mean that enableApiControl has no effect. This can be checked by <code>isApiControlEnabled</code>.</li> <li><code>isApiControlEnabled</code>: Returns true if API control is established. If false (which is default) then API calls would be ignored. After a successful call to <code>enableApiControl</code>, the <code>isApiControlEnabled</code> should return true.</li> <li><code>ping</code>: If connection is established then this call will return true otherwise it will be blocked until timeout.</li> <li><code>simPrintLogMessage</code>: Prints the specified message in the simulator's window. If message_param is also supplied then its printed next to the message and in that case if this API is called with same message value but different message_param again then previous line is overwritten with new line (instead of API creating new line on display). For example, <code>simPrintLogMessage(\"Iteration: \", to_string(i))</code> keeps updating same line on display when API is called with different values of i. The valid values of severity parameter is 0 to 3 inclusive that corresponds to different colors.</li> <li><code>simGetObjectPose</code>, <code>simSetObjectPose</code>: Gets and sets the pose of specified object in Unreal environment. Here the object means \"actor\" in Unreal terminology. They are searched by tag as well as name. Please note that the names shown in UE Editor are auto-generated in each run and are not permanent. So if you want to refer to actor by name, you must change its auto-generated name in UE Editor. Alternatively you can add a tag to actor which can be done by clicking on that actor in Unreal Editor and then going to Tags property, click \"+\" sign and add some string value. If multiple actors have same tag then the first match is returned. If no matches are found then NaN pose is returned. The returned pose is in NED coordinates in SI units in the world frame. For <code>simSetObjectPose</code>, the specified actor must have Mobility set to Movable or otherwise you will get undefined behavior. The <code>simSetObjectPose</code> has parameter <code>teleport</code> which means object is moved through other objects in its way and it returns true if move was successful</li> </ul>"},{"location":"apis.html#imagecomputer-vision-apis","title":"Image/Computer Vision APIs","text":"<p>AutonomySim offers comprehensive images APIs to retrieve synchronized images from multiple cameras along with ground truth including depth, disparity, surface normals and vision. You can set the resolution, FOV, motion blur etc parameters in settings.json. There is also API for detecting collision state. See also complete code that generates specified number of stereo images and ground truth depth with normalization to camera plane, computation of disparity image and saving it to pfm format.</p> <p>More on image APIs and Computer Vision mode. For vision problems that can benefit from domain randomization, there is also an object texture_swapping API, which can be used in supported scenes.</p>"},{"location":"apis.html#pause-and-continue-apis","title":"Pause and Continue APIs","text":"<p>AutonomySim allows to pause and continue the simulation through <code>pause(is_paused)</code> API. To pause the simulation call <code>pause(True)</code> and to continue the simulation call <code>pause(False)</code>. You may have scenario, especially while using reinforcement learning, to run the simulation for specified amount of time and then automatically pause. While simulation is paused, you may then do some expensive computation, send a new command and then again run the simulation for specified amount of time. This can be achieved by API <code>continueForTime(seconds)</code>. This API runs the simulation for the specified number of seconds and then pauses the simulation. For example usage, please see pause_continue_car.py and pause_continue_drone.py.</p>"},{"location":"apis.html#collision-api","title":"Collision API","text":"<p>The collision information can be obtained using <code>simGetCollisionInfo</code> API. This call returns a struct that has information not only whether collision occurred but also collision position, surface normal, penetration depth and so on.</p>"},{"location":"apis.html#time-of-day-api","title":"Time-of-day API","text":"<p>AutonomySim assumes there exist sky sphere of class <code>EngineSky/BP_Sky_Sphere</code> in your environment with ADirectionalLight actor. By default, the position of the sun in the scene doesn't move with time. You can use settings to set up latitude, longitude, date and time which AutonomySim uses to compute the position of sun in the scene.</p> <p>You can also use following API call to set the sun position according to given date time:</p> <pre><code>simSetTimeOfDay(self, is_enabled, start_datetime = \"\", is_start_datetime_dst = False, celestial_clock_speed = 1, update_interval_secs = 60, move_sun = True)\n</code></pre> <p>The <code>is_enabled</code> parameter must be <code>True</code> to enable time of day effect. If it is <code>False</code> then sun position is reset to its original in the environment.</p> <p>Other parameters are same as in settings.</p>"},{"location":"apis.html#line-of-sight-and-world-extent-apis","title":"Line-of-sight and world extent APIs","text":"<p>To test line-of-sight in the sim from a vehicle to a point or between two points, see simTestLineOfSightToPoint(point, vehicle_name) and simTestLineOfSightBetweenPoints(point1, point2), respectively. Sim world extent, in the form of a vector of two GeoPoints, can be retrieved using simGetWorldExtents().</p>"},{"location":"apis.html#weather-apis","title":"Weather APIs","text":"<p>By default all weather effects are disabled. To enable weather effect, first call:</p> <pre><code>simEnableWeather(True)\n</code></pre> <p>Various weather effects can be enabled by using <code>simSetWeatherParameter</code> method which takes <code>WeatherParameter</code>, for example,</p> <p><pre><code>client.simSetWeatherParameter(AutonomySim.WeatherParameter.Rain, 0.25);\n</code></pre> The second parameter value is from 0 to 1. The first parameter provides following options:</p> <pre><code>class WeatherParameter:\n    Rain = 0\n    Roadwetness = 1\n    Snow = 2\n    RoadSnow = 3\n    MapleLeaf = 4\n    RoadLeaf = 5\n    Dust = 6\n    Fog = 7\n</code></pre> <p>Please note that <code>Roadwetness</code>, <code>RoadSnow</code> and <code>RoadLeaf</code> effects requires adding materials to your scene.</p> <p>Please see example code for more details.</p>"},{"location":"apis.html#recording-apis","title":"Recording APIs","text":"<p>Recording APIs can be used to start recording data through APIs. Data to be recorded can be specified using settings. To start recording, use -</p> <pre><code>client.startRecording()\n</code></pre> <p>Similarly, to stop recording, use <code>client.stopRecording()</code>. To check whether Recording is running, call <code>client.isRecording()</code>, returns a <code>bool</code>.</p> <p>This API works alongwith toggling Recording using R button, therefore if it's enabled using R key, <code>isRecording()</code> will return <code>True</code>, and recording can be stopped via API using <code>stopRecording()</code>. Similarly, recording started using API will be stopped if R key is pressed in Viewport. LogMessage will also appear in the top-left of the viewport if recording is started or stopped using API.</p> <p>Note that this will only save the data as specfied in the settings. For full freedom in storing data such as certain sensor information, or in a different format or layout, use the other APIs to fetch the data and save as desired. Check out Modifying Recording Data for details on how to modify the kinematics data being recorded.</p>"},{"location":"apis.html#wind-api","title":"Wind API","text":"<p>Wind can be changed during simulation using <code>simSetWind()</code>. Wind is specified in World frame, NED direction and m/s values</p> <p>For example, to set 20m/s wind in north (forward) direction:</p> <pre><code># Set wind to (20,0,0) in NED (forward direction)\nwind = AutonomySim.Vector3r(20, 0, 0)\nclient.simSetWind(wind)\n</code></pre> <p>Also see example script in set_wind.py</p>"},{"location":"apis.html#lidar-apis","title":"Lidar APIs","text":"<p>AutonomySim offers API to retrieve point cloud data from Lidar sensors on vehicles. You can set the number of channels, points per second, horizontal and vertical FOV, etc parameters in settings.json.</p> <p>More on lidar APIs and settings and sensor settings</p>"},{"location":"apis.html#light-control-apis","title":"Light Control APIs","text":"<p>Lights that can be manipulated inside AutonomySim can be created via the <code>simSpawnObject()</code> API by passing either <code>PointLightBP</code> or <code>SpotLightBP</code> as the <code>asset_name</code> parameter and <code>True</code> as the <code>is_blueprint</code> parameter. Once a light has been spawned, it can be manipulated using the following API:</p> <ul> <li><code>simSetLightIntensity</code>: This allows you to edit a light's intensity or brightness. It takes two parameters, <code>light_name</code>, the name of the light object returned by a previous call to <code>simSpawnObject()</code>, and <code>intensity</code>, a float value.</li> </ul>"},{"location":"apis.html#texture-apis","title":"Texture APIs","text":"<p>Textures can be dynamically set on objects via these APIs:</p> <ul> <li><code>simSetObjectMaterial</code>: This sets an object's material using an existing Unreal material asset. It takes two string parameters, <code>object_name</code> and <code>material_name</code>.</li> <li><code>simSetObjectMaterialFromTexture</code>: This sets an object's material using a path to a texture. It takes two string parameters, <code>object_name</code> and <code>texture_path</code>.</li> </ul>"},{"location":"apis.html#multiple-vehicles","title":"Multiple Vehicles","text":"<p>AutonomySim supports multiple vehicles and control them through APIs. Please Multiple Vehicles doc.</p>"},{"location":"apis.html#coordinate-system","title":"Coordinate System","text":"<p>All AutonomySim API uses NED coordinate system, i.e., +X is North, +Y is East and +Z is Down. All units are in SI system. Please note that this is different from coordinate system used internally by Unreal Engine. In Unreal Engine, +Z is up instead of down and length unit is in centimeters instead of meters. AutonomySim APIs takes care of the appropriate conversions. The starting point of the vehicle is always coordinates (0, 0, 0) in NED system. Thus when converting from Unreal coordinates to NED, we first subtract the starting offset and then scale by 100 for cm to m conversion. The vehicle is spawned in Unreal environment where the Player Start component is placed. There is a setting called <code>OriginGeopoint</code> in settings.json which assigns geographic longitude, longitude and altitude to the Player Start component.</p>"},{"location":"apis.html#vehicle-specific-apis","title":"Vehicle Specific APIs","text":""},{"location":"apis.html#apis-for-car","title":"APIs for Car","text":"<p>Car has followings APIs available:</p> <ul> <li><code>setCarControls</code>: This allows you to set throttle, steering, handbrake and auto or manual gear.</li> <li><code>getCarState</code>: This retrieves the state information including speed, current gear and 6 kinematics quantities: position, orientation, linear and angular velocity, linear and angular acceleration. All quantities are in NED coordinate system, SI units in world frame except for angular velocity and accelerations which are in body frame.</li> <li>Image APIs.</li> </ul>"},{"location":"apis.html#apis-for-multirotor","title":"APIs for Multirotor","text":"<p>Multirotor can be controlled by specifying angles, velocity vector, destination position or some combination of these. There are corresponding <code>move*</code> APIs for this purpose. When doing position control, we need to use some path following algorithm. By default AutonomySim uses carrot following algorithm. This is often referred to as \"high level control\" because you just need to specify high level goal and the firmware takes care of the rest. Currently lowest level control available in AutonomySim is <code>moveByAngleThrottleAsync</code> API.</p>"},{"location":"apis.html#getmultirotorstate","title":"getMultirotorState","text":"<p>This API returns the state of the vehicle in one call. The state includes, collision, estimated kinematics (i.e. kinematics computed by fusing sensors), and timestamp (nano seconds since epoch). The kinematics here means 6 quantities: position, orientation, linear and angular velocity, linear and angular acceleration. Please note that simple_slight currently doesn't support state estimator which means estimated and ground truth kinematics values would be same for simple_flight. Estimated kinematics are however available for PX4 except for angular acceleration. All quantities are in NED coordinate system, SI units in world frame except for angular velocity and accelerations which are in body frame.</p>"},{"location":"apis.html#async-methods-duration-and-max_wait_seconds","title":"Async methods, duration and max_wait_seconds","text":"<p>Many API methods has parameters named <code>duration</code> or <code>max_wait_seconds</code> and they have Async as suffix, for example, <code>takeoffAsync</code>. These methods will return immediately after starting the task in AutonomySim so that your client code can do something else while that task is being executed. If you want to wait for this task to complete then you can call <code>waitOnLastTask</code> like this:</p> <pre><code>//C++\nclient.takeoffAsync()-&gt;waitOnLastTask();\n</code></pre> <pre><code># Python\nclient.takeoffAsync().join()\n</code></pre> <p>If you start another command then it automatically cancels the previous task and starts new command. This allows to use pattern where your coded continuously does the sensing, computes a new trajectory to follow and issues that path to vehicle in AutonomySim. Each newly issued trajectory cancels the previous trajectory allowing your code to continuously do the update as new sensor data arrives.</p> <p>All Async method returns <code>concurrent.futures.Future</code> in Python (<code>std::future</code> in C++). Please note that these future classes currently do not allow to check status or cancel the task; they only allow to wait for task to complete. AutonomySim does provide API <code>cancelLastTask</code>, however.</p>"},{"location":"apis.html#drivetrain","title":"drivetrain","text":"<p>There are two modes you can fly vehicle: <code>drivetrain</code> parameter is set to <code>AutonomySim.DrivetrainType.ForwardOnly</code> or <code>AutonomySim.DrivetrainType.MaxDegreeOfFreedom</code>. When you specify ForwardOnly, you are saying that vehicle's front should always point in the direction of travel. So if you want drone to take left turn then it would first rotate so front points to left. This mode is useful when you have only front camera and you are operating vehicle using FPV view. This is more or less like travelling in car where you always have front view. The MaxDegreeOfFreedom means you don't care where the front points to. So when you take left turn, you just start going left like crab. Quadrotors can go in any direction regardless of where front points to. The MaxDegreeOfFreedom enables this mode.</p>"},{"location":"apis.html#yaw_mode","title":"yaw_mode","text":"<p><code>yaw_mode</code> is a struct <code>YawMode</code> with two fields, <code>yaw_or_rate</code> and <code>is_rate</code>. If <code>is_rate</code> field is True then <code>yaw_or_rate</code> field is interpreted as angular velocity in degrees/sec which means you want vehicle to rotate continuously around its axis at that angular velocity while moving. If <code>is_rate</code> is False then <code>yaw_or_rate</code> is interpreted as angle in degrees which means you want vehicle to rotate to specific angle (i.e. yaw) and keep that angle while moving.</p> <p>You can probably see that when <code>yaw_mode.is_rate == true</code>, the <code>drivetrain</code> parameter shouldn't be set to <code>ForwardOnly</code> because you are contradicting by saying that keep front pointing ahead but also rotate continuously. However if you have <code>yaw_mode.is_rate = false</code> in <code>ForwardOnly</code> mode then you can do some funky stuff. For example, you can have drone do circles and have yaw_or_rate set to 90 so camera is always pointed to center (\"super cool selfie mode\"). In <code>MaxDegreeofFreedom</code> also you can get some funky stuff by setting <code>yaw_mode.is_rate = true</code> and say <code>yaw_mode.yaw_or_rate = 20</code>. This will cause drone to go in its path while rotating which may allow to do 360 scanning.</p> <p>In most cases, you just don't want yaw to change which you can do by setting yaw rate of 0. The shorthand for this is <code>AutonomySim.YawMode.Zero()</code> (or in C++: <code>YawMode::Zero()</code>).</p>"},{"location":"apis.html#lookahead-and-adaptive_lookahead","title":"lookahead and adaptive_lookahead","text":"<p>When you ask vehicle to follow a path, AutonomySim uses \"carrot following\" algorithm. This algorithm operates by looking ahead on path and adjusting its velocity vector. The parameters for this algorithm is specified by <code>lookahead</code> and <code>adaptive_lookahead</code>. For most of the time you want algorithm to auto-decide the values by simply setting <code>lookahead = -1</code> and <code>adaptive_lookahead = 0</code>.</p>"},{"location":"apis.html#using-apis-on-real-vehicles","title":"Using APIs on Real Vehicles","text":"<p>We want to be able to run same code that runs in simulation as on real vehicle. This allows you to test your code in simulator and deploy to real vehicle.</p> <p>Generally speaking, APIs therefore shouldn't allow you to do something that cannot be done on real vehicle (for example, getting the ground truth). But, of course, simulator has much more information and it would be useful in applications that may not care about running things on real vehicle. For this reason, we clearly delineate between sim-only APIs by attaching <code>sim</code> prefix, for example, <code>simGetGroundTruthKinematics</code>. This way you can avoid using these simulation-only APIs if you care about running your code on real vehicles.</p> <p>The AutonomyLib is self-contained library that you can put on an offboard computing module such as the Gigabyte barebone Mini PC. This module then can talk to the flight controllers such as PX4 using exact same code and flight controller protocol. The code you write for testing in the simulator remains unchanged. See AutonomyLib on custom drones.</p>"},{"location":"apis.html#adding-new-apis-to-autonomysim","title":"Adding New APIs to AutonomySim","text":"<p>See the Adding New APIs page</p>"},{"location":"apis.html#references-and-examples","title":"References and Examples","text":"<ul> <li>C++ API Examples</li> <li>Car Examples</li> <li>Multirotor Examples</li> <li>Computer Vision Examples</li> <li>Move on Path demo showing video of fast multirotor flight through Modular Neighborhood environment</li> <li>Building a Hexacopter</li> <li>Building Point Clouds</li> </ul>"},{"location":"apis.html#faq","title":"FAQ","text":""},{"location":"apis.html#unreal-is-slowed-down-dramatically-when-i-run-api","title":"Unreal is slowed down dramatically when I run API","text":"<p>If you see Unreal getting slowed down dramatically when Unreal Engine window loses focus then go to 'Edit-&gt;Editor Preferences' in Unreal Editor, in the 'Search' box type 'CPU' and ensure that the 'Use Less CPU when in Background' is unchecked.</p>"},{"location":"apis.html#do-i-need-anything-else-on-windows","title":"Do I need anything else on Windows?","text":"<p>You should install VS2019 with VC++, Windows SDK 10.0 and Python. To use Python APIs you will need Python 3.5 or later (install it using Anaconda).</p>"},{"location":"apis.html#which-version-of-python-should-i-use","title":"Which version of Python should I use?","text":"<p>We recommend Anaconda to get Python tools and libraries. Our code is tested with Python 3.5.3 :: Anaconda 4.4.0. This is important because older version have been known to have problems.</p>"},{"location":"apis.html#i-get-error-on-import-cv2","title":"I get error on <code>import cv2</code>","text":"<p>You can install OpenCV using:</p> <pre><code>conda install opencv\npip install opencv-python\n</code></pre>"},{"location":"apis.html#typeerror-unsupported-operand-types-for-asyncioloop-and-float","title":"TypeError: unsupported operand type(s) for *: 'AsyncIOLoop' and 'float'","text":"<p>This error happens if you install Jupyter, which somehow breaks the msgpackrpc library.  Create a new python environment which the minimal required packages.</p>"},{"location":"apis_cpp.html","title":"Using the C++ APIs","text":"<p>Please read the general API documentation first if you haven't already. This document describes C++ API examples and other C++ specific details.</p>"},{"location":"apis_cpp.html#quick-start","title":"Quick Start","text":"<p>The fastest way to get started is to open <code>AutonomySim.sln</code> in Visual Studio 2022. You will see Hello Car and Hello Drone examples in the solution. These examples will show you the include paths and lib paths you will need to setup in your VC++ projects. If you are using Linux then you will specify these paths either in your cmake file or on compiler command line.</p>"},{"location":"apis_cpp.html#include-and-lib-folders","title":"Include and Lib Folders","text":"<ul> <li>Include folders: <code>$(ProjectDir)..\\AutonomyLib\\deps\\rpclib\\include;include;$(ProjectDir)..\\AutonomyLib\\deps\\eigen3;$(ProjectDir)..\\AutonomyLib\\include</code></li> <li>Dependencies: <code>rpc.lib</code></li> <li>Lib folders: <code>$(ProjectDir)\\..\\AutonomyLib\\deps\\MavLinkCom\\lib\\$(Platform)\\$(Configuration);$(ProjectDir)\\..\\AutonomyLib\\deps\\rpclib\\lib\\$(Platform)\\$(Configuration);$(ProjectDir)\\..\\AutonomyLib\\lib\\$(Platform)\\$(Configuration)</code></li> <li>References: Reference AutonomyLib and MavLinkCom to the project references. (Right click your project then go to <code>References</code>, <code>Add reference...</code>, and then select AutonomyLib and MavLinkCom)</li> </ul>"},{"location":"apis_cpp.html#hello-car","title":"Hello Car","text":"<p>Here's how to use AutonomySim APIs using C++ to control simulated car (see also Python example):</p> <pre><code>// ready to run example: https://github.com/nervosys/AutonomySim/blob/master/HelloCar/main.cpp\n\n#include &lt;iostream&gt;\n#include \"vehicles/car/api/CarRpcLibClient.hpp\"\n\nint main() {\n\n    nervosys::autonomylib::CarRpcLibClient client;\n    client.enableApiControl(true); //this disables manual control\n    CarControllerBase::CarControls controls;\n\n    std::cout &lt;&lt; \"Press enter to drive forward\" &lt;&lt; std::endl; std::cin.get();\n    controls.throttle = 1;\n    client.setCarControls(controls);\n\n    std::cout &lt;&lt; \"Press Enter to activate handbrake\" &lt;&lt; std::endl; std::cin.get();\n    controls.handbrake = true;\n    client.setCarControls(controls);\n\n    std::cout &lt;&lt; \"Press Enter to take turn and drive backward\" &lt;&lt; std::endl; std::cin.get();\n    controls.handbrake = false;\n    controls.throttle = -1;\n    controls.steering = 1;\n    client.setCarControls(controls);\n\n    std::cout &lt;&lt; \"Press Enter to stop\" &lt;&lt; std::endl; std::cin.get();\n    client.setCarControls(CarControllerBase::CarControls());\n\n    return 0;\n}\n</code></pre>"},{"location":"apis_cpp.html#hello-drone","title":"Hello Drone","text":"<p>Here's how to use AutonomySim APIs using C++ to control simulated quadrotor (see also Python example):</p> <pre><code>// ready to run example: https://github.com/nervosys/AutonomySim/blob/master/HelloDrone/main.cpp\n\n#include &lt;iostream&gt;\n#include \"vehicles/multirotor/api/MultirotorRpcLibClient.hpp\"\n\nint main() {\n\n    nervosys::autonomylib::MultirotorRpcLibClient client;\n\n    std::cout &lt;&lt; \"Press Enter to enable API control\\n\"; std::cin.get();\n    client.enableApiControl(true);\n\n    std::cout &lt;&lt; \"Press Enter to arm the drone\\n\"; std::cin.get();\n    client.armDisarm(true);\n\n    std::cout &lt;&lt; \"Press Enter to takeoff\\n\"; std::cin.get();\n    client.takeoffAsync(5)-&gt;waitOnLastTask();\n\n    std::cout &lt;&lt; \"Press Enter to move 5 meters in x direction with 1 m/s velocity\\n\"; std::cin.get();\n    auto position = client.getMultirotorState().getPosition(); // from current location\n    client.moveToPositionAsync(position.x() + 5, position.y(), position.z(), 1)-&gt;waitOnLastTask();\n\n    std::cout &lt;&lt; \"Press Enter to land\\n\"; std::cin.get();\n    client.landAsync()-&gt;waitOnLastTask();\n\n    return 0;\n}\n</code></pre>"},{"location":"apis_cpp.html#see-also","title":"See Also","text":"<ul> <li>Examples of how to use internal infrastructure in AutonomySim in your other projects</li> <li>DroneShell app shows how to make simple interface using C++ APIs to control drones</li> <li>HelloSpawnedDrones app shows how to make additional vehicles on the fly</li> <li>Python APIs</li> </ul>"},{"location":"apis_image.html","title":"Image APIs","text":"<p>Please read general API doc first if you are not familiar with <code>AutonomySim</code> APIs.</p>"},{"location":"apis_image.html#getting-a-single-image","title":"Getting a Single Image","text":"<p>Here's a sample code to get a single image from camera named \"0\". The returned value is bytes of png format image. To get uncompressed and other format as well as available cameras please see next sections.</p>"},{"location":"apis_image.html#python","title":"Python","text":"<pre><code>import AutonomySim  # pip install AutonomySim\n\n# for car use CarClient() \nclient = AutonomySim.MultirotorClient()\n\npng_image = client.simGetImage(\"0\", AutonomySim.ImageType.Scene)\n# do something with image\n</code></pre>"},{"location":"apis_image.html#c","title":"C++","text":"<pre><code>#include \"vehicles/multirotor/api/MultirotorRpcLibClient.hpp\"\n\nint getOneImage() {\n    using namespace nervosys::autonomylib;\n\n    // for car use CarRpcLibClient\n    MultirotorRpcLibClient client;\n\n    std::vector&lt;uint8_t&gt; png_image = client.simGetImage(\"0\", VehicleCameraBase::ImageType::Scene);\n    // do something with images\n}\n</code></pre>"},{"location":"apis_image.html#getting-images-with-greater-flexibility","title":"Getting Images with Greater Flexibility","text":"<p>The <code>simGetImages</code> API which is slightly more complex to use than <code>simGetImage</code> API, for example, you can get left camera view, right camera view and depth image from left camera in a single API call. The <code>simGetImages</code> API also allows you to get uncompressed images as well as floating point single channel images (instead of 3 channel (RGB), each 8 bit).</p>"},{"location":"apis_image.html#python_1","title":"Python","text":"<pre><code>import AutonomySim  # pip install AutonomySim\n\n# for car use CarClient() \nclient = AutonomySim.MultirotorClient()\n\nresponses = client.simGetImages([\n    # png format\n    AutonomySim.ImageRequest(0, AutonomySim.ImageType.Scene), \n    # uncompressed RGB array bytes\n    AutonomySim.ImageRequest(1, AutonomySim.ImageType.Scene, False, False),\n    # floating point uncompressed image\n    AutonomySim.ImageRequest(1, AutonomySim.ImageType.DepthPlanar, True)\n])\n\n# do something with response which contains image data, pose, timestamp etc\n</code></pre>"},{"location":"apis_image.html#using-autonomysim-images-with-numpy","title":"Using AutonomySim Images with NumPy","text":"<p>If you plan to use numpy for image manipulation, you should get uncompressed RGB image and then convert to numpy like this:</p> <pre><code>responses = client.simGetImages([AutonomySim.ImageRequest(\"0\", AutonomySim.ImageType.Scene, False, False)])\nresponse = responses[0]\n\n# get numpy array\nimg1d = np.fromstring(response.image_data_uint8, dtype=np.uint8) \n\n# reshape array to 4 channel image array H X W X 4\nimg_rgb = img1d.reshape(response.height, response.width, 3)\n\n# original image is fliped vertically\nimg_rgb = np.flipud(img_rgb)\n\n# write to png \nAutonomySim.write_png(os.path.normpath(filename + '.png'), img_rgb) \n</code></pre>"},{"location":"apis_image.html#tips","title":"Tips","text":"<ul> <li> <p>The API <code>simGetImage</code> returns <code>binary string literal</code> which means you can simply dump it in binary file to create a .png file. However if you want to process it in any other way than you can handy function <code>AutonomySim.string_to_uint8_array</code>. This converts binary string literal to NumPy uint8 array.</p> </li> <li> <p>The API <code>simGetImages</code> can accept request for multiple image types from any cameras in single call. You can specify if image is png compressed, RGB uncompressed or float array. For png compressed images, you get <code>binary string literal</code>. For float array you get Python list of float64. You can convert this float array to NumPy 2D array using:</p> <pre><code>AutonomySim.list_to_2d_float_array(response.image_data_float, response.width, response.height)\n</code></pre> <p>You can also save float array to .pfm file (Portable Float Map format) using <code>AutonomySim.write_pfm()</code> function.</p> </li> <li> <p>If you are looking to query position and orientation information in sync with a call to one of the image APIs, you can use <code>client.simPause(True)</code> and <code>client.simPause(False)</code> to pause the simulation while calling the image API and querying the desired physics state, ensuring that the physics state remains the same immediately after the image API call.</p> </li> </ul>"},{"location":"apis_image.html#c_1","title":"C++","text":"<pre><code>int getStereoAndDepthImages() {\n    using namespace nervosys::autonomylib;\n\n    typedef VehicleCameraBase::ImageRequest ImageRequest;\n    typedef VehicleCameraBase::ImageResponse ImageResponse;\n    typedef VehicleCameraBase::ImageType ImageType;\n\n    // for car use\n    // CarRpcLibClient client;\n    MultirotorRpcLibClient client;\n\n    // get right, left and depth images. First two as png, second as float16.\n    std::vector&lt;ImageRequest&gt; request = { \n        //png format\n        ImageRequest(\"0\", ImageType::Scene),\n        //uncompressed RGB array bytes\n        ImageRequest(\"1\", ImageType::Scene, false, false),       \n        //floating point uncompressed image  \n        ImageRequest(\"1\", ImageType::DepthPlanar, true) \n    };\n\n    const std::vector&lt;ImageResponse&gt;&amp; response = client.simGetImages(request);\n    // do something with response which contains image data, pose, timestamp etc\n}\n</code></pre>"},{"location":"apis_image.html#complete-examples","title":"Complete Examples","text":""},{"location":"apis_image.html#python_2","title":"Python","text":"<p>For a more complete ready to run sample code please see sample code in AutonomySimClient project for multirotors or HelloCar sample. This code also demonstrates simple activities such as saving images in files or using <code>numpy</code> to manipulate images.</p>"},{"location":"apis_image.html#c_2","title":"C++","text":"<p>For a more complete ready to run sample code please see sample code in HelloDrone project for multirotors or HelloCar project. </p> <p>See also other example code that generates specified number of stereo images along with ground truth depth and disparity and saving it to pfm format.</p>"},{"location":"apis_image.html#available-cameras","title":"Available Cameras","text":"<p>These are the default cameras already available in each vehicle. Apart from these, you can add more cameras to the vehicles and external cameras which are not attached to any vehicle through the settings.</p>"},{"location":"apis_image.html#car","title":"Car","text":"<p>The cameras on car can be accessed by following names in API calls: <code>front_center</code>, <code>front_right</code>, <code>front_left</code>, <code>fpv</code> and <code>back_center</code>. Here FPV camera is driver's head position in the car.</p>"},{"location":"apis_image.html#multirotor","title":"Multirotor","text":"<p>The cameras on the drone can be accessed by following names in API calls: <code>front_center</code>, <code>front_right</code>, <code>front_left</code>, <code>bottom_center</code> and <code>back_center</code>. </p>"},{"location":"apis_image.html#computer-vision-mode","title":"Computer Vision Mode","text":"<p>Camera names are same as in multirotor.</p>"},{"location":"apis_image.html#backward-compatibility-for-camera-names","title":"Backward compatibility for camera names","text":"<p>Before AutonomySim v1.2, cameras were accessed using ID numbers instead of names. For backward compatibility you can still use following ID numbers for above camera names in same order as above: <code>\"0\"</code>, <code>\"1\"</code>, <code>\"2\"</code>, <code>\"3\"</code>, <code>\"4\"</code>. In addition, camera name <code>\"\"</code> is also available to access the default camera which is generally the camera <code>\"0\"</code>.</p>"},{"location":"apis_image.html#computer-vision-mode_1","title":"\"Computer Vision\" Mode","text":"<p>You can use AutonomySim in so-called \"Computer Vision\" mode. In this mode, physics engine is disabled and there is no vehicle, just cameras (If you want to have the vehicle but without its kinematics, you can use the Multirotor mode with the Physics Engine ExternalPhysicsEngine). You can move around using keyboard (use F1 to see help on keys). You can press Record button to continuously generate images. Or you can call APIs to move cameras around and take images.</p> <p>To active this mode, edit settings.json that you can find in your <code>Documents\\AutonomySim</code> folder (or <code>~/Documents/AutonomySim</code> on Linux) and make sure following values exist at root level:</p> <pre><code>{\n  \"SettingsVersion\": 1.2,\n  \"SimMode\": \"ComputerVision\"\n}\n</code></pre> <p>Here's the Python code example to move camera around and capture images.</p> <p>This mode was inspired from UnrealCV project.</p>"},{"location":"apis_image.html#setting-pose-in-computer-vision-mode","title":"Setting Pose in Computer Vision Mode","text":"<p>To move around the environment using APIs you can use <code>simSetVehiclePose</code> API. This API takes position and orientation and sets that on the invisible vehicle where the front-center camera is located. All rest of the cameras move along keeping the relative position. If you don't want to change position (or orientation) then just set components of position (or orientation) to floating point nan values. The <code>simGetVehiclePose</code> allows to retrieve the current pose. You can also use <code>simGetGroundTruthKinematics</code> to get the quantities kinematics quantities for the movement. Many other non-vehicle specific APIs are also available such as segmentation APIs, collision APIs and camera APIs.</p>"},{"location":"apis_image.html#camera-apis","title":"Camera APIs","text":"<p>The <code>simGetCameraInfo</code> returns the pose (in world frame, NED coordinates, SI units) and FOV (in degrees) for the specified camera. Please see example usage.</p> <p>The <code>simSetCameraPose</code> sets the pose for the specified camera while taking an input pose as a combination of relative position and a quaternion in NED frame. The handy <code>AutonomySim.to_quaternion()</code> function allows to convert pitch, roll, yaw to quaternion. For example, to set camera-0 to 15-degree pitch while maintaining the same position, you can use:</p> <pre><code>camera_pose = AutonomySim.Pose(AutonomySim.Vector3r(0, 0, 0), AutonomySim.to_quaternion(0.261799, 0, 0))  #PRY in radians\nclient.simSetCameraPose(0, camera_pose);\n</code></pre> <ul> <li><code>simSetCameraFov</code> allows changing the Field-of-View of the camera at runtime.</li> <li><code>simSetDistortionParams</code>, <code>simGetDistortionParams</code> allow setting and fetching the distortion parameters K1, K2, K3, P1, P2</li> </ul> <p>All Camera APIs take in 3 common parameters apart from the API-specific ones, <code>camera_name</code>(str), <code>vehicle_name</code>(str) and <code>external</code>(bool, to indicate External Camera). Camera and vehicle name is used to get the specific camera, if <code>external</code> is set to <code>true</code>, then the vehicle name is ignored. Also see external_camera.py for example usage of these APIs.</p>"},{"location":"apis_image.html#gimbal","title":"Gimbal","text":"<p>You can set stabilization for pitch, roll or yaw for any camera using settings.</p> <p>Please see example usage.</p>"},{"location":"apis_image.html#changing-camera-resolution-and-parameters","title":"Changing Camera Resolution and Parameters","text":"<p>To change resolution, FOV etc, you can use settings.json. For example, below addition in settings.json sets parameters for scene capture and uses \"Computer Vision\" mode described above. If you omit any setting then below default values will be used. For more information see settings doc. If you are using stereo camera, currently the distance between left and right is fixed at 25 cm.</p> <pre><code>{\n  \"SettingsVersion\": 1.2,\n  \"CameraDefaults\": {\n      \"CaptureSettings\": [\n        {\n          \"ImageType\": 0,\n          \"Width\": 256,\n          \"Height\": 144,\n          \"FOV_Degrees\": 90,\n          \"AutoExposureSpeed\": 100,\n          \"MotionBlurAmount\": 0\n        }\n    ]\n  },\n  \"SimMode\": \"ComputerVision\"\n}\n</code></pre>"},{"location":"apis_image.html#what-pixel-values-mean-in-different-image-types","title":"What Pixel Values Mean in Different Image Types","text":""},{"location":"apis_image.html#available-imagetype-values","title":"Available ImageType Values","text":"<pre><code>  Scene = 0, \n  DepthPlanar = 1, \n  DepthPerspective = 2,\n  DepthVis = 3, \n  DisparityNormalized = 4,\n  Segmentation = 5,\n  SurfaceNormals = 6,\n  Infrared = 7,\n  OpticalFlow = 8,\n  OpticalFlowVis = 9\n</code></pre>"},{"location":"apis_image.html#depthplanar-and-depthperspective","title":"DepthPlanar and DepthPerspective","text":"<p>You normally want to retrieve the depth image as float (i.e. set <code>pixels_as_float = true</code>) and specify <code>ImageType = DepthPlanar</code> or <code>ImageType = DepthPerspective</code> in <code>ImageRequest</code>. For <code>ImageType = DepthPlanar</code>, you get depth in camera plane, i.e., all points that are plane-parallel to the camera have same depth. For <code>ImageType = DepthPerspective</code>, you get depth from camera using a projection ray that hits that pixel. Depending on your use case, planner depth or perspective depth may be the ground truth image that you want. For example, you may be able to feed perspective depth to ROS package such as <code>depth_image_proc</code> to generate a point cloud. Or planner depth may be more compatible with estimated depth image generated by stereo algorithms such as SGM.</p>"},{"location":"apis_image.html#depthvis","title":"DepthVis","text":"<p>When you specify <code>ImageType = DepthVis</code> in <code>ImageRequest</code>, you get an image that helps depth visualization. In this case, each pixel value is interpolated from black to white depending on depth in camera plane in meters. The pixels with pure white means depth of 100m or more while pure black means depth of 0 meters.</p>"},{"location":"apis_image.html#disparitynormalized","title":"DisparityNormalized","text":"<p>You normally want to retrieve disparity image as float (i.e. set <code>pixels_as_float = true</code> and specify <code>ImageType = DisparityNormalized</code> in <code>ImageRequest</code>) in which case each pixel is <code>(Xl - Xr)/Xmax</code>, which is thereby normalized to values between 0 to 1.</p>"},{"location":"apis_image.html#segmentation","title":"Segmentation","text":"<p>When you specify <code>ImageType = Segmentation</code> in <code>ImageRequest</code>, you get an image that gives you ground truth segmentation of the scene. At the startup, AutonomySim assigns value 0 to 255 to each mesh available in environment. This value is then mapped to a specific color in the pallet. The RGB values for each object ID can be found in this file.</p> <p>You can assign a specific value (limited to the range 0-255) to a specific mesh using APIs. For example, below Python code sets the object ID for the mesh called \"Ground\" to 20 in Blocks environment and hence changes its color in Segmentation view:</p> <pre><code>success = client.simSetSegmentationObjectID(\"Ground\", 20);\n</code></pre> <p>The return value is a boolean type that lets you know if the mesh was found.</p> <p>Notice that typical Unreal environments, like Blocks, usually have many other meshes that comprises of same object, for example, \"Ground_2\", \"Ground_3\" and so on. As it is tedious to set object ID for all of these meshes, AutonomySim also supports regular expressions. For example, the code below sets all meshes which have names starting with \"ground\" (ignoring case) to 21 with just one line:</p> <pre><code>success = client.simSetSegmentationObjectID(\"ground[\\w]*\", 21, True);\n</code></pre> <p>The return value is true if at least one mesh was found using regular expression matching.</p> <p>It is recommended that you request uncompressed image using this API to ensure you get precise RGB values for segmentation image:</p> <pre><code>responses = client.simGetImages([ImageRequest(0, AutonomySimImageType.Segmentation, False, False)])\nimg1d = np.fromstring(response.image_data_uint8, dtype=np.uint8) #get numpy array\nimg_rgb = img1d.reshape(response.height, response.width, 3) #reshape array to 3 channel image array H X W X 3\nimg_rgb = np.flipud(img_rgb) #original image is fliped vertically\n\n#find unique colors\nprint(np.unique(img_rgb[:,:,0], return_counts=True)) #red\nprint(np.unique(img_rgb[:,:,1], return_counts=True)) #green\nprint(np.unique(img_rgb[:,:,2], return_counts=True)) #blue  \n</code></pre> <p>A complete ready-to-run example can be found in segmentation.py.</p>"},{"location":"apis_image.html#unsetting-an-object-id","title":"Unsetting an Object ID","text":"<p>An object's ID can be set to -1 to make it not show up on the segmentation image.</p>"},{"location":"apis_image.html#how-to-find-mesh-names","title":"How to Find Mesh Names?","text":"<p>To get desired ground truth segmentation you will need to know the names of the meshes in your Unreal environment. To do this, you will need to open up Unreal Environment in Unreal Editor and then inspect the names of the meshes you are interested in using the World Outliner. For example, below we see the mesh names for the ground in Blocks environment in right panel in the editor:</p> <p></p> <p>If you don't know how to open Unreal Environment in Unreal Editor then try following the guide for building from source.</p> <p>Once you decide on the meshes you are interested, note down their names and use above API to set their object IDs. There are few settings available to change object ID generation behavior.</p>"},{"location":"apis_image.html#changing-colors-for-object-ids","title":"Changing Colors for Object IDs","text":"<p>At present the color for each object ID is fixed as in this pallet. We will be adding ability to change colors for object IDs to desired values shortly. In the meantime you can open the segmentation image in your favorite image editor and get the RGB values you are interested in.</p>"},{"location":"apis_image.html#initial-object-ids","title":"Initial Object IDs","text":"<p>At the start, AutonomySim assigns object ID to each object found in environment of type <code>UStaticMeshComponent</code> or <code>ALandscapeProxy</code>. It then either uses mesh name or owner name (depending on settings), lower cases it, removes any chars below ASCII 97 to remove numbers and some punctuations, sums int value of all chars and modulo 255 to generate the object ID. In other words, all object with same alphabet chars would get same object ID. This heuristic is simple and effective for many Unreal environments but may not be what you want. In that case, please use above APIs to change object IDs to your desired values. There are few settings available to change this behavior.</p>"},{"location":"apis_image.html#getting-object-id-for-mesh","title":"Getting Object ID for Mesh","text":"<p>The <code>simGetSegmentationObjectID</code> API allows you get object ID for given mesh name.</p>"},{"location":"apis_image.html#infrared","title":"Infrared","text":"<p>Currently this is just a map from object ID to grey scale 0-255. So any mesh with object ID 42 shows up with color (42, 42, 42). Please see segmentation section for more details on how to set object IDs. Typically noise setting can be applied for this image type to get slightly more realistic effect. We are still working on adding other infrared artifacts and any contributions are welcome.</p>"},{"location":"apis_image.html#opticalflow-and-opticalflowvis","title":"OpticalFlow and OpticalFlowVis","text":"<p>These image types return information about motion perceived by the point of view of the camera. OpticalFlow returns a 2-channel image where the channels correspond to vx and vy respectively. OpticalFlowVis is similar to OpticalFlow but converts flow data to RGB for a more 'visual' output.</p>"},{"location":"apis_image.html#example-code","title":"Example Code","text":"<p>A complete example of setting vehicle positions at random locations and orientations and then taking images can be found in GenerateImageGenerator.hpp. This example generates specified number of stereo images and ground truth disparity image and saving it to pfm format.</p>"},{"location":"apis_new.html","title":"Adding New APIs","text":"<p>Adding new <code>AutonomySim</code> APIs requires modifying the source code. These changes are required for the various levels of abstractions that <code>AutonomySim</code> supports. The primary files that need to be modified are described below, along with demonstration commits and pull requests (PRs). Specific sections of PRs or commits might be linked in some places, but it'll be helpful to have a look at the entire <code>diff</code> to get a sense of the workflow.</p> <p>Do not hesitate to open an issue or a draft PR if you are unsure about how to make changes or to get feedback.</p>"},{"location":"apis_new.html#implementing-the-api","title":"Implementing the API","text":"<p>Before adding the wrapper code to call and handle the API, it needs to be implemented first. The exact files where this will occur varies depending on what it does. Few examples are given below which might help you in getting started.</p>"},{"location":"apis_new.html#vehicle-based-apis","title":"Vehicle-based APIs","text":"<p><code>moveByVelocityBodyFrameAsync</code> API for velocity-based movement in the multirotor's X-Y frame.</p> <p>The main implementation is done in MultirotorBaseApi.cpp, where most of the multirotor APIs are implemented.</p> <p>In some cases, additional structures might be needed for storing data, <code>getRotorStates</code> API is a good example for this, here the <code>RotorStates</code> struct is defined in 2 places for conversion from RPC to internal code. It also requires modifications in AutonomyLib as well as Unreal/Plugins for the implementation.</p>"},{"location":"apis_new.html#environment-related-apis","title":"Environment-related APIs","text":"<p>These APIs need to interact with the simulation environment itself, hence it's likely that it'll be implemented inside the <code>Unreal/Plugins</code> folder.</p> <ul> <li> <p><code>simCreateVoxelGrid</code> API to generate and save a binvox-formatted grid of the environment - WorldSimApi.cpp</p> </li> <li> <p><code>simAddVehicle</code> API to create vehicles at runtime - SimMode*, WorldSimApi files</p> </li> </ul>"},{"location":"apis_new.html#physics-related-apis","title":"Physics-related APIs","text":"<p><code>simSetWind</code> API shows an example of modifying the physics behaviour and adding an API + settings field for the same. See the PR for details about the code.</p>"},{"location":"apis_new.html#rpc-wrappers","title":"RPC Wrappers","text":"<p>The APIs use msgpack-rpc protocol over TCP/IP through rpclib developed by Tam\u00c3\u00a1s Szelei which allows you to use variety of programming languages including C++, C#, Python, Java etc. When AutonomySim starts, it opens port 41451 (this can be changed via settings) and listens for incoming request. The Python or C++ client code connects to this port and sends RPC calls using msgpack serialization format.</p> <p>To add the RPC code to call the new API, follow the steps below. Follow the implementation of other APIs defined in the files.</p> <ol> <li>Add an RPC handler in the server which calls your implemented method in RpcLibServerBase.cpp. Vehicle-specific APIs are in their respective vehicle subfolder.</li> <li>Add the C++ client API method in RpcClientBase.cpp</li> <li>Add the Python client API method in client.py. If needed, add or modify a structure definition in types.py</li> </ol>"},{"location":"apis_new.html#testing","title":"Testing","text":"<p>Testing is required to ensure that the API is working as expected. For this, as expected, you'll have to use the source-built AutonomySim and Blocks environment. Apart from this, if using the Python APIs, you'll have to use the <code>AutonomySim</code> package from source rather than the PyPI package. Below are 2 ways described to go about using the package from source -</p> <ol> <li>Use setup_path.py. It will setup the path such that the local AutonomySim module is used instead of the pip installed package. This is the method used in many of the scripts since the user doesn't need to do anything other than run the script. Place your example script in one of the folders inside <code>PythonClient</code> like <code>multirotor</code>, <code>car</code>, etc. You can also create one to keep things separate, and copy the <code>setup_path.py</code> file from another folder. Add <code>import setup_path</code> before <code>import AutonomySim</code> in your files. Now the latest main API (or any branch currently checked out) will be used.</li> <li>Use a local project pip install. Regular install would create a copy of the current source and use it, whereas Editable install (<code>pip install -e .</code> from inside the <code>PythonClient</code> folder) would change the package whenever the Python API files are changed. Editable install has the benefit when working on several branches or API is not finalized.</li> </ol> <p>It is recommended to use a virtual environment for dealing with Python packaging so as to not break any existing setup.</p> <p>When opening a PR, make sure to follow the coding guidelines. Also add a docstring for the API in the Python files, and please include any example scripts and settings required in the script as well.</p>"},{"location":"apis_upgrading.html","title":"Upgrading Existing APIs","text":"<p>There have been several API changes in <code>AutonomySim</code> v1.2 that we hope removes inconsistency, adds future extensibility and presents cleaner interface. Many of these changes are however breaking changes, which means that you will need to modify your client code that communicates with <code>AutonomySim</code>.</p>"},{"location":"apis_upgrading.html#a-faster-way","title":"A Faster Way","text":"<p>While most changes you need to do in your client code are fairly easy, a faster way is to simply take a look at the example code, such as Hello Droneor Hello Car, to gain an understanding of the changes.</p>"},{"location":"apis_upgrading.html#importing-autonomysim","title":"Importing <code>AutonomySim</code>","text":"<p>Instead of this:</p> <p><pre><code>from AutonomySimClient import *\n</code></pre> use this:</p> <pre><code>import AutonomySim\n</code></pre> <p>Above assumes you have installed AutonomySim module using, </p> <pre><code>pip install --user AutonomySim\n</code></pre> <p>If you are running you code from PythonClient folder in repo then you can also do this:</p> <pre><code>import setup_path \nimport AutonomySim\n</code></pre> <p>Here setup_path.py should exist in your folder and it will set the path of <code>AutonomySim</code> package in <code>PythonClient</code> repo folder. All examples in PythonClient folder uses this method.</p>"},{"location":"apis_upgrading.html#using-autonomysim-classes","title":"Using AutonomySim Classes","text":"<p>As we have everything now in package, you will need to use explicit namespace for AutonomySim classes like shown below.</p> <p>Instead of this:</p> <pre><code>client1 = CarClient()\n</code></pre> <p>use this:</p> <pre><code>client1 = AutonomySim.CarClient()\n</code></pre>"},{"location":"apis_upgrading.html#autonomysim-types","title":"AutonomySim Types","text":"<p>We have moved all types in <code>AutonomySim</code> namespace.</p> <p>Instead of this:</p> <pre><code>image_type = AutonomySimImageType.DepthVis\n\nd = DrivetrainType.MaxDegreeOfFreedom\n</code></pre> <p>use this:</p> <pre><code>image_type = AutonomySim.ImageType.DepthVis\n\nd = AutonomySim.DrivetrainType.MaxDegreeOfFreedom\n</code></pre>"},{"location":"apis_upgrading.html#getting-images","title":"Getting Images","text":"<p>Nothing new below, it's just combination of above. Note that all APIs that previously took <code>camera_id</code>, now takes <code>camera_name</code> instead. You can take a look at available cameras here.</p> <p>Instead of this:</p> <pre><code>responses = client.simGetImages([ImageRequest(0, AutonomySimImageType.DepthVis)])\n</code></pre> <p>use this:</p> <pre><code>responses = client.simGetImages([AutonomySim.ImageRequest(\"0\", AutonomySim.ImageType.DepthVis)])\n</code></pre>"},{"location":"apis_upgrading.html#utility-methods","title":"Utility Methods","text":"<p>In earlier version, we provided several utility methods as part of <code>AutonomySimClientBase</code>. These methods are now moved to <code>AutonomySim</code> namespace for more pythonic interface.</p> <p>Instead of this:</p> <pre><code>AutonomySimClientBase.write_png(my_path, img_rgba) \n\nAutonomySimClientBase.wait_key('Press any key')\n</code></pre> <p>use this:</p> <pre><code>AutonomySim.write_png(my_path, img_rgba)\n\nAutonomySim.wait_key('Press any key')\n</code></pre>"},{"location":"apis_upgrading.html#camera-names","title":"Camera Names","text":"<p>AutonomySim now uses names to reference cameras instead of index numbers. However to retain backward compatibility, these names are aliased with old index numbers as string.</p> <p>Instead of this:</p> <pre><code>client.simGetCameraInfo(0)\n</code></pre> <p>use this:</p> <pre><code>client.simGetCameraInfo(\"0\")\n\n# or\n\nclient.simGetCameraInfo(\"front-center\")\n</code></pre>"},{"location":"apis_upgrading.html#async-methods","title":"Async Methods","text":"<p>For multirotors, AutonomySim had various methods such as <code>takeoff</code> or <code>moveByVelocityZ</code> that would take long time to complete. All of such methods are now renamed by adding the suffix Async as shown below.</p> <p>Instead of this:</p> <pre><code>client.takeoff()\n\nclient.moveToPosition(-10, 10, -10, 5)\n</code></pre> <p>use this:</p> <pre><code>client.takeoffAsync().join()\n\nclient.moveToPositionAsync(-10, 10, -10, 5).join()\n</code></pre> <p>Here <code>.join()</code> is a call on Python's <code>Future</code> class to wait for the async call to complete. You can also choose to do some other computation instead while the call is in progress.</p>"},{"location":"apis_upgrading.html#simulation-only-methods","title":"Simulation-Only Methods","text":"<p>Now we have clear distinction between methods that are only available in simulation from the ones that may be available on actual vehicle. The simulation only methods are prefixed with <code>sim</code> as shown below.</p> <pre><code>getCollisionInfo()      is renamed to       simGetCollisionInfo()\ngetCameraInfo()         is renamed to       simGetCameraInfo()\nsetCameraOrientation()  is renamed to       simSetCameraOrientation()\n</code></pre>"},{"location":"apis_upgrading.html#state-information","title":"State Information","text":"<p>Previously <code>CarState</code> mixed simulation-only information like <code>kinematics_true</code>. Moving forward, <code>CarState</code> will only contain information that can be obtained in real world.</p> <pre><code>k = car_state.kinematics_true\n</code></pre> <p>use this:</p> <pre><code>k = car_state.kinematics_estimated\n\n# or\n\nk = client.simGetGroundTruthKinematics()\n</code></pre>"},{"location":"azure.html","title":"Cloud Development Environments","text":"<p>This document explains how to automate the creation of a <code>Microsoft Azure</code> cloud development environment (MSA-CDE) to code and debug a Python application connected to <code>AutonomySim</code> using Visual Studio Code.</p>"},{"location":"azure.html#automatically-deploy-the-azure-virtual-machine-vm","title":"Automatically Deploy the Azure virtual machine (VM)","text":"<p>Click the blue button to start the Azure deployment.</p> <p>Note</p> <p>The template is pre-filled with the recommended virtual machine size for the use cases of the following two tutorials.</p> <p> </p> <p>Note</p> <p>The VM deployment and configuration process may take over 20 minutes to complete.</p>"},{"location":"azure.html#regarding-the-deployment-of-the-azure-vm","title":"Regarding the deployment of the Azure VM","text":"<ul> <li> <p>When using an Azure Trial account, the default vCPU quota is not enough to allocate the required VM to run AutonomySim. If that's the case, you will see an error when trying to create the VM and will have to submit a request for Quota increase. Be sure to understand how and how much you are going to be charged for the use of the VM</p> </li> <li> <p>To avoid charges for the Virtual Machine usage while not in use, remember to deallocate its resources from the Azure Portal or use the following command from the Azure CLI:</p> </li> </ul> <pre><code>az vm deallocate --resource-group MyResourceGroup --name MyVMName\n</code></pre>"},{"location":"azure.html#code-and-debug-from-visual-studio-code-and-remote-ssh","title":"Code and debug from Visual Studio Code and Remote SSH","text":"<ul> <li>Install Visual Studio Code</li> <li>Install the Remote - SSH extension</li> <li>Press <code>F1</code> and run the <code>Remote - SSH: Connect to host...</code> command</li> <li>Add the recently create VM details. For instance, <code>AzureUser@11.22.33.44</code></li> <li>Run the <code>Remote - SSH: Connect to host...</code> command again, and now select the newly added connection.</li> <li>Once connected, click on the <code>Clone Repository</code> button in Visual Studio Code, and either clone this repository in the remote VM and open just the <code>azure</code> folder, or create a brand new repository, clone it and copy the contents of the <code>azure</code> folder from this repository in it. It is important to open that directory so Visual Studio Code can use the specific <code>.vscode</code> directory for the scenario and not the general AutonomySim <code>.vscode</code> directory. It contains the recommended extensions to install, the task to start AutonomySim remotely and the launch configuration for the Python application.</li> <li>Install all the recommended extensions</li> <li>Press <code>F1</code> and select the <code>Tasks: Run Task</code> option. Then, select the <code>Start AutonomySim</code> task from Visual Studio Code to execute the <code>start-AutonomySim.ps1</code> script from Visual Studio Code.</li> <li>Open the <code>multirotor.py</code> file inside the <code>app</code> directory</li> <li>Start debugging with Python</li> <li>When finished, remember to stop an deallocate the Azure VM to avoid extra charges</li> </ul>"},{"location":"azure.html#code-and-debug-on-local-and-connect-to-remote-via-forwarded-ports","title":"Code and debug on local and connect to remote via forwarded ports","text":"<p>Note</p> <p>In this scenario, we use two Visual Studio Code instances. The first will be used as a bridge to forward ports via SSH to the Azure VM and execute remote processes and the second will be used for local Python development. To be able to reach the VM from the local Python code, it is required to keep the <code>Remote - SSH</code> instance of Visual Studio Code opened while working with the local Python environment on the second instance.</p> <ul> <li>Open the first Visual Studio Code instance</li> <li>Follow the steps in the previous section to connect via <code>Remote - SSH</code></li> <li>In the Remote Explorer, add the port <code>41451</code> as a forwarded port to localhost</li> <li>Either run the <code>Start AutonomySim</code> task on the Visual Studio Code with the remote session as explained in the previous scenario or manually start the AutonomySim binary in the VM</li> <li>Open a second Visual Studio Code instance, without disconnecting or closing the first one</li> <li>Either clone this repository locally and open just the <code>azure</code> folder in Visual Studio Code, or create a brand new repository, clone it and copy the contents of the <code>azure</code> folder from this repository in it.</li> <li>Run <code>pip install -r requirements.txt</code> inside the <code>app</code> directory</li> <li>Open the <code>multirotor.py</code> file inside the <code>app</code> directory </li> <li>Start debugging with Python</li> <li>When finished, remember to stop an deallocate the Azure VM to avoid extra charges</li> </ul>"},{"location":"azure.html#running-with-docker","title":"Running with Docker","text":"<p>Once both the AutonomySim environment and the Python application are ready, you can package everything as a Docker image. The sample project inside the <code>azure</code> directory is already prepared to run a prebuilt AutonomySim binary and Python code using Docker.</p> <p>This would be a perfect scenario when you want to run the simulation at scale. For instance, you could have several different configurations for the same simulation and execute them in a parallel, unattended way using a Docker image on Azure Container Services</p> <p>Since AutonomySim requires access to the host GPU, it is required to use a Docker runtime that supports it. For more information about running AutonomySim in Docker, click here.</p> <p>When using Azure Container Services to run this image, the only extra-requirement is to add GPU support to the Container Group where it will be deployed. </p> <p>It can use either public docker images from DockerHub or images deployed to a private Azure Container Registry</p>"},{"location":"azure.html#building-the-docker-image","title":"Building the docker image","text":"<pre><code>docker build -t &lt;your-registry-url&gt;/&lt;your-image-name&gt; -f ./docker/Dockerfile .`\n</code></pre>"},{"location":"azure.html#using-a-different-autonomysim-binary","title":"Using a different AutonomySim binary","text":"<p>To use a different <code>AutonomySim</code> binary, first check the official documentation on How to Build AutonomySim on Windows and How to Build AutonomySim on Linux if you also want to run it with Docker</p> <p>Once you have a zip file with the new AutonomySim environment (or prefer to use one from the Official Releases), you need to modify some of the scripts in the <code>azure</code> directory of the repository to point to the new environment:</p> <ul> <li>In <code>azure/azure-env-creation/configure-vm.ps1</code>, modify all the parameters starting with <code>$AutonomySimBinary</code> with the new values</li> <li>In <code>azure/start-AutonomySim.ps1</code>, modify <code>$AutonomySimExecutable</code> and <code>$AutonomySimProcessName</code> with the new values</li> </ul> <p>If you are using the docker image, you also need a Linux binary zip file and modify the following Docker-related files:</p> <ul> <li>In <code>azure/docker/Dockerfile</code>, modify the <code>AUTONOMYSIM_BINARY_ZIP_URL</code> and <code>AUTONOMYSIM_BINARY_ZIP_FILENAME</code> ENV declarations with the new values</li> <li>In <code>azure/docker/docker-entrypoint.sh</code>, modify <code>AUTONOMYSIM_EXECUTABLE</code> with the new value </li> </ul>"},{"location":"azure.html#maintaining-the-development-environment","title":"Maintaining the development environment","text":"<p>Several components of this development environment (ARM templates, initialization scripts and VSCode tasks) directly depend on the current directory structures file names and repository locations. When planning to modify/fork any of those, make sure to check every script and template to make any required adjustment.</p>"},{"location":"build_docs.html","title":"Generating the Documentation","text":"<p>Note</p> <p>This is only useful if you want to host your own <code>AutonomySim</code> documentation site (e.g., in a secure enclave).</p> <p>The <code>AutonomySim</code> documentation website HTML and CSS files are automatically generated from Markdown files and deployed to GitHub Pages using the <code>mkdocs</code> package for Python. You can also self-host the documentation or redirect GitHub Pages to your own domain. The choice is yours.</p> <p>Compared to <code>AirSim</code> and its forks, we offer simpler and cleaner documentation in a modern style.</p>"},{"location":"build_docs.html#configure-github-repository","title":"Configure GitHub Repository","text":"<p>Configure the GitHub repository to automatically deploy documentation to a GitHub project site from the <code>gh-pages</code> branch:</p> <ul> <li>Web browser \u2192 GitHub repository URL</li> <li><code>Settings</code> \u2192 <code>Build and deployment</code></li> <li><code>Source</code> \u2192 <code>Deploy from a branch</code></li> <li><code>Branch</code> \u2192 <code>gh-pages/(root)</code></li> <li><code>Save</code></li> </ul> <p>For more information on this process, see the GitHub Pages documentation.</p>"},{"location":"build_docs.html#install-python-mkdocs-and-mkdocstrings","title":"Install Python <code>mkdocs</code> and <code>mkdocstrings</code>","text":"<p>Using a package manager such as Anaconda or Mamba, install <code>mkdocs</code>:</p> <pre><code>micromamba create -n mkdocs python=3.11 pip\nmicromamba activate mkdocs\npip install mkdocs mkdocs-material pymdown-extensions mkdocstrings[python]\nmkdocs --version\n</code></pre>"},{"location":"build_docs.html#edit-the-configuration-file","title":"Edit the Configuration File","text":"<pre><code>vim mkdocs.yaml\n</code></pre>"},{"location":"build_docs.html#build-and-deploy-documentation","title":"Build and Deploy Documentation","text":"<p>Build and deploy the documentation to GitHub Pages:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>The documentation should soon be viewable on GitHub Pages. For more information, see the <code>mkdocs</code> documentation here and here.</p>"},{"location":"build_faq.html","title":"AutonomySim Build FAQ","text":""},{"location":"build_faq.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>AutonomySim Build FAQ</li> <li>Table of Contents</li> <li>Windows Builds<ul> <li>How to force Unreal to use Visual Studio 2019?</li> <li>I get error: 'where' is not recognized as an internal or external command</li> <li>I'm getting error <code>&lt;MyProject&gt; could not be compiled. Try rebuilding from source manually</code></li> <li>I get <code>error C100 : An internal error has occurred in the compiler</code> when running build.cmd</li> <li>I get error \"'corecrt.h': No such file or directory\" or \"Windows SDK version 8.1 not found\"</li> <li>How do I use PX4 firmware with AutonomySim?</li> <li>I made changes in Visual Studio but there is no effect</li> <li>Unreal still uses VS2015 or I'm getting some link error</li> </ul> </li> <li>Linux Builds<ul> <li>I'm getting error <code>&lt;MyProject&gt; could not be compiled. Try rebuilding from source manually</code>.</li> <li>Unreal crashed! How do I know what went wrong?</li> <li>How do I use an IDE on Linux?</li> <li>Can I cross compile for Linux from a Windows machine?</li> <li>What compiler and stdlib does AutonomySim use?</li> <li>What version of CMake does the AutonomySim build use?</li> <li>Can I compile AutonomySim in BashOnWindows?</li> <li>Where can I find more info on running Unreal on Linux?</li> </ul> </li> <li>Other<ul> <li>Packaging a binary including the AutonomySim plugin</li> </ul> </li> </ul>"},{"location":"build_faq.html#windows-builds","title":"Windows Builds","text":""},{"location":"build_faq.html#how-to-force-unreal-to-use-visual-studio-2019","title":"How to force Unreal to use Visual Studio 2019?","text":"<p>If the default <code>update_from_git.cmd</code> file results in VS 2017 project, then you may need to run the <code>C:\\Program Files\\Epic Games\\UE_4.25\\Engine\\Binaries\\DotNET\\UnrealBuildTool.exe</code> tool manually, with the command line options <code>-projectfiles -project=&lt;your.uproject&gt;  -game -rocket -progress -2019</code>.</p> <p>If you are upgrading from 4.18 to 4.25 you may also need to add <code>BuildSettingsVersion.V2</code> to your <code>*.Target.cs</code> and <code>*Editor.Target.cs</code> build files, like this:</p> <pre><code>public AutonomySimNHTestTarget(TargetInfo Target) : base(Target)\n{\n  Type = TargetType.Game;\n  DefaultBuildSettings = BuildSettingsVersion.V2;\n  ExtraModuleNames.AddRange(new string[] { \"AutonomySimNHTest\" });\n}\n</code></pre> <p>You may also need to edit this file:</p> <pre><code>\"%APPDATA%\\Unreal Engine\\UnrealBuildTool\\BuildConfiguration.xml\n</code></pre> <p>And add this Compiler version setting:</p> <pre><code>&lt;Configuration xmlns=\"https://www.unrealengine.com/BuildConfiguration\"&gt;\n  &lt;WindowsPlatform&gt;\n    &lt;Compiler&gt;VisualStudio2019&lt;/Compiler&gt;\n  &lt;/WindowsPlatform&gt;\n&lt;/Configuration&gt;\n</code></pre>"},{"location":"build_faq.html#i-get-error-where-is-not-recognized-as-an-internal-or-external-command","title":"I get error: 'where' is not recognized as an internal or external command","text":"<p>You have to add <code>C:\\WINDOWS\\SYSTEM32</code> to your PATH enviroment variable.</p>"},{"location":"build_faq.html#im-getting-error-myproject-could-not-be-compiled-try-rebuilding-from-source-manually","title":"I'm getting error <code>&lt;MyProject&gt; could not be compiled. Try rebuilding from source manually</code>","text":"<p>This will occur when there are compilation errors. Logs are stored in <code>&lt;My-Project&gt;\\Saved\\Logs</code> which can be used to figure out the problem.</p> <p>A common problem could be Visual Studio version conflict, AutonomySim uses VS 2019 while UE is using VS 2017, this can be found by searching for <code>2017</code> in the Log file. In that case, see the answer above.</p> <p>If you have modified the AutonomySim plugin files, then you can right-click the <code>.uproject</code> file, select <code>Generate Visual Studio solution file</code> and then open the <code>.sln</code> file in VS to fix the errors and build again.</p>"},{"location":"build_faq.html#i-get-error-c100-an-internal-error-has-occurred-in-the-compiler-when-running-buildcmd","title":"I get <code>error C100 : An internal error has occurred in the compiler</code> when running build.cmd","text":"<p>We have noticed this happening with VS version <code>15.9.0</code> and have checked-in a workaround in AutonomySim code. If you have this VS version, please make sure to pull the latest AutonomySim code.</p>"},{"location":"build_faq.html#i-get-error-corecrth-no-such-file-or-directory-or-windows-sdk-version-81-not-found","title":"I get error \"'corecrt.h': No such file or directory\" or \"Windows SDK version 8.1 not found\"","text":"<p>Very likely you don't have Windows SDK installed with Visual Studio.</p>"},{"location":"build_faq.html#how-do-i-use-px4-firmware-with-autonomysim","title":"How do I use PX4 firmware with AutonomySim?","text":"<p>By default, AutonomySim uses its own built-in firmware called simple_flight. There is no additional setup if you just want to go with it. If you want to switch to using PX4 instead then please see this guide.</p>"},{"location":"build_faq.html#i-made-changes-in-visual-studio-but-there-is-no-effect","title":"I made changes in Visual Studio but there is no effect","text":"<p>Sometimes the Unreal + VS build system doesn't recompile if you make changes to only header files. To ensure a recompile, make some Unreal based cpp file \"dirty\" like AutonomySimGameMode.cpp.</p>"},{"location":"build_faq.html#unreal-still-uses-vs2015-or-im-getting-some-link-error","title":"Unreal still uses VS2015 or I'm getting some link error","text":"<p>Running several versions of VS can lead to issues when compiling UE projects. One problem that may arise is that UE will try to compile with an older version of VS which may or may not work. There are two settings in Unreal, one for for the engine and one for the project, to adjust the version of VS to be used.</p> <ol> <li>Edit -&gt; Editor preferences -&gt; General -&gt; Source code -&gt; Source Code Editor</li> <li>Edit -&gt; Project Settings -&gt; Platforms -&gt; Windows -&gt; Toolchain -&gt;CompilerVersion</li> </ol> <p>In some cases, these settings will still not lead to the desired result and errors such as the following might be produced: LINK : fatal error LNK1181: cannot open input file 'ws2_32.lib'</p> <p>To resolve such issues the following procedure can be applied:</p> <ol> <li>Uninstall all old versions of VS using the VisualStudioUninstaller</li> <li>Repair/Install VS 2019</li> <li>Restart machine and install Epic launcher and desired version of the engine</li> </ol>"},{"location":"build_faq.html#linux-builds","title":"Linux Builds","text":""},{"location":"build_faq.html#im-getting-error-myproject-could-not-be-compiled-try-rebuilding-from-source-manually_1","title":"I'm getting error <code>&lt;MyProject&gt; could not be compiled. Try rebuilding from source manually</code>.","text":"<p>This could either happen because of compile error or the fact that your gch files are outdated. Look in to your console window. Do you see something like below?</p> <p><code>fatal error: file '/usr/include/linux/version.h''/usr/include/linux/version.h' has been modified since the precompiled header</code></p> <p>If this is the case then look for *.gch file(s) that follows after that message, delete them and try again. Here's relevant thread on Unreal Engine forums.</p> <p>If you see other compile errors in console then open up those source files and see if it is due to changes you made. If not, then report it as issue on GitHub.</p>"},{"location":"build_faq.html#unreal-crashed-how-do-i-know-what-went-wrong","title":"Unreal crashed! How do I know what went wrong?","text":"<p>Go to the <code>MyUnrealProject/Saved/Crashes</code> folder and search for the file <code>MyProject.log</code> within its subdirectories. At the end of this file you will see the stack trace and messages. You can also take a look at the <code>Diagnostics.txt</code> file.</p>"},{"location":"build_faq.html#how-do-i-use-an-ide-on-linux","title":"How do I use an IDE on Linux?","text":"<p>You can use Qt Creator or CodeLite. Instructions for Qt Creator are available here.</p>"},{"location":"build_faq.html#can-i-cross-compile-for-linux-from-a-windows-machine","title":"Can I cross compile for Linux from a Windows machine?","text":"<p>Yes, you can, but we haven't tested it. You can find the instructions here.</p>"},{"location":"build_faq.html#what-compiler-and-stdlib-does-autonomysim-use","title":"What compiler and stdlib does AutonomySim use?","text":"<p>We use the same compiler that Unreal Engine uses, Clang 8, and stdlib, libc++. AutonomySim's <code>setup.sh</code> will automatically download them.</p>"},{"location":"build_faq.html#what-version-of-cmake-does-the-autonomysim-build-use","title":"What version of CMake does the AutonomySim build use?","text":"<p>3.10.0 or higher. This is not the default in Ubuntu 16.04 so setup.sh installs it for you. You can check your CMake version using <code>cmake --version</code>. If you have an older version, follow these instructions or see the CMake website.</p>"},{"location":"build_faq.html#can-i-compile-autonomysim-in-bashonwindows","title":"Can I compile AutonomySim in BashOnWindows?","text":"<p>Yes, however, you can't run Unreal from BashOnWindows. So this is kind of useful to check a Linux compile, but not for an end-to-end run. See the WSL install guide. Make sure to have the latest version (Windows 10 Creators Edition) as previous versions had various issues. Also, don't invoke <code>bash</code> from <code>Visual Studio Command Prompt</code>, otherwise CMake might find VC++ and try and use that!</p>"},{"location":"build_faq.html#where-can-i-find-more-info-on-running-unreal-on-linux","title":"Where can I find more info on running Unreal on Linux?","text":"<p>Start here: Unreal on Linux Building Unreal on Linux Unreal Linux Support Unreal Cross Compilation</p>"},{"location":"build_faq.html#other","title":"Other","text":""},{"location":"build_faq.html#packaging-a-binary-including-the-autonomysim-plugin","title":"Packaging a binary including the AutonomySim plugin","text":"<p>In order to package a custom environment with the AutonomySim plugin, there are a few project settings that are necessary for ensuring all required assets needed for AutonomySim are included inside the package. Under <code>Edit -&gt; Project Settings... -&gt; Project -&gt; Packaging</code>, please ensure the following settings are configured properly:</p> <ul> <li><code>List of maps to include in a packaged build</code>: ensure one entry exists for <code>/AutonomySim/AutonomySimAssets</code> </li> <li><code>Additional Asset Directories to Cook</code>: ensure one entry exists for <code>/AutonomySim/HUDAssets</code></li> </ul>"},{"location":"build_linux.html","title":"Build AutonomySim on Linux","text":"<p>The current recommended and tested environment is Ubuntu 18.04 LTS. Theoretically, you can build on other distros as well, but we haven't tested it.</p> <p>There are two options:</p> <ol> <li>Build inside Docker containers</li> <li>Build on your host machine</li> </ol>"},{"location":"build_linux.html#docker","title":"Docker","text":"<p>Please see instructions here</p>"},{"location":"build_linux.html#host-machine","title":"Host machine","text":""},{"location":"build_linux.html#pre-build-setup","title":"Pre-build Setup","text":""},{"location":"build_linux.html#build-unreal-engine","title":"Build Unreal Engine","text":"<ul> <li>Make sure you are registered with Epic Games. This is required to get source code access for Unreal Engine.</li> <li>Clone Unreal in your favorite folder and build it (this may take a while!). Note: We only support Unreal &gt;= 4.27 at present. We recommend using 4.27.</li> </ul> <pre><code># go to the folder where you clone GitHub projects\ngit clone -b 4.27 git@github.com:EpicGames/UnrealEngine.git\ncd UnrealEngine\n./Setup.sh\n./GenerateProjectFiles.sh\nmake\n</code></pre>"},{"location":"build_linux.html#build-autonomysim","title":"Build AutonomySim","text":"<ul> <li>Clone AutonomySim and build it:</li> </ul> <pre><code># go to the folder where you clone GitHub projects\ngit clone https://github.com/nervosys/AutonomySim.git\ncd AutonomySim\n</code></pre> <p>By default AutonomySim uses clang 8 to build for compatibility with UE 4.27. The setup script will install the right version of cmake, llvm, and eigen.</p> <pre><code>./setup.sh\n./build.sh\n# use ./build.sh --debug to build in debug mode\n</code></pre>"},{"location":"build_linux.html#build-unreal-environment","title":"Build Unreal Environment","text":"<p>Finally, you will need an Unreal project that hosts the environment for your vehicles. AutonomySim comes with a built-in \"Blocks Environment\" which you can use, or you can create your own. Please see setting up Unreal Environment if you'd like to setup your own environment.</p>"},{"location":"build_linux.html#how-to-use-autonomysim","title":"How to Use AutonomySim","text":"<p>Once AutonomySim is setup:</p> <ul> <li>Go to <code>UnrealEngine</code> installation folder and start Unreal by running <code>./Engine/Binaries/Linux/UE4Editor</code>.</li> <li>When Unreal Engine prompts for opening or creating project, select Browse and choose <code>AutonomySim/Unreal/Environments/Blocks</code> (or your custom Unreal project).</li> <li>Alternatively, the project file can be passed as a commandline argument. For Blocks: <code>./Engine/Binaries/Linux/UE4Editor &lt;autonomysim_path&gt;/Unreal/Environments/Blocks/Blocks.uproject</code></li> <li>If you get prompts to convert project, look for More Options or Convert-In-Place option. If you get prompted to build, choose Yes. If you get prompted to disable AutonomySim plugin, choose No.</li> <li>After Unreal Editor loads, press Play button.</li> </ul> <p>See Using APIs and settings.json for various options available for AutonomySim usage.</p> <p>Tip</p> <p>Go to 'Edit-&gt;Editor Preferences', in the 'Search' box type 'CPU' and ensure that the 'Use Less CPU when in Background' is unchecked.</p>"},{"location":"build_linux.html#optional-setup-remote-control-multirotor-only","title":"[Optional] Setup Remote Control (Multirotor Only)","text":"<p>A remote control is required if you want to fly manually. See the remote control setup for more details.</p> <p>Alternatively, you can use APIs for programmatic control or use the so-called Computer Vision mode to move around using the keyboard.</p>"},{"location":"build_macos.html","title":"Build AutonomySim on macOS","text":"<p>Only macOS Catalina (10.15) has currently been tested. Theoretically, AutonomySim should work on higher macOS versions and Apple Silicon hardware, but this path is not offically supported.</p> <p>There are two options:</p> <ol> <li>Build inside Docker containers</li> <li>Build on your host machine</li> </ol>"},{"location":"build_macos.html#docker","title":"Docker","text":"<p>Please see instructions here</p>"},{"location":"build_macos.html#host-machine","title":"Host machine","text":""},{"location":"build_macos.html#pre-build-setup","title":"Pre-build Setup","text":""},{"location":"build_macos.html#download-unreal-engine","title":"Download Unreal Engine","text":"<ol> <li>Download the Epic Games Launcher. While the Unreal Engine is open source and free to download, registration is still required.</li> <li>Run the Epic Games Launcher, open the <code>Library</code> tab on the left pane.    Click on the <code>Add Versions</code> which should show the option to download Unreal 4.27 as shown below. If you have multiple versions of Unreal installed then make sure 4.27 is set to <code>current</code> by clicking down arrow next to the Launch button for the version.</li> </ol> <p>Note: AutonomySim also works with UE &gt;= 4.24, however, we recommend 4.27.    Note: If you have UE 4.16 or older projects, please see the upgrade guide to upgrade your projects.</p>"},{"location":"build_macos.html#build-autonomysim","title":"Build AutonomySim","text":"<ul> <li>Clone AutonomySim and build it:</li> </ul> <pre><code># go to the folder where you clone GitHub projects\ngit clone https://github.com/nervosys/AutonomySim.git\ncd AutonomySim\n</code></pre> <p>By default AutonomySim uses <code>clang-8</code> to build for compatibility with UE 4.25. The setup script will install the right version of <code>cmake</code>, <code>llvm</code>, and <code>eigen</code>.</p> <p>CMake 3.19.2 is required for building on Apple silicon.</p> <pre><code>./setup.sh\n./build.sh\n# use ./build.sh --debug to build in debug mode\n</code></pre>"},{"location":"build_macos.html#build-unreal-environment","title":"Build Unreal Environment","text":"<p>Finally, you will need an Unreal project that hosts the environment for your vehicles. AutonomySim comes with a built-in \"Blocks Environment\" which you can use, or you can create your own. Please see setting up Unreal Environment if you'd like to setup your own environment.</p>"},{"location":"build_macos.html#how-to-use-autonomysim","title":"How to Use AutonomySim","text":"<ul> <li>Browse to <code>AutonomySim/Unreal/Environments/Blocks</code>.</li> <li>Run <code>./GenerateProjectFiles.sh &lt;UE_PATH&gt;</code> from the terminal, where <code>UE_PATH</code> is the path to the Unreal installation folder. (By default, this is <code>/Users/Shared/Epic\\ Games/UE_4.27/</code>) The script creates an XCode workspace by the name Blocks.xcworkspace.</li> <li>Open the XCode workspace, and press the Build and run button in the top left.</li> <li>After Unreal Editor loads, press Play button.</li> </ul> <p>See Using APIs and settings.json for various options available for AutonomySim usage.</p> <p>Tip</p> <p>Go to 'Edit-&gt;Editor Preferences', in the 'Search' box type 'CPU' and ensure that the 'Use Less CPU when in Background' is unchecked.</p>"},{"location":"build_macos.html#optional-setup-remote-control-multirotor-only","title":"[Optional] Setup Remote Control (Multirotor Only)","text":"<p>A remote control is required if you want to fly manually. See the remote control setup for more details.</p> <p>Alternatively, you can use APIs for programmatic control or use the so-called Computer Vision mode to move around using the keyboard.</p>"},{"location":"build_windows.html","title":"Build AutonomySim on Windows","text":""},{"location":"build_windows.html#install-unreal-engine-5","title":"Install Unreal Engine 5","text":"<ol> <li>Download the Epic Games Launcher. Although Unreal Engine (UE) is open-source, registration is required to access the source code on GitHub. To do so, you will need to be accepted into the Epic Games organization.</li> <li>Run the <code>Epic Games Launcher</code> program.</li> <li>If this is the first time you have installed Unreal Engine, click on the big yellow button at the top-right of the main window and select <code>Install Engine</code>. Skip ahead to step 7.</li> <li>Otherwise, select <code>Unreal Engine</code> on the left window pane.</li> <li>Select the <code>Library</code> tab at the top of the main window.</li> <li>Click on the <code>+</code> button next to <code>ENGINE VERSIONS</code> at the top-left to install a new engine version.</li> <li>Select the desired engine version (e.g., 5.3.2) by clicking the down-arrow next to the version number.</li> <li>Click the <code>Install</code> button below the version number.</li> <li>If you have multiple versions installed, click the down-arrow next to the version number and select <code>Set Current</code>.</li> </ol> <p>Congratulations! Unreal Engine is now installed and ready to use.</p> <p>Note</p> <p>If you created projects with UE 4.16 or older, see the upgrade guide to upgrade your projects.</p> <p></p> <p></p>"},{"location":"build_windows.html#install-visual-studio-2022","title":"Install Visual Studio 2022","text":"<ul> <li>Download the Visual Studio (VS) 2022 installer</li> <li>Install Visual Studio 2022 with the following optional Workloads:</li> <li>.NET desktop development</li> <li>Desktop development with C++</li> <li>Universal Windows Platform development</li> <li>Game development with C++</li> <li>For the Game development with C++ Workload, ensure the following are selected to install:</li> <li>C++ profiling tools</li> <li>C++ AddressSanitizer</li> <li>Windows 10 SDK (10.0.18362 or Newer)</li> <li>Unreal Engine installer </li> </ul>"},{"location":"build_windows.html#build-autonomysim-from-source","title":"Build AutonomySim from Source","text":"<ul> <li>Open Visual Studio 2022</li> <li>Open one of the below VS developer shell environments, depending on your preferred language:</li> <li><code>Tools &gt; Command Line &gt; Developer Command Prompt</code></li> <li><code>Tools &gt; Command Line &gt; Developer PowerShell</code></li> <li>Ensure you have CMake version 3.14 or greater installed:</li> <li><code>cmake --version</code></li> <li>If not, download and install the latest CMake.</li> <li>Clone the AutonomySim git repository:</li> <li><code>git clone https://github.com/nervosys/AutonomySim.git</code></li> <li>Enter the AutonomySim directory:</li> <li><code>cd AutonomySim</code></li> <li>Run one of the below build scripts from the command line, depending on your preferred language. This script generates ready-to-use Unreal Engine Plugin components in the <code>Unreal\\Plugins</code> directory, which can be copied into any Unreal project.</li> <li><code>.\\scripts\\build.cmd</code></li> <li><code>.\\scripts\\build.ps1</code></li> <li><code>./scripts/build.sh</code></li> </ul> <p>Note</p> <p>We are actively porting the DOS-era Windows batch (.bat) and command (.cmd) scripts to PowerShell (.ps1), as it offers modern features such as cross-platform support, unicode text encoding, and system object piping. Linux and MacOS benefit from supporting a common language, BASH. While MacOS now uses Zsh for its default shell, it is backwards compatible with BASH. Eventually, we may only support PowerShell or BASH (or maybe Batsh) on all platforms.</p> <p>Note</p> <p>Installing AutonomySim on the <code>C:\\</code> drive may cause scripts to fail and may also require running VS in Admin mode. If possible, clone the project into a directory on a different drive. If not, ensure correct behaviour.</p>"},{"location":"build_windows.html#build-an-unreal-project","title":"Build an Unreal Project","text":"<p>Next, you will need an Unreal project to host an environment for your vehicles. Close and re-open <code>Unreal Engine</code> and <code>Epic Games Launcher</code> before building your first environment. After restarting <code>Epic Games Launcher</code>, it may ask if you want to associate Unreal project files with <code>Unreal Engine</code>. Click on <code>fix now</code> to do so. While AutonomySim includes the pre-built <code>Blocks</code> environment, you can also create new environments. For more information, see the Unreal environment setup guide or learn more about these virtual worlds on the Epic Games website here.</p>"},{"location":"build_windows.html#setup-a-remote-control","title":"Setup a Remote Control","text":"<p>Note</p> <p>The below only applies to multi-rotor drones.</p> <p>To fly drones manually, a physical (or software-emulated) controller is required. For more information, see the remote control setup guide. Alternatively, you may (a) wrap application programming interfaces (APIs) calls for software control or (b) use the computer vision mode for manual keyboard control.</p>"},{"location":"build_windows.html#how-to-use-autonomysim","title":"How to Use AutonomySim","text":"<p>Once AutonomySim is set up by following above steps, you can,</p> <ol> <li>Double click on .sln file to load the Blocks project in <code>Unreal\\Environments\\Blocks</code> (or .sln file in your own custom Unreal project). If you don't see .sln file then you probably haven't completed steps in Build Unreal Project section above.</li> </ol> <p>Note</p> <p>Unreal 4.27 will auto-generate the .sln file targetting Visual Studio 2019. Visual Studio 2022 will be able to load and run this .sln, but if you want full Visual Studio 2022 support, you will need to explicitly enable support by going to 'Edit-&gt;Editor Preferences-&gt;Source Code' and selecting 'Visual Studio 2022' for the 'Source Code Editor' setting.</p> <ol> <li>Select your Unreal project as Start Up project (for example, Blocks project) and make sure Build config is set to \"Develop Editor\" and x64.</li> <li>After Unreal Editor loads, press Play button. </li> </ol> <p>Tip</p> <p>Go to 'Edit-&gt;Editor Preferences', in the 'Search' box type 'CPU' and ensure that the 'Use Less CPU when in Background' is unchecked.</p> <p>See Using APIs and settings.json for various options available.</p>"},{"location":"camera_views.html","title":"Camera Views","text":"<p>The camera views shown on-screen are the image streams that can be fetched via the Image APIs.</p> <p></p> <p>From left to right is the depth view, segmentation view and the FPV view. See Image APIs for description of various available views.</p>"},{"location":"camera_views.html#turning-onoff-views","title":"Turning ON/OFF Views","text":"<p>Press F1 key to see keyboard shortcuts for turning on/off any or all views. You can also select various view modes there, such as \"Fly with Me\" mode, FPV mode and \"Ground View\" mode.</p>"},{"location":"camera_views.html#controlling-manual-camera","title":"Controlling Manual Camera","text":"<p>You can switch to manual camera control by pressing the M key. While manual camera control mode is selected, you can use the following keys to control the camera:</p> Key Action Arrow keys move the camera forward/back and left/right Page up/down move the camera up/down W/A/S/D control pitch up/down and yaw left/right Left shift increase movement speed Left control decrease movement speed"},{"location":"camera_views.html#configuring-sub-windows","title":"Configuring Sub-Windows","text":"<p>Now you can select what is shown by each of above sub windows. For instance, you can chose to show surface normals in first window (instead of depth) and disparity in second window (instead of segmentation). Below is the settings value you can use in settings.json:</p> <pre><code>{\n  \"SubWindows\": [\n    {\"WindowID\": 1, \"CameraName\": \"0\", \"ImageType\": 5, \"VehicleName\": \"\", \"Visible\": false},\n    {\"WindowID\": 2, \"CameraName\": \"0\", \"ImageType\": 3, \"VehicleName\": \"\", \"Visible\": false}\n  ]\n}\n</code></pre>"},{"location":"camera_views.html#performance-impact","title":"Performance Impact","text":"<p>Note</p> <p>This section is outdated and has not been updated for new performance enhancement changes.</p> <p>Now rendering these views does impact the FPS performance of the game, since this is additional work for the GPU.  The following shows the impact on FPS when you open these views.</p> <p></p> <p>This is measured on <code>Intel Core i7</code> computer with 32 gb RAM and a <code>GeForce GTX 1080</code> graphics card running the Modular Neighborhood map, using cooked debug bits, no debugger or GameEditor open.  The normal state with no subviews open is measuring around 16 ms per frame, which means it is keeping a nice steady 60 FPS (which is the target FPS).  As it climbs up to 35ms the FPS drops to around 28 frames per second, spiking to 40ms means a few drops to 25 fps.</p> <p>The simulator can still function and fly correctly when all this is going on even in the worse case because the physics is decoupled from the rendering.  However if the delay gets too high such that the communication with PX4 hardware is interrupted due to overly busy CPU then the flight can stall due to timeout in the offboard control messages.</p> <p>On the computer where this was measured the drone could fly the <code>path.py</code> program without any problems with all views open, and with 3 python scripts running to capture each view type.  But there was one stall during this flight, but it recovered gracefully and completed the path. So it was right on the limit.</p> <p>The following shows the impact on CPU, perhaps a bit surprisingly, the CPU impact is also non trivial.</p> <p></p>"},{"location":"cmake_linux.html","title":"Installing CMake on Linux","text":"<p>If you don't have CMake version 3.10 or greater, you can run the following to install it:</p> <pre><code>mkdir ~/cmake-3.10.2\ncd ~/cmake-3.10.2\nwget https://cmake.org/files/v3.10/cmake-3.10.2-Linux-x86_64.sh\n</code></pre> <p>Now, you have to run this command by itself (it is interactive):</p> <pre><code>sh cmake-3.10.2-Linux-x86_64.sh --prefix ~/cmake-3.10.2\n</code></pre> <p>Answer <code>n</code> to the question about creating another <code>cmake-3.10.2-Linux-x86_64</code> folder and then this:</p> <pre><code>sudo update-alternatives --install /usr/bin/cmake cmake ~/cmake-3.10.2/bin/cmake 60\n</code></pre> <p>Now type <code>cmake --version</code> to ensure your CMake version is equal to <code>3.10.2</code>.</p>"},{"location":"controller_remote.html","title":"Remote Control","text":"<p>To fly drones manually, a physical (or software-emulated) controller is required. Alternatively, you may (a) wrap application programming interfaces (APIs) calls for software control or (b) use the computer vision mode for manual keyboard control.</p>"},{"location":"controller_remote.html#default-configuration","title":"Default Configuration","text":"<p>By default, AutonomySim uses the simple flight controller, which connects your computer to a physical controller via USB port.</p>"},{"location":"controller_remote.html#controller-notes","title":"Controller Notes","text":"<p>To date, XBox and FrSky Taranis X9D Plus controllers have been verified as supported. Other controllers from Microsoft, Sony, Logitech, FrSky/Turnigy, Spektrum, Futaba, and TBS may also work. If you have a remote control (RC) or radio-frequency (RF) controller that lacks USB support, you will need to convert the signal to USB. Such converters are often called trainer cables or dongles. Learn how to make your own here. If you have a Steam Deck, which runs a flavor of Arch Linux (SteamOS), you may be able to run AutonomySim directly on your controller.</p> <p>AutonomySim can detect large variety of devices. However, devices other than those listed above may require extra configuration. In the future, we may add relared configuration options in the <code>settings.json</code> file. If your controller does not work, we recommend trying workarounds such as x360ce or modifying the SimJoystick.cpp file.</p> <p>Note</p> <p>If a realistic experience is desired, the XBox 360 controller is not recommended as it has insufficient potentiometer encoding precision. For more information, see the FAQ below.</p>"},{"location":"controller_remote.html#frsky-taranis-x9d-plus","title":"FrSky Taranis X9D Plus","text":"<p>The FrSky Taranis X9D Plus is a modern RC controller with a USB port so that it can directly connect to PCs. Download the AutonomySim config file and follow this tutorial to import it into your RC. You should then see <code>sim</code> model in the RC controller with all channels properly configured. As this controller runs the open-source OpenTX transmitter software, it may be adapted to other controllers as well.</p>"},{"location":"controller_remote.html#linux","title":"Linux","text":"<p>The current default configuation on Linux uses an Xbox controller. Other controllers may not properly function. In the future, we may add the ability to configure RC controllers in the <code>settings.json</code> file. For now, you may have to modify the SimJoystick.cpp file to support other devices.</p>"},{"location":"controller_remote.html#controller-configuration-for-px4","title":"Controller Configuration for PX4","text":"<p>AutonomySim supports the PX4 flight control system (FCS), typically run on a Pixhawk flight control unit (FCU). However, PX4 requires additional setup. There are many remote control options available for multi-rotor aircraft. We have successfully used the FrSky Taranis X9D Plus, FlySky FS-TH9X, and Futaba 14SG RC controllers with AutonomySim. The steps to configure your RC controller are as follows:</p> <ol> <li>If you are going to use hardware-in-the-loop (HITL) mode, as opposed to software-in-the-loop (SITL) mode, you need a compatible receiver to bind to your RC transmitter. For more information, see the manual for your RC controller.</li> <li>For hardware-in-the-loop (HITL) mode, connect the transmitter directly to Pixhawk. View the online documentation and/or YouTube tutorials on how to do so.</li> <li>Calibrate your controller in QGroundControl.</li> </ol> <p>Please see the PX4 controller configuration and this guide for more information. </p>"},{"location":"controller_remote.html#xbox-360-controller","title":"XBox 360 Controller","text":"<p>You can also use an xbox controller in SITL mode, it just won't be as precise as a real RC controller. See xbox controller for details on how to set that up.</p>"},{"location":"controller_remote.html#playstation-3-controller","title":"Playstation 3 Controller","text":"<p>A Playstation 3 controller is confirmed to work as an AutonomySim controller. On Windows, an emulator to make it look like an Xbox 360 controller, is required however. Many different solutions are available online, for example x360ce Xbox 360 Controller Emulator.</p>"},{"location":"controller_remote.html#dji-controller","title":"DJI Controller","text":"<p>Nils Tijtgat wrote an excellent blog on how to get the DJI controller working with AutonomySim.</p>"},{"location":"controller_remote.html#faq","title":"FAQ","text":""},{"location":"controller_remote.html#autonomysim-says-my-usb-controller-is-not-detected","title":"AutonomySim says my USB controller is not detected","text":"<p>This typically happens if you have multiple RCs and or XBox/Playstation gamepads etc connected. In Windows, hit Windows+S key and search for \"Set up USB Game controllers\" (in older versions of Windows try \"joystick\"). This will show you all game controllers connected to your PC. If you don't see yours than Windows haven't detected it and so you need to first solve that issue. If you do see yours but not at the top of the list (i.e. index 0) than you need to tell AutonomySim because AutonomySim by default tries to use RC at index 0. To do this, navigate to your <code>~/Documents/AutonomySim</code> folder, open up <code>settings.json</code> and add/modify following setting. Below tells AutonomySim to use RC at <code>index = 2</code>.</p> <pre><code>{\n    \"SettingsVersion\": 1.2,\n    \"SimMode\": \"Multirotor\",\n    \"Vehicles\": {\n        \"SimpleFlight\": {\n            \"VehicleType\": \"SimpleFlight\",\n            \"RC\": {\n              \"RemoteControlID\": 2\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"controller_remote.html#the-vehicle-is-unstable-when-using-xboxps3-controller","title":"The vehicle is unstable when using XBox/PS3 controller","text":"<p>Regular gamepads are not very precise and have lot of random noise. Most of the times you may see significant offsets as well (i.e. output is not zero when sticks are at zero). So this behavior is expected.</p>"},{"location":"controller_remote.html#where-is-the-rc-controller-calibration-utility-in-autonomysim","title":"Where is the RC controller calibration utility in AutonomySim?","text":"<p>We haven't implemented it yet. This means your RC firmware will need to have a capability to do calibration for now.</p>"},{"location":"controller_remote.html#the-rc-controller-is-not-working-with-px4","title":"The RC controller is not working with PX4","text":"<p>First, ensure your RC controller is working in QGroundControl. If it doesn't then it will sure not work in AutonomySim. The PX4 mode is suitable for folks who have at least intermediate level of experience to deal with various issues related to PX4 and we would generally refer you to get help from PX4 forums.</p>"},{"location":"controller_robot.html","title":"Robot Controller","text":""},{"location":"controller_robot.html#what-is-a-robot-controller","title":"What is a Robot Controller?","text":"<p>A robot controller is any device or computer program used to control a robot. Robot controllers generally take one of five forms:</p> <ol> <li>Human wetware controller</li> <li>Handheld remote control (RC) or radio frequency (RF) controller<ul> <li>ACSST/FASST/DSM2/ELRS/Crossfire/Tracer, ZigBee, LoRaWAN, WiFi HaLow, or 5G/6G handheld controller</li> <li>Protocol-agnostic software-defined radio (SDR) handheld controller</li> </ul> </li> <li>Handheld hard-wired controller<ul> <li>Serial/USB game-pad, flight-stick, or keyboard controller</li> <li>Ethernet pendant controller (common for industrial robotic arms, where the pendant also performs computation)</li> </ul> </li> <li>Computer software controller</li> <li>A software architecture or control system</li> <li>ArduPilot, PX4, BetaPilot, OpenPilot, dRehmFlight, etc.</li> <li>Computer hardware controller</li> <li>Runs a computer software control system</li> <li>ArduPilot Mega (APM) 2.6, Pixhawk V6X, VOXL 2, SpeedyBee F405, Blade F722, GOKU GN745, RUSH Core 7, Naze32, DJI Naza, etc.</li> <li>Mechanical hardware controller</li> <li>The earliest autopilot systems were purely mechanical</li> <li>Many assembly lines or continuous-flow production machines still use mechanical controllers</li> <li>Hybrid controller</li> <li>Most controllers today are human-assisted computer controllers</li> <li>Examples include the DJI RC Pro Remote Controller and Skydio Controller, which are handheld computers, as well as traditional fly-by-wire and stabilization modes</li> <li>Human-in-the-loop (HITL) or human-computer interaction (HCI) for semi-autonomous systems</li> <li>Robots are fundamentally cyberphysical systems</li> <li>Remote Controller</li> <li>Any controller operated ex situ rather than in situ</li> </ol> <p>While (1) involves manual control, (2-4) involve automatic control, (5) involves a semi-automatic control, and (6) involves a variety of control approaches.</p> <p>Other types of controllers, such as electrochemical controllers (as in the nervous system) and chemosensory controllers (as in pheromone signaling), remain theoretical. We do, however, use magnetic fields for navigation and thus control, similar to a wide variety of animal species.</p>"},{"location":"controller_robot.html#what-is-a-flight-controller","title":"What is a Flight Controller?","text":"<p>A flight controller is simply a robot controller for the airborne or atmospheric domain. Colloquially, the term 'flight controller' commonly refers to a flight control unit (FCU) or computer, such as a microcontroller, running flight control software on a real-time operating system (RTOS). The flight control hardware and software together comprise a flight control system (FCS). Alternatively, the term 'flight controller' may also refer to a handheld RC transmitter, used to control aircraft for over a century (Tesla demonstrated radio control in 1898, following the 1870 wired demonstration by Siemens).</p>"},{"location":"controller_robot.html#which-other-domains-exist-for-robot-controllers","title":"Which Other Domains Exist for Robot Controllers?","text":"<p>There are robot controllers for every domain on Earth and beyond:</p> <ul> <li>Air</li> <li>Ground</li> <li>Surface (water)</li> <li>Subsurface (water)</li> <li>Space</li> <li>Other Worlds</li> </ul> <p>These domains can be found in common robotic system classifications:</p> <ul> <li>Unmanned Aerial Vehicle (UAV)</li> <li>Unmanned Ground Vehicle (UGV)</li> <li>Unmanned Surface Vehicle (USV)</li> <li>Unmanned Underwater Vehicle (UUV)</li> <li>Unmanned Spacecraft (US)</li> </ul>"},{"location":"controller_robot.html#why-do-i-need-a-robot-controller-for-a-simulator","title":"Why Do I Need a Robot Controller for a Simulator?","text":"<p>Robots are empty vassels without a sense of agency or goal-directed behaviour. Breaking these plans down into a series of actions further necessitates a control system to translate the human and/or computer inputs into desired outputs or behaviour. The primary job of the control system is to estimate the current state from sensor data, determine the desired future state and required control inputs to optimally converge toward this state, and translate control inputs into actuator and robot motion.</p> <pre><code>%%{init: {\"graph\": {\"htmlLabels\": false}}}%%\ngraph LR;\n\n   S_previous[\"` $State_{t-1}$ `\"];\n\n   A[State Previous (t-1)];\n   B[State Physical (t0)];\n   C[Sensor Noise];\n   D[Sensors Current (t0)];\n   E[State Estimator];\n   F[State Current (t0)];\n   G[Path Planner];\n   H[State Future (t+1)];\n   I[Actuator Commands];\n   J[Actuator Noise];\n   K[Actuator Future];\n\n   State Previous (t_-1);\n   State Physical + Encoder Noise --&gt; Sensors Current;\n   State Previous + Sensors Current + State Estimator --&gt; State Current (t_0);\n   State Current + Path Planner --&gt; State Desired (t_+1);\n   State Current + Actuator Commands --&gt; State Future;\n\n    A --&gt; B;\n    B --&gt; C;\n    C --&gt; D;\n    D --&gt; E;\n</code></pre> <p>Where sensors noise is related to resolution, precision, latency, hysteresis, thermal, optical, electromagnetic, and model error.</p> <p><code>$S_{t-1}$</code></p> <p>The state estimator is commonly a form of K\u00e1lm\u00e1n filter (not to be confused with his contemporary campatriot, von K\u00e1rm\u00e1n):</p> <pre><code>\\mathrm{Initial State}\n\\mathbf{x}_0 P_0\n\n\\mathrm{Prediction}\n\\mathbf{x}_{k+1}^{(P)} &amp;= A \\mathbf{x}_k + B {\\color{orange} a_k} P_{k+1}^{(P)} &amp;= A P_k A^\\tran + C_k^{(r_s)}\n\n\\mathrm{Update}\nk \\leftarrow k + 1\n\n\\mathrm{Innovation}\nK_k &amp;= P_k^{(P)} H^\\tran {\\left (H P_k^{(P)} H^\\tran + C_k^{(r_m)} \\right)}^{-1} {\\color{blue} \\mathbf{x}_k} &amp;= (I - K_k H) \\mathbf{x}_k^{(P)} + K_k {\\color{orange} z_k}{\\color{blue} P_k} &amp;= (I - K_k H) P_k^{(P)}\n</code></pre> <p>For quadrotors, desired state can be specified as roll, pitch and yaw, for example. It then estimates actual roll, pitch and yaw using gyroscope and accelerometer. Then it generates appropriate motor signals so actual state becomes desired state. You can find more in-depth in our paper.</p>"},{"location":"controller_robot.html#how-do-robot-controllers-communicate-with-the-simulator","title":"How Do Robot Controllers Communicate with the Simulator?","text":"<p>Simulator consumes the motor signals generated by flight controller to figure out force and thrust generated by each actuator (i.e. propellers in case of quadrotor). This is then used by the physics engine to compute the kinetic properties of the vehicle. This in turn generates simulated sensor data and feed it back to the flight controller. You can find more in-depth in our paper.</p>"},{"location":"controller_robot.html#what-is-hardware-in-the-loop-and-software-in-the-loop","title":"What is Hardware-in-the-loop and Software-in-the-loop?","text":"<p>Hardware-in-the-loop (HITL) means that the flight controller runs on physical hardware, such as an APM, Pixhawk, or Naze32 flight control unit (FCU). While some FCUs use real-time (<code>RT_PREEMPT</code>) Linux operating systems (OSes), typically running on a 64-bit ARM system-on-chip (SoC), most FCUs use hard real-time OSes running on microcontrollers.</p> <p>You then connect this hardware to PC using USB port. Simulator talks to the device to retrieve actuator signals and send it simulated sensor data. This is obviously as close as you can get to real thing. However, it typically requires more steps to set up and usually hard to debug. One big issue is that simulator clock and device clock runs on their own speed and accuracy. Also, USB connection (which is usually only USB 2.0) may not be enough for real-time communication.</p> <p>Software-in-the-loop (SITL) means that the flight controller software runs on your computer as opposed to separate board. This is generally fine except that now you are not touching any code paths that are specific to your device. Also, none of your code now runs with real-time clock usually provided by specialized hardware board. For well-designed flight controllers with software clock, these are usually not concerning issues.</p>"},{"location":"controller_robot.html#which-flight-controllers-are-supported","title":"Which Flight Controllers are Supported?","text":"<p>AutonomySim has built-in flight controller called simple_flight and it is used by default. You don't need to do anything to use or configure it. AutonomySim also supports PX4 &amp; ArduPilot as external flight controllers for advanced users.</p>"},{"location":"controller_robot.html#using-autonomysim-without-flight-controller","title":"Using AutonomySim Without Flight Controller","text":"<p>Yes, now it's possible to use AutonomySim without flight controller. Please see the instructions here for how to use so-called \"Computer Vision\" mode. If you don't need vehicle dynamics, we highly recommend using this mode.</p>"},{"location":"controller_wheel.html","title":"Steering Wheels","text":"<p>To use the <code>Logitech G920 steering wheel</code> with <code>AutonomySim</code> follow these steps:</p> <ol> <li> <p>Connect the steering wheel to the computer and wait until drivers installation complete.</p> </li> <li> <p>Install Logitech Gaming Software from here</p> </li> <li> <p>Before debug, you\u2019ll have to normalize the values in <code>AutonomySim</code> code. Perform this changes in <code>CarPawn.cpp</code> (according to the current update in the <code>git</code>):</p> </li> <li> <p>In line 382, change <code>Val</code> to <code>1 \u2013 Val</code>. (the complementary value in the range [0.0,1.0]).</p> </li> <li>In line 388, change <code>Val</code> to <code>5Val - 2.5</code> (Change the range of the given input from [0.0,1.0] to [-1.0,1.0]).</li> <li> <p>In line 404, change <code>Val</code> to <code>4(1 \u2013 Val)</code>. (the complementary value in the range [0.0,1.0]).</p> </li> <li> <p>Debug the <code>AutonomySim</code> project (while the steering wheel is connected; it\u2019s important).</p> </li> <li> <p>On Unreal Editor, go to <code>Edit-&gt;Plugins-&gt;Input Devices</code> and enable <code>Windows RawInput</code>.</p> </li> <li> <p>Go to <code>Edit-&gt;Project Settings-&gt;Raw Input</code>, and add new device configuration:   Vendor ID: 0x046d (In case of Logitech G920, otherwise you might need to check it).   Product ID: 0xc261 (In case of Logitech G920, otherwise you might need to check it).   Under <code>Axis Properties</code>, make sure that <code>GenericUSBController Axis 2</code>, <code>GenericUSBController Axis 4</code> and <code>GenericUSBController Axis 5</code> are all enabled with an offset of 1.0.   Explanation: axis 2 is responsible for steering movement, axis 4 is for brake and axis 5 is for gas. If you need to configure the clutch, it\u2019s on axis 3.</p> </li> </ol> <p></p> <ol> <li> <p>Go to <code>Edit-&gt;Project Settings-&gt;Input</code>. Under <code>Bindings</code> in <code>Axis Mappings</code>:</p> </li> <li> <p>Remove existing mappings from the groups <code>MoveRight</code> and <code>MoveForward</code>.</p> </li> <li>Add new axis mapping to the group <code>MoveRight</code>, use <code>GenericUSBController Axis 2</code> with a scale of 1.0.</li> <li>Add new axis mapping to the group <code>MoveForward</code>, use <code>GenericUSBController Axis 5</code> with a scale of 1.0.</li> <li>Add a new group of axis mappings, name it <code>FootBrake</code> and add new axis mapping to this group, use <code>GenericUSBController Axis 4</code> with a scale of 1.0.</li> </ol> <p></p> <ol> <li>Play and drive!</li> </ol>"},{"location":"controller_wheel.html#pay-attention","title":"Pay Attention","text":"<p>Notice that in the first time we 'play' after debugging, we need to touch the wheel to 'reset' the values. </p> <p>Tip</p> <p>In the game engine software, you can configure buttons as keyboard shortcuts, we used it to configure a shortcut to record dataset or to play in full screen.</p>"},{"location":"controller_wired.html","title":"Wired Controllers","text":"<p>To use an <code>XBox</code> or similar with <code>AutonomySim</code>, follow the below steps:</p> <p>Note</p> <p>Xbox controllers can be emulated by theoretically any controller in software. A number of programs exist for this purpose, such as HideHide, X360CE, or DS4Windows. In fact, you can also write Python scripts to emulate physical controllers if so desired.</p> <ol> <li>Connect the XBox controller so that it shows up in your PC Game Controllers:</li> </ol> <p></p> <ol> <li>Launch <code>QGroundControl</code> and you should see a new Joystick tab under settings:</li> </ol> <p></p> <p>Now calibrate the radio, and setup some handy button actions. For example, I set mine so that  the <code>A</code> button arms the drone, <code>B</code> put it in manual flight mode, <code>X</code> puts it in altitude hold mode and <code>Y</code> puts it in position hold mode. I also prefer the feel of the controller when I check the box labelled <code>Use exponential curve on roll, pitch, yaw</code> because this gives me more sensitivity for small movements.</p> <p><code>QGroundControl</code> will find your Pixhawk via the UDP proxy port 14550 setup by <code>MavLinkTest</code> above. <code>AutonomySim</code> will find your Pixhawk via the other UDP server port 14570 also setup by MavLinkTest above. You can also use all the QGroundControl controls for autonomous flying at this point too.</p> <ol> <li>Connect to Pixhawk serial port using MavLinkTest.exe like this:</li> </ol> <pre><code>MavLinkTest.exe -serial:*,115200 -proxy:127.0.0.1:14550 -server:127.0.0.1:14570\n</code></pre> <ol> <li>Run <code>AutonomySim</code> with the following settings in <code>AutonomySim/settings.json</code>:</li> </ol> <pre><code>\"Vehicles\": {\n    \"PX4\": {\n        \"VehicleType\": \"PX4Multirotor\",\n\n        \"SitlIp\": \"\",\n        \"SitlPort\": 14560,\n        \"UdpIp\": \"127.0.0.1\",\n        \"UdpPort\": 14570,\n        \"UseSerial\": false\n    }\n}\n</code></pre>"},{"location":"controller_wired.html#advanced","title":"Advanced","text":"<p>If the Joystick tab doesn't show up in QGroundControl then Click on the purple \"Q\" icon on left in tool bar to reveal the Preferences panel. Go to General tab and check the Virtual Joystick checkbox.  Go back to settings screen (gears icon), click on Parameters tab, type <code>COM_RC_IN_MODE</code> in search box and change its value to either <code>Joystick/No RC Checks</code> or <code>Virtual RC by Joystick</code>.</p>"},{"location":"controller_wired.html#other-options","title":"Other Options","text":"<p>See remote controller options</p>"},{"location":"convert_pointcloud.html","title":"Convert Depth Images to Point Clouds","text":"<p>Moved here from there.</p> <p>A Python script point_cloud.py shows how to convert depth images returned from <code>AutonomySim</code> into point clouds.</p> <p>The following depth image was captured using the <code>Modular Neighborhood</code> environment:</p> <p></p> <p>And with the appropriate projection matrix, the OpenCV <code>reprojectImageTo3D</code> function can turn this into a point cloud. The following is the result, which is also available here: https://skfb.ly/68r7y.</p> <p></p> <p>SketchFab can upload the resulting file <code>cloud.asc</code> and render it for you.</p> <p>Warning</p> <p>You may notice the scene is reflected on the y-axis, so we may have a sign wrong in the projection matrix.</p>"},{"location":"create_issues.html","title":"How to Create Issue or Ask Questions Effectively","text":"<p><code>AutonomySim</code> is an open-source project. Contributors like you keep it going. It is important to respect contributors' time and effort when asking a question or filing an issue. Your chances of receiving helpful response will likely improve by following the below guidelines.</p>"},{"location":"create_issues.html#good-practices","title":"Good Practices","text":"<ul> <li>Search issues to see if someone already has asked it.</li> <li>Chose title that is short and summarizes well. </li> <li>Copy and paste full error message.</li> <li>Precisely describe steps you used that produced the error message or symptom.</li> <li>Describe what vehicle, mode, OS, AutonomySim version and other settings you are using.</li> <li>Copy and paste minimal version of code that reproduces the problem.</li> <li>Tell us what the goal you want to achieve or expected output.</li> <li>Tell us what you did so far to debug this issue.</li> </ul>"},{"location":"create_issues.html#bad-practices","title":"Bad Practices","text":"<ul> <li>Do not use \"Please help\" etc in the title. See above.</li> <li>Do not copy and paste screen shot of error message. Copy and paste text.</li> <li>Do not use \"it doesn't work\". Precisely state what is the error message or symptom.</li> <li>Do not ask to write code for you. Contribute!</li> </ul>"},{"location":"data_capture.html","title":"Capturing Data","text":"<p><code>AutonomySim</code> has a <code>Recording</code> feature to easily collect data and images. The Recording APIs also allows starting and stopping the recording using API.</p> <p>However, the data recorded by default might not be sufficient for your use cases, and it might be preferable to record additional data such as IMU, GPS sensors, Rotor speed for copters, etc. You can use the existing Python and C++ APIs to get the information and store it as required, especially for LiDAR. Another option for adding small fields such as GPS or internal data such as Unreal position or something else is possible through modifying the recording methods inside <code>AutonomySim</code>. This page describes the specific methods which you might need to change.</p> <p>The recorded data is written in a <code>autonomysim_rec.txt</code> file in a tab-separated format, with images in an <code>images/</code> folder. The entire folder is by default present in the <code>Documents</code> folder (or specified in settings) with the timestamp of when the recording started in <code>%Y-%M-%D-%H-%M-%S</code> format.</p> <p>The <code>Car</code> vehicle records the following fields:</p> <pre><code>VehicleName TimeStamp   POS_X   POS_Y   POS_Z   Q_W Q_X Q_Y Q_Z Throttle    Steering    Brake   Gear    Handbrake   RPM Speed   ImageFile\n</code></pre> <p>The <code>Multirotor</code> fields:</p> <pre><code>VehicleName TimeStamp   POS_X   POS_Y   POS_Z   Q_W Q_X Q_Y Q_Z ImageFile\n</code></pre>"},{"location":"data_capture.html#code-changes","title":"Code Changes","text":"<p>Note that this requires building and using AutonomySim from source. You can compile a binary yourself after modifying if needed.</p> <p>The primary method which fills the data to be stored is <code>PawnSimApi::getRecordFileLine</code>, it's the base method for all the vehicles, and Car overrides it to log additional data, as can be seen in <code>CarPawnSimApi::getRecordFileLine</code>.</p> <p>To record additional data for multirotor, you can add a similar method in MultirotorPawnSimApi.cpp/h files which overrides the base class implementation and append other data. The currently logged data can also be modified and removed as needed.</p> <p>For example, recording GPS, IMU, and Barometer data for multirotor:</p> <pre><code>// MultirotorPawnSimApi.cpp\nstd::string MultirotorPawnSimApi::getRecordFileLine(bool is_header_line) const {\n    std::string common_line = PawnSimApi::getRecordFileLine(is_header_line);\n\n    if (is_header_line) {\n        return common_line + \"Latitude\\tLongitude\\tAltitude\\tPressure\\tAccX\\tAccY\\tAccZ\\t\";\n    }\n\n    const auto&amp; state = vehicle_api_-&gt;getMultirotorState();\n    const auto&amp; bar_data = vehicle_api_-&gt;getBarometerData(\"\");\n    const auto&amp; imu_data = vehicle_api_-&gt;getImuData(\"\");\n\n    std::ostringstream ss;\n    ss &lt;&lt; common_line;\n    ss &lt;&lt; state.gps_location.latitude &lt;&lt; \"\\t\" &lt;&lt; state.gps_location.longitude &lt;&lt; \"\\t\"\n       &lt;&lt; state.gps_location.altitude &lt;&lt; \"\\t\";\n\n    ss &lt;&lt; bar_data.pressure &lt;&lt; \"\\t\";\n\n    ss &lt;&lt; imu_data.linear_acceleration.x() &lt;&lt; \"\\t\" &lt;&lt; imu_data.linear_acceleration.y() &lt;&lt; \"\\t\"\n       &lt;&lt; imu_data.linear_acceleration.z() &lt;&lt; \"\\t\";\n\n    return ss.str();\n}\n</code></pre> <pre><code>// MultirotorPawnSimApi.h\nvirtual std::string getRecordFileLine(bool is_header_line) const override;\n</code></pre>"},{"location":"development_workflow.html","title":"Development Workflow","text":"<p>Below is the guide on how to perform different development activities while working with AutonomySim. If you are new to Unreal Engine based projects and want to contribute to AutonomySim or make your own forks for your custom requirements, this might save you some time.</p>"},{"location":"development_workflow.html#development-environment","title":"Development Environment","text":""},{"location":"development_workflow.html#os","title":"OS","text":"<p>We highly recommend Windows 10 and Visual Studio 2019 as your development environment. The support for other OSes and IDE is unfortunately not as mature on the Unreal Engine side and you may risk severe loss of productivity trying to do workarounds and jumping through the hoops.</p>"},{"location":"development_workflow.html#hardware","title":"Hardware","text":"<p>We recommend GPUs such as NVidia 1080 or NVidia Titan series with powerful desktop such as one with 64GB RAM, 6+ cores, SSDs and 2-3 displays (ideally 4K). We have found HP Z840 work quite well for our needs. The development experience on high-end laptops is generally sub-par compared to powerful desktops however they might be useful in a pinch. You generally want laptops with discrete NVidia GPU (at least M2000 or better) with 64GB RAM, SSDs and hopefully 4K display. We have found models such as Lenovo P50 work well for our needs. Laptops with only integrated graphics might not work well.</p>"},{"location":"development_workflow.html#updating-and-changing-autonomysim-code","title":"Updating and Changing AutonomySim Code","text":""},{"location":"development_workflow.html#overview","title":"Overview","text":"<p>AutonomySim is designed as plugin. This means it can't run by itself, you need to put it in an Unreal project (we call it \"environment\"). So building and testing AutonomySim has two steps: (1) build the plugin (2) deploy plugin in Unreal project and run the project. </p> <p>The first step is accomplished by build.cmd available in AutonomySim root. This command will update everything you need for the plugin in the <code>Unreal\\Plugins</code> folder. So to deploy the plugin, you just need to copy <code>Unreal\\Plugins</code> folder in to your Unreal project folder. Next you should remove all  intermediate files in your Unreal project and then regenerate .sln file for your Unreal project. To do this, we have two handy .cmd files in <code>Unreal\\Environments\\Blocks</code> folder: <code>clean.cmd</code> and <code>GenerateProjectFiles.cmd</code>. So just run these bat files in sequence from root of your Unreal project. Now you are ready to open new .sln in Visual Studio and press F5 to run it.</p>"},{"location":"development_workflow.html#steps","title":"Steps","text":"<p>Below are the steps we use to make changes in AutonomySim and test them out. The best way to do development in AutonomySim code is to use Blocks project. This is the light weight project so compile time is relatively faster. Generally the workflow is,</p> <pre><code>REM //Use x64 Native Tools Command Prompt for VS 2019\nREM //Navigate to AutonomySim repo folder\n\ngit pull                          \nbuild.cmd                        \ncd Unreal\\Environments\\Blocks         \nupdate_from_git.cmd\nstart Blocks.sln\n</code></pre> <p>Above commands first builds the AutonomySim plugin and then deploys it to Blocks project using handy <code>update_from_git.cmd</code>. Now you can work inside Visual Studio solution, make changes to the code and just run F5 to build, run and test your changes. The debugging, break points etc should work as usual. </p> <p>After you are done with you code changes, you might want to push your changes back to AutonomySim repo or your own fork or you may deploy the new plugin to your custom Unreal project. To do this, go back to command prompt and first update the AutonomySim repo folder:</p> <pre><code>REM //Use x64 Native Tools Command Prompt for VS 2019\nREM //run this from Unreal\\Environments\\Blocks \n\nupdate_to_git.cmd\nbuild.cmd\n</code></pre> <p>Above command will transfer your code changes from Unreal project folder back to <code>Unreal\\Plugins</code> folder. Now your changes are ready to be pushed to AutonomySim repo or your own fork. You can also copy <code>Unreal\\Plugins</code> to your custom Unreal engine project and see if everything works in your custom project as well.</p>"},{"location":"development_workflow.html#takeaway","title":"Takeaway","text":"<p>Once you understand how the Unreal Build system and plugin model works, as well as why we are doing above steps, you should feel comfortable in following this workflow. Don't be afraid of opening up <code>.cmd</code> files to peek inside and see what they are doing. These files are often minimal and straightforward (except, of course, <code>build.cmd</code>).</p>"},{"location":"development_workflow.html#faq","title":"FAQ","text":""},{"location":"development_workflow.html#i-made-changes-in-code-in-blocks-project-but-its-not-working","title":"I made changes in code in Blocks project but its not working.","text":"<p>When you press F5 or F6 in Visual Studio to start build, the Unreal Build system kicks in and it tries to find out if any files are dirty and what it needs to build. Unfortunately, it often fails to recognize dirty files that is not the code that uses Unreal headers and object hierarchy. So, the trick is to just make some file dirty that Unreal Build system always recognizes. My favorite one is AutonomySimGameMode.cpp. Just insert a line, delete it and save the file.</p>"},{"location":"development_workflow.html#i-made-changes-in-the-code-outside-of-visual-studio-but-its-not-working","title":"I made changes in the code outside of Visual Studio but its not working.","text":"<p>Don't do that! Unreal Build system assumes that you are using Visual Studio and it does bunch of things to integrate with Visual Studio. If you do insist on using other editors then look up how to do command line builds in Unreal projects OR see docs on your editor on how it can integrate with Unreal build system OR run <code>clean.cmd</code> + <code>GenerateProjectFiles.cmd</code> to make sure VS solution is in sync.</p>"},{"location":"development_workflow.html#im-trying-to-add-new-file-in-the-unreal-project-and-its-not-working","title":"I'm trying to add new file in the Unreal Project and its not working.","text":"<p>It won't! While you are indeed using Visual Studio solution, remember that this solution was actually generated by Unreal Build system. If you want to add new files in your project, first shut down Visual Studio, add an empty file at desired location and then run <code>GenerateProjectFiles.cmd</code> which will scan all files in your project and then re-create the .sln file. Now open this new .sln file and you are in business.</p>"},{"location":"development_workflow.html#i-copied-unrealplugins-folder-but-nothing-happens-in-unreal-project","title":"I copied Unreal\\Plugins folder but nothing happens in Unreal Project.","text":"<p>First make sure your project's .uproject file is referencing the plugin. Then make sure you have run <code>clean.cmd</code> and then <code>GenerateProjectFiles.cmd</code> as described in Overview above.</p>"},{"location":"development_workflow.html#i-have-multiple-unreal-projects-with-autonomysim-plugin-how-do-i-update-them-easily","title":"I have multiple Unreal projects with AutonomySim plugin. How do I update them easily?","text":"<p>You are in luck! We have <code>build_all_ue_projects.cmd</code> which exactly does that. Don't treat it as black box (at least not yet), open it up and see what it does.  It has 4 variables that are being set from command line args. If these args is not supplied they are set to default values in next set of statements. You might want to change default values for the paths. This batch file builds AutonomySim plugin, deploys it to all listed projects (see CALL statements later in the batch file), runs packaging for those projects and puts final binaries in specified folder - all in one step! This is what we use to create our own binary releases.</p>"},{"location":"development_workflow.html#how-do-i-contribute-back-to-autonomysim","title":"How do I contribute back to AutonomySim?","text":"<p>Before making any changes make sure you have created your feature branch. After you test your code changes in Blocks environment, follow the usual steps to make contributions just like any other GitHub projects. Please use rebase and squash merge, for more information see An introduction to Git merge and rebase: what they are, and how to use them.</p>"},{"location":"distance_sensor.html","title":"Distance Sensor","text":"<p>By default, the <code>Distance Sensor</code> points to the front of the vehicle. It can be pointed in any direction by modifying the settings</p>"},{"location":"distance_sensor.html#configurable-parameters","title":"Configurable Parameters","text":"Parameter Description X Y Z Position of the sensor relative to the vehicle (in NED, in meters) (Default (0,0,0)-Multirotor, (0,0,-1)-Car) Yaw Pitch Roll Orientation of the sensor relative to the vehicle (degrees) (Default (0,0,0)) MinDistance Minimum distance measured by distance sensor (metres, only used to fill Mavlink message for PX4) (Default 0.2m) MaxDistance Maximum distance measured by distance sensor (metres) (Default 40.0m) ExternalController Whether data is to be sent to external controller such as ArduPilot or PX4 if being used (default <code>true</code>) <p>For example, to make the sensor point towards the ground (for altitude measurement similar to barometer), the orientation can be modified as follows -</p> <pre><code>\"Distance\": {\n    \"SensorType\": 5,\n    \"Enabled\" : true,\n    \"Yaw\": 0,\n    \"Pitch\": -90,\n    \"Roll\": 0\n}\n</code></pre> <p>Note</p> <p>For Cars, the sensor is placed 1 meter above the vehicle center by default. This is required since otherwise the sensor gives strange data due it being inside the vehicle. This doesn't affect the sensor values say when measuring the distance between 2 cars. See <code>PythonClient/car/distance_sensor_multi.py</code> for an example usage.</p>"},{"location":"docker_ubuntu.html","title":"Docker on Linux","text":"<p>There are two options for Docker:</p> <ol> <li>Build an image for running AutonomySim linux binaries</li> <li>Build an image for compiling Unreal Engine and AutonomySim from source</li> </ol>"},{"location":"docker_ubuntu.html#binaries","title":"Binaries","text":""},{"location":"docker_ubuntu.html#requirements","title":"Requirements","text":"<ul> <li>Install nvidia-docker2</li> </ul>"},{"location":"docker_ubuntu.html#build-the-docker-image","title":"Build the docker image","text":"<ul> <li>Below are the default arguments.   <code>--base_image</code>: This is image over which we'll install AutonomySim. We've tested on Ubuntu 18.04 with CUDA 10.0.    You can specify any NVIDIA cudagl at your own risk.    <code>--target_image</code> is the desired name of your docker image.    Defaults to <code>autonomysim_binary</code> with same tag as the base image</li> </ul> <pre><code>   cd AutonomySim/docker\n   python build_autonomysim_image.py \\\n      --base_image=nvidia/cudagl:10.0-devel-ubuntu18.04 \\\n      --target_image=autonomysim_binary:10.0-devel-ubuntu18.04\n</code></pre> <ul> <li>Verify you have an image by: <code>$ docker images | grep AutonomySim</code></li> </ul>"},{"location":"docker_ubuntu.html#running-an-unreal-binary-inside-a-docker-container","title":"Running an unreal binary inside a docker container","text":"<ul> <li>Get a Linux binary or package your own project in Ubuntu. Let's take the Blocks binary as an example. You can download it by running the folowing:</li> </ul> <pre><code>   cd AutonomySim/docker\n   ./download_blocks_env_binary.sh\n</code></pre> <p>Modify it to fetch the specific binary required.</p> <ul> <li>Running an unreal binary inside a docker container, the syntax is:</li> </ul> <pre><code>   ./run_autonomysim_image_binary.sh DOCKER_IMAGE_NAME UNREAL_BINARY_SHELL_SCRIPT UNREAL_BINARY_ARGUMENTS -- headless\n</code></pre> <ul> <li> <p>For Blocks, you can do a <code>$ ./run_autonomysim_image_binary.sh autonomysim_binary:10.0-devel-ubuntu18.04 Blocks/Blocks.sh -windowed -ResX=1080 -ResY=720</code></p> </li> <li> <p><code>DOCKER_IMAGE_NAME</code>: Same as <code>target_image</code> parameter in previous step. By default, enter <code>autonomysim_binary:10.0-devel-ubuntu18.04</code></p> </li> <li><code>UNREAL_BINARY_SHELL_SCRIPT</code>: for Blocks enviroment, it will be <code>Blocks/Blocks.sh</code></li> <li><code>UNREAL_BINARY_ARGUMENTS</code>:</li> </ul> <p>For AutonomySim, most relevant would be <code>-windowed</code>, <code>-ResX</code>, <code>-ResY</code>. Click on link to see all options.</p> <ul> <li>Running in Headless mode: suffix <code>-- headless</code> at the end:</li> </ul> <pre><code>./run_autonomysim_image_binary.sh Blocks/Blocks.sh -- headless\n</code></pre> <ul> <li>Specifying a <code>settings.json</code></li> </ul>"},{"location":"docker_ubuntu.html#source","title":"Source","text":""},{"location":"docker_ubuntu.html#requirements_1","title":"Requirements","text":"<ul> <li>Install nvidia-docker2</li> <li>Install ue4-docker</li> </ul>"},{"location":"docker_ubuntu.html#build-unreal-engine-inside-docker","title":"Build Unreal Engine inside docker","text":"<ul> <li> <p>To get access to Unreal Engine's source code, register on Epic Games' website and link it to your github account, as explained in the <code>Required Steps</code> section here.</p> <p>Note that you don't need to do <code>Step 2: Downloading UE4 on Linux</code>!</p> </li> <li> <p>Build unreal engine 4.19.2 docker image. We're going to use CUDA 10.0 in our example.    <code>$ ue4-docker build 4.19.2 --cuda=10.0 --no-full</code></p> </li> <li>[optional] <code>$ ue4-docker clean</code> to free up some space. Details here</li> <li><code>ue4-docker</code> supports all CUDA version listed on NVIDIA's cudagl dockerhub here.</li> <li> <p>Please see this page for advanced configurations using <code>ue4-docker</code></p> </li> <li> <p>Disk space:</p> </li> <li>The unreal images and containers can take up a lot of space, especially if you try more than one version.</li> <li>Here's a list of useful links to monitor space used by docker and clean up intermediate builds:<ul> <li>Large container images primer</li> <li><code>docker system df</code></li> <li><code>docker container prune</code></li> <li><code>docker image prune</code></li> <li><code>docker system prune</code></li> </ul> </li> </ul>"},{"location":"docker_ubuntu.html#building-autonomysim-inside-ue4-docker-container","title":"Building AutonomySim inside UE4 docker container","text":"<ul> <li>Build AutonomySim docker image (which lays over the unreal image we just built)   Below are the default arguments.</li> <li><code>--base_image</code>: This is image over which we'll install AutonomySim. We've tested on <code>adamrehn/ue4-engine:4.19.2-cudagl10.0</code>. See ue4-docker for other versions.</li> <li><code>--target_image</code> is the desired name of your docker image.</li> </ul> <p>Defaults to <code>autonomysim_source</code> with same tag as the base image</p> <pre><code>cd AutonomySim/docker;\npython build_autonomysim_image.py \\\n   --source \\\n   ----base_image adamrehn/ue4-engine:4.19.2-cudagl10.0 \\\n   --target_image=autonomysim_source:4.19.2-cudagl10.0\n</code></pre>"},{"location":"docker_ubuntu.html#running-autonomysim-container","title":"Running AutonomySim container","text":"<ul> <li>Run the AutonomySim source image we built by:</li> </ul> <pre><code>   ./run_autonomysim_image_source.sh autonomysim_source:4.19.2-cudagl10.0\n</code></pre> <ul> <li> <p>Syntax is <code>./run_autonomysim_image_source.sh DOCKER_IMAGE_NAME -- headless</code> <code>-- headless</code>: suffix this to run in optional headless mode.</p> </li> <li> <p>Inside the container, you can see <code>UnrealEngine</code> and <code>AutonomySim</code> under <code>/home/ue4</code>.</p> </li> <li>Start unreal engine inside the container:    <code>ue4@HOSTMACHINE:~$ /home/ue4/UnrealEngine/Engine/Binaries/Linux/UE4Editor</code></li> <li>Specifying an AutonomySim settings.json</li> <li>Continue with AutonomySim's Linux docs.</li> </ul>"},{"location":"docker_ubuntu.html#misc-packaging-unreal-environments-in-autonomysim_source-containers","title":"[Misc] Packaging Unreal Environments in <code>autonomysim_source</code> containers","text":"<ul> <li>Let's take the Blocks environment as an example. In the following script, specify the full path to your unreal uproject file by <code>project</code> and the directory where you want the binaries to be placed by <code>archivedirectory</code></li> </ul> <pre><code>/home/ue4/UnrealEngine/Engine/Build/BatchFiles/RunUAT.sh BuildCookRun \\\n   -platform=Linux -clientconfig=Shipping -serverconfig=Shipping -noP4 \\\n   -cook -allmaps -build -stage -prereqs -pak -archive \\\n   -archivedirectory=/home/ue4/Binaries/Blocks/ \\\n   -project=/home/ue4/AutonomySim/Unreal/Environments/Blocks/Blocks.uproject\n</code></pre> <p>This would create a Blocks binary in <code>/home/ue4/Binaries/Blocks/</code>. You can test it by running <code>/home/ue4/Binaries/Blocks/LinuxNoEditor/Blocks.sh -windowed</code></p>"},{"location":"docker_ubuntu.html#specifying-settingsjson","title":"Specifying settings.json","text":""},{"location":"docker_ubuntu.html#autonomysim_binary-docker-image","title":"<code>autonomysim_binary</code> Docker image","text":"<ul> <li>We're mapping the host machine's <code>PATH/TO/AutonomySim/docker/settings.json</code> to the docker container's <code>/home/autonomysim_user/Documents/AutonomySim/settings.json</code>.</li> <li>Hence, we can load any settings file by simply modifying <code>PATH_TO_YOUR/settings.json</code> by modifying the following snippets in <code>run_autonomysim_image_binary.sh</code></li> </ul> <pre><code>nvidia-docker run --runtime=nvidia -it \\\n   -v $PATH_TO_YOUR/settings.json:/home/autonomysim_user/Documents/AutonomySim/settings.json \\\n   -v $UNREAL_BINARY_PATH:$UNREAL_BINARY_PATH \\\n   -e SDL_VIDEODRIVER=$SDL_VIDEODRIVER_VALUE \\\n   -e SDL_HINT_CUDA_DEVICE='0' \\\n   --net=host \\\n   --env=\"DISPLAY=$DISPLAY\" \\\n   --env=\"QT_X11_NO_MITSHM=1\" \\\n   --volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\\n   -env=\"XAUTHORITY=$XAUTH\" \\\n   --volume=\"$XAUTH:$XAUTH\" \\\n   --rm $DOCKER_IMAGE_NAME \\\n   /bin/bash -c \"$UNREAL_BINARY_COMMAND\"\n</code></pre> <p>Note</p> <p>Docker version &gt;=19.03 (check using <code>docker -v</code>), natively supports Nvidia GPUs, so run using <code>--gpus all</code> flag as given -</p> <pre><code>docker run --gpus all -it ...\n</code></pre>"},{"location":"docker_ubuntu.html#autonomysim_source-docker-image","title":"<code>autonomysim_source</code> docker image","text":"<ul> <li>We're mapping the host machine's <code>PATH/TO/AutonomySim/docker/settings.json</code> to the docker container's <code>/home/autonomysim_user/Documents/AutonomySim/settings.json</code>.</li> <li>Hence, we can load any settings file by simply modifying <code>PATH_TO_YOUR/settings.json</code> by modifying the following snippets in <code>run_autonomysim_image_source.sh</code>:</li> </ul> <pre><code>   nvidia-docker run --runtime=nvidia -it \\\n      -v $(pwd)/settings.json:/home/autonomysim_user/Documents/AutonomySim/settings.json \\\n      -e SDL_VIDEODRIVER=$SDL_VIDEODRIVER_VALUE \\\n      -e SDL_HINT_CUDA_DEVICE='0' \\\n      --net=host \\\n      --env=\"DISPLAY=$DISPLAY\" \\\n      --env=\"QT_X11_NO_MITSHM=1\" \\\n      --volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\\n      -env=\"XAUTHORITY=$XAUTH\" \\\n      --volume=\"$XAUTH:$XAUTH\" \\\n      --rm $DOCKER_IMAGE_NAME\n</code></pre>"},{"location":"faq.html","title":"FAQ","text":""},{"location":"faq.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>FAQ</li> <li>Table of Contents</li> <li>General<ul> <li>Unreal editor is slow when it is not the active window</li> <li>My mouse disappears in Unreal</li> <li>Where is the setting file and how do I modify it?</li> <li>How do I arm my drone?</li> <li>When making API call I get error</li> <li>I'm getting Eigen not found error when compiling Unreal project</li> <li>Something went wrong. How do I debug?</li> <li>What do the colors mean in the Segmentation View?</li> <li>Unreal 4.xx doesn't look as good as 4.yy</li> <li>Can I use an XBox controller to fly?</li> <li>Can I build a hexacopter with AutonomySim?</li> <li>How do I use AutonomySim with multiple vehicles?</li> <li>What computer do you need?</li> <li>How do I report issues?</li> </ul> </li> <li>Others</li> </ul>"},{"location":"faq.html#general","title":"General","text":""},{"location":"faq.html#unreal-editor-is-slow-when-it-is-not-the-active-window","title":"Unreal editor is slow when it is not the active window","text":"<p>Go to Edit/Editor Preferences, select \"All Settings\" and type \"CPU\" in the search box. It should find the setting titled \"Use Less CPU when in Background\", and you want to uncheck this checkbox.</p>"},{"location":"faq.html#my-mouse-disappears-in-unreal","title":"My mouse disappears in Unreal","text":"<p>Yes, Unreal steals the mouse, and we don't draw one.  So to get your mouse back just use Alt+TAB to switch to a different window. To avoid this entirely, go to Project settings &gt;in Unreal Editor, go to Input tab and disable all settings for mouse capture.</p>"},{"location":"faq.html#where-is-the-setting-file-and-how-do-i-modify-it","title":"Where is the setting file and how do I modify it?","text":"<p>AutonomySim will create empty settings file at <code>~/Documents/AutonomySim/settings.json</code>. You can view the available settings options.</p>"},{"location":"faq.html#how-do-i-arm-my-drone","title":"How do I arm my drone?","text":"<p>If you're using simple_flight, your vehicle is already armed and ready to fly. For PX4 you can arm by holding both sticks on remote control down and to the center.</p>"},{"location":"faq.html#when-making-api-call-i-get-error","title":"When making API call I get error","text":"<p>If you are getting this error, <pre><code>TypeError: unsupported operand type(s) for *: 'AsyncIOLoop' and 'float'\n</code></pre> It is probably due to upgraded version of tornado package with version &gt; 5.0 in Python that conflicts with <code>msgpack-rpc-python</code> which requires tornado package &lt; 5.0. To fix this &gt;you can update the package like this: <pre><code>pip install --upgrade msgpack-rpc-python\n</code></pre> But this might break something (for example, PyTorch 0.4+) because it will uninstall newer tornado and re-install older one. To avoid this you should create new conda &gt;environment.</p>"},{"location":"faq.html#im-getting-eigen-not-found-error-when-compiling-unreal-project","title":"I'm getting Eigen not found error when compiling Unreal project","text":"<p>This is most likely because AutonomySim wasn't built and Plugin folder was copied in Unreal project folder. To fix this make sure you build AutonomySim first (run &gt;<code>build.cmd</code> in Windows).</p>"},{"location":"faq.html#something-went-wrong-how-do-i-debug","title":"Something went wrong. How do I debug?","text":"<p>First turn on C++ exceptions from the Exceptions window:</p> <p></p> <p>and copy the stack trace of all exceptions you see there during execution that look relevant (for example, there might be an initial exception from VSPerf140 that you can &gt;ignore) then paste these call stacks into a new AutonomySim GitHub issue, thanks.</p>"},{"location":"faq.html#what-do-the-colors-mean-in-the-segmentation-view","title":"What do the colors mean in the Segmentation View?","text":"<p>See Camera Views for information on the camera views and how to change them.</p>"},{"location":"faq.html#unreal-4xx-doesnt-look-as-good-as-4yy","title":"Unreal 4.xx doesn't look as good as 4.yy","text":"<p>Unreal 4.15 added the ability for Foliage LOD dithering to be disabled on a case-by-case basis by unchecking the <code>Dithered LOD Transition</code> checkbox in the foliage materials. &gt;Note that all materials used on all LODs need to have the checkbox checked in order for dithered LOD transitions to work.  When checked the transition of generated foliage will &gt;be a lot smoother and will look better than 4.14.</p>"},{"location":"faq.html#can-i-use-an-xbox-controller-to-fly","title":"Can I use an XBox controller to fly?","text":"<p>See XBox controller for details.</p>"},{"location":"faq.html#can-i-build-a-hexacopter-with-autonomysim","title":"Can I build a hexacopter with AutonomySim?","text":"<p>See how to build a hexacopter.</p>"},{"location":"faq.html#how-do-i-use-autonomysim-with-multiple-vehicles","title":"How do I use AutonomySim with multiple vehicles?","text":"<p>Here is multi-vehicle setup guide.</p>"},{"location":"faq.html#what-computer-do-you-need","title":"What computer do you need?","text":"<p>It depends on how big your Unreal Environment is. The Blocks environment that comes with AutonomySim is very basic and works on typical laptops. The Modular Neighborhood Pack that we use ourselves for research requires GPUs with at least 4GB of RAM. The Open World environment needs GPU with 8GB RAM. Our typical development machines have 32GB of RAM and NVIDIA TitanX and a fast hard drive.</p>"},{"location":"faq.html#how-do-i-report-issues","title":"How do I report issues?","text":"<p>It's a good idea to include your configuration like below. If you can also include logs, that could also expedite the investigation.</p> <pre><code>Operating System: Windows 10 64bit\nCPU: Intel Core i7\nGPU: Nvidia GTX 1080\nRAM: 32 GB\nFlight Controller: Pixhawk v2\nRemote Control: Futaba\n</code></pre> <p>If you have modified the default <code>~/Document/AutonomySim/settings.json</code>, please include your settings also.</p> <p>If you are using PX4 then try to capture log from MavLink or PX4.</p> <p>File an issue through GitHub Issues.</p>"},{"location":"faq.html#others","title":"Others","text":"<ul> <li>Linux Build FAQ</li> <li>Windows Build FAQ</li> <li>PX4 Setup FAQ</li> <li>Remote Control FAQ</li> <li>Unreal Blocks Environment FAQ</li> <li>Unreal Custom Environment FAQ</li> <li>Packaging AutonomySim</li> </ul>"},{"location":"format_pfm.html","title":"PFM Image File Format","text":"<p>The Portable FloatMap (PFM) file format stores images with floating-point pixels and hence is not restricted to the 8-bit unsigned integer value range of 0-255. This is useful for HDR images or images that describes something other than colors, such as depth.</p> <p>A good viewer for this file format is PfmPad. We do not recommend the <code>Maverick</code> photo viewer because it doesn't display depth images properly.</p> <p><code>AutonomySim</code> provides code to write <code>pfm</code> files in C++ and, to read and write <code>pfm</code> files in Python.</p>"},{"location":"getting_started.html","title":"Getting Started","text":"<p>Coming soon.</p>"},{"location":"hello_drone.html","title":"Hello Drone","text":""},{"location":"hello_drone.html#how-does-hello-drone-work","title":"How does Hello Drone work?","text":"<p><code>Hello Drone</code> uses the RPC client to connect to the RPC server that is automatically started by <code>AutonomySim</code>. The RPC server routes all the commands to a class that implements <code>MultirotorApiBase</code>. In essence, <code>MultirotorApiBase</code> defines our abstract interface for getting data from the quadrotor and sending back commands. We currently have concrete implementation for <code>MultirotorApiBase</code> for MavLink based vehicles. The implementation for DJI drone platforms, specifically Matrice, is in works.</p>"},{"location":"language_guidelines.html","title":"Programming Language Guidelines","text":"<p>We adopt the modern C++[11..23] standards. Smart pointers, lambdas, and multithreading primitives are your friend.</p> <p>For Python and C#, we adopt the same standards as C++ where possible.</p>"},{"location":"language_guidelines.html#a-note-on-standards","title":"A Note on Standards","text":"<p>The great thing about 'standards' is that there are many to chose from: ISO, Sutter &amp; Stroustrup, ROS, Linux, Google, Microsoft, CERN, GCC, ARM, LLVM, Epic Games and probably thousands of others. Unfortunately, most of these disagree about basic things, such as how to name a class or a constant. This is due to the fact that the standards often inherit legacy issues in order to support existing codebases. The intention behind this document is to provide guidance that remains as close to ISO, Sutter &amp; Stroustrup and ROS while resolving as many conflicts, disadvantages and inconsistencies as possible among them.</p> <p>Note</p> <p>Since we have dropped support for all other game engines, we will be refactoring our C++ code to better comply with the Epic Games standard for Unreal Engine version 5.3 or greater.</p>"},{"location":"language_guidelines.html#clang-format","title":"clang-format","text":"<p>Formatting the syntax of C++ is normalized by the clang-format tool which has settings checked into this project in the file <code>.clang-format</code>. These settings are set to match the formatting guidelines listed below.  You can \"format\" a file using clang-format command line or by enabling Visual Studio automatic-clang formatting either during every edit or when you save the file.  All files have been formatted this way and the github workflow called <code>clang-format</code> will also ensure all pull requests are correctly formatted so it should stay clean.  Obviously this does not include external code like <code>Eigen</code> or <code>rpclib</code>.  </p> <p>If you find a bug in clang-format you can disable clang formatting of a specific block of code by using the following comments pair:</p> <pre><code>// clang-format off\n...\n// clang-format on\n</code></pre>"},{"location":"language_guidelines.html#naming-conventions","title":"Naming Conventions","text":"<p>Avoid using any sort of Hungarian notation on names and <code>_ptr</code> on pointers.</p> Code Element Style Comment Namespace under_scored Differentiate from class names Class name CamelCase To differentiate from STL types which ISO recommends (do not use \"C\" or \"T\" prefixes) Function name camelCase Lower case start is almost universal except for .Net world Parameters/Locals under_scored Vast majority of standards recommends this because _ is more readable to C++ crowd (although not much to Java/.Net crowd) Member variables under_scored_with_ The prefix _ is heavily discouraged as ISO has rules around reserving _identifiers, so we recommend suffix instead Enums and its members CamelCase Most except very old standards agree with this one Globals g_under_scored You shouldn't have these in first place! Constants UPPER_CASE Very contentious and we just have to pick one here, unless if is a private constant in class or method, then use naming for Members or Locals File names Match case of class name in file Lot of pro and cons either way but this removes inconsistency in auto generated code (important for ROS)"},{"location":"language_guidelines.html#header-files","title":"Header Files","text":"<p>Use a namespace qualified #ifdef to protect against multiple inclusion:</p> <pre><code>#ifndef nervosys_AutonomySim_MyHeader_hpp\n#define nervosys_AutonomySim_MyHeader_hpp\n\n//--your code\n\n#endif\n</code></pre> <p>The reason we don't use #pragma once is because it's not supported if same header file exists at multiple places (which might be possible under ROS build system!).</p>"},{"location":"language_guidelines.html#bracketing","title":"Bracketing","text":"<p>Inside function or method body place curly bracket on same line. Outside that the Namespace, Class and methods levels use separate line.This is called K&amp;R style and its variants are widely used in C++ vs other styles which are more popular in other languages. Notice that curlies are not required if you have single statement, but complex statements are easier to keep correct with the braces.</p> <pre><code>int main(int argc, char* argv[]) {\n     while (x == y) {\n        f0();\n        if (cont()) {\n            f1();\n        } else {\n            f2();\n            f3();\n        }\n        if (x &gt; 100)\n            break;\n    }\n}\n</code></pre>"},{"location":"language_guidelines.html#const-and-references","title":"Const and References","text":"<p>Religiously review all non-scalar parameters you declare to be candidate for const and references. If you are coming from languages such as C#/Java/Python, the most often mistake you would make is to pass parameters by value instead of <code>const T&amp;;</code> Especially most of the strings, vectors and maps you want to pass as <code>const T&amp;;</code> (if they are readonly) or <code>T&amp;</code> (if they are writable). Also add <code>const</code> suffix to methods as much as possible.</p>"},{"location":"language_guidelines.html#overriding","title":"Overriding","text":"<p>When overriding virtual method, use override suffix.</p>"},{"location":"language_guidelines.html#pointers","title":"Pointers","text":"<p>This is really about memory management. A simulator has much performance critical code, so we try and avoid overloading the memory manager with lots of calls to new/delete. We also want to avoid too much copying of things on the stack, so we pass things by reference when ever possible. But, when the object really needs to live longer than the call stack you often need to allocate that object on the heap, and so you have a pointer.  Now, if management of the lifetime of that object is going to be tricky we recommend using C++ 11 smart pointers. But smart pointers do have a cost, so don\u2019t use them blindly everywhere.  For private code where performance is paramount, raw pointers can be used. Raw pointers are also often needed when interfacing with legacy systems that only accept pointer types, for example, sockets API. But we try to wrap those legacy interfaces as much as possible and avoid that style of programming from leaking into the larger code base.</p> <p>Religiously check if you can use const everywhere, for example, <code>const float * const xP</code>. Avoid using prefix or suffix to indicate pointer types in variable names, i.e., use <code>my_obj</code> instead of <code>myobj_ptr</code> except in cases where it might make sense to differentiate variables better, for example, <code>int mynum = 5; int* mynum_ptr = mynum;</code></p>"},{"location":"language_guidelines.html#null-checking","title":"Null Checking","text":"<p>In Unreal C++ code, when checking if a pointer is null, it is preferable to use <code>IsValid(ptr)</code>. In addition to checking for a null pointer, this function will also return whether a UObject is properly initialized. This is useful in situations where a <code>UObject</code> is in the process of being garbage collected but still set to a non-null value.</p>"},{"location":"language_guidelines.html#indentation","title":"Indentation","text":"<p>The C++ code base uses four spaces for indentation (not tabs).</p>"},{"location":"language_guidelines.html#line-breaks","title":"Line Breaks","text":"<p>Files should be committed with Unix line breaks. When working on Windows, git can be configured to checkout files with Windows line breaks and automatically convert from Windows to Unix line breaks when committing by running the following command:</p> <pre><code>git config --global core.autocrlf true\n</code></pre> <p>When working on Linux, it is preferable to configure git to checkout files with Unix line breaks by running the following command:</p> <pre><code>git config --global core.autocrlf input\n</code></pre> <p>For more details on this setting, see this documentation.</p>"},{"location":"language_guidelines.html#on-brevity","title":"On Brevity","text":"<p>This document is intentionally brief, as nobody likes to read a 200-page set of code guidelines. The goal here is to cover only the most significant items that are already not covered by strict mode compilation in GCC and Level 4 warnings-as-errors in Visual C++. If you would like to learn how to write better C++ code, please read the GotW and Effective Modern C++ books.</p>"},{"location":"lidar.html","title":"LiDAR Sensor","text":"<p><code>AutonomySim</code> supports light detection-and-ranging (LiDAR) sensors.</p> <p>The enablement of lidar and the other lidar settings can be configured via <code>AutonomySimSettings.json</code>. Please see general sensors for information on configruation of general/shared sensor settings.</p>"},{"location":"lidar.html#enabling-lidar-on-a-vehicle","title":"Enabling LiDAR on a Vehicle","text":"<ul> <li>By default, LiDAR is disabled. To enable it, set the SensorType and Enabled attributes in settings json:</li> </ul> <pre><code>    \"Lidar1\": {\n         \"SensorType\": 6,\n         \"Enabled\" : true,\n    }\n</code></pre> <ul> <li>Multiple lidars can be enabled on a vehicle.</li> </ul>"},{"location":"lidar.html#lidar-configuration","title":"LiDAR configuration","text":"<p>The following parameters can be configured right now via settings json.</p> Parameter Description NumberOfChannels Number of channels/lasers of the lidar Range Range, in meters PointsPerSecond Number of points captured per second RotationsPerSecond Rotations per second HorizontalFOVStart Horizontal FOV start for the lidar, in degrees HorizontalFOVEnd Horizontal FOV end for the lidar, in degrees VerticalFOVUpper Vertical FOV upper limit for the lidar, in degrees VerticalFOVLower Vertical FOV lower limit for the lidar, in degrees X Y Z Position of the lidar relative to the vehicle (in NED, in meters) Roll Pitch Yaw Orientation of the lidar relative to the vehicle  (in degrees, yaw-pitch-roll order to front vector +X) DataFrame Frame for the points in output (\"VehicleInertialFrame\" or \"SensorLocalFrame\") ExternalController Whether data is to be sent to external controller such as ArduPilot or PX4 if being used (default <code>true</code>) (PX4 doesn't send Lidar data currently) <p>For example:</p> <pre><code>{\n    \"SeeDocsAt\": \"https://nervosys.github.io/AutonomySim/settings/\",\n    \"SettingsVersion\": 1.2,\n\n    \"SimMode\": \"Multirotor\",\n\n     \"Vehicles\": {\n        \"Drone1\": {\n            \"VehicleType\": \"simpleflight\",\n            \"AutoCreate\": true,\n            \"Sensors\": {\n                \"LidarSensor1\": {\n                    \"SensorType\": 6,\n                    \"Enabled\" : true,\n                    \"NumberOfChannels\": 16,\n                    \"RotationsPerSecond\": 10,\n                    \"PointsPerSecond\": 100000,\n                    \"X\": 0, \"Y\": 0, \"Z\": -1,\n                    \"Roll\": 0, \"Pitch\": 0, \"Yaw\" : 0,\n                    \"VerticalFOVUpper\": -15,\n                    \"VerticalFOVLower\": -25,\n                    \"HorizontalFOVStart\": -20,\n                    \"HorizontalFOVEnd\": 20,\n                    \"DrawDebugPoints\": true,\n                    \"DataFrame\": \"SensorLocalFrame\"\n                },\n                \"LidarSensor2\": {\n                   \"SensorType\": 6,\n                    \"Enabled\" : true,\n                    \"NumberOfChannels\": 4,\n                    \"RotationsPerSecond\": 10,\n                    \"PointsPerSecond\": 10000,\n                    \"X\": 0, \"Y\": 0, \"Z\": -1,\n                    \"Roll\": 0, \"Pitch\": 0, \"Yaw\" : 0,\n                    \"VerticalFOVUpper\": -15,\n                    \"VerticalFOVLower\": -25,\n                    \"DrawDebugPoints\": true,\n                    \"DataFrame\": \"SensorLocalFrame\"\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"lidar.html#server-side-visualization-for-debugging","title":"Server side visualization for debugging","text":"<p>By default, the LiDAR points are not drawn on the viewport. To enable the drawing of hit laser points on the viewport, please enable setting <code>DrawDebugPoints</code> via settings json.</p> <pre><code>    \"Lidar1\": {\n         ...\n         \"DrawDebugPoints\": true\n    },\n</code></pre> <p>Note</p> <p>Enabling <code>DrawDebugPoints</code> can cause excessive memory usage and crash in releases <code>v1.3.1</code>, <code>v1.3.0</code>. This has been fixed in the main branch and should work in later releases</p>"},{"location":"lidar.html#client-api","title":"Client API","text":"<p>Use <code>getLidarData()</code> API to retrieve the Lidar data.</p> <ul> <li>The API returns a Point-Cloud as a flat array of floats along with the timestamp of the capture and lidar pose.</li> <li>Point-Cloud:<ul> <li>The floats represent [x,y,z] coordinate for each point hit within the range in the last scan.</li> <li>The frame for the points in the output is configurable using \"DataFrame\" attribute -<ul> <li>\"\" or <code>VehicleInertialFrame</code> -- default; returned points are in vehicle inertial frame (in NED, in meters)</li> <li><code>SensorLocalFrame</code> -- returned points are in lidar local frame (in NED, in meters)</li> </ul> </li> </ul> </li> <li>LiDAR Pose:<ul> <li>Lidar pose in the vehicle inertial frame (in NED, in meters)</li> <li>Can be used to transform points to other frames.</li> </ul> </li> <li>Segmentation: The segmentation of each lidar point's collided object</li> </ul>"},{"location":"lidar.html#python-examples","title":"Python Examples","text":"<ul> <li>drone_lidar.py</li> <li>car_lidar.py</li> <li>sensorframe_lidar_pointcloud.py</li> <li>vehicleframe_lidar_pointcloud.py</li> </ul>"},{"location":"lidar.html#coming-soon","title":"Coming soon","text":"<ul> <li>Visualization of LiDAR data on client side.</li> </ul>"},{"location":"manuscripts.html","title":"Manuscripts","text":"<p>You can read more about our architecture and design in the original draft manuscript. You may cite it as follows:</p> <pre><code>@techreport{MSR-TR-2017-9,\n  author = {Shital Shah and Debadeepta Dey and Chris Lovett and Ashish Kapoor},\n  year = 2017,\n  title = {{Aerial Informatics and Robotics Platform}},\n  number = {MSR-TR-2017-9},\n  institution = {Microsoft Research},\n  url = {https://www.microsoft.com/en-us/research/project/aerial-informatics-robotics-platform/},\n  eprint = {https://www.microsoft.com/en-us/research/wp-content/uploads/2017/02/aerial-informatics-robotics.pdf},\n  note = {AirSim draft manuscript}\n}\n</code></pre>"},{"location":"manuscripts.html#architecture","title":"Architecture","text":"<p>Below is high level overview of how different components interact with each other.</p> <p></p>"},{"location":"mavlink_comm.html","title":"MAVLink Communication (MavLinkCom)","text":"<p><code>MavLinkCom</code> is a cross-platform C++ library that helps connect to and communicate with MavLink based vehicles. Specifically this library is designed to work well with PX4 based drones.</p>"},{"location":"mavlink_comm.html#design","title":"Design","text":"<p>You can view and edit the Design.dgml diagram in Visual Studio. </p> <p>The following are the most important classes in this library.</p>"},{"location":"mavlink_comm.html#mavlinknode","title":"MavLinkNode","text":"<p>This is the base class for all MavLinkNodes (subclasses include MavLinkVehicle, MavLinkVideoClient and MavLinkVideoServer). The node connects to your mavlink enabled vehicle via a MavLinkConnection and provides methods for sending MavLinkMessages and MavLinkCommands and for subscribing to receive messages.  This base class also stores the local system id and component id your app wants to use to identify itself to your remote vehicle.  You can also call startHeartbeat to send regular heartbeat messages to keep the connection alive.</p>"},{"location":"mavlink_comm.html#mavlinkmessage","title":"MavLinkMessage","text":"<p>This is the encoded MavLinkMessage.  For those who have used the mavlink.h C API, this is similar to mavlink_message_t. You do not create these manually, they are encoded from a strongly typed MavLinkMessageBase subclass.</p>"},{"location":"mavlink_comm.html#strongly-typed-message-and-command-classes","title":"Strongly typed message and command classes","text":"<p>The MavLinkComGenerator parses the mavlink common.xml message definitions and generates all the MavLink* MavLinkMessageBase subclasses as well as a bunch of handy mavlink enums and a bunch of strongly typed MavLinkCommand subclasses.</p>"},{"location":"mavlink_comm.html#mavlinkmessagebase","title":"MavLinkMessageBase","text":"<p>This is the base class for a set of strongly typed message classes that are code generated by the MavLinkComGenerator project. This replaces the C messages defined in the mavlink C API and provides a slightly more object oriented way to send and receive messages via sendMessage on MavLinkNode.  These classes have encode/decode methods that convert to and from the MavLinkMessage class. </p>"},{"location":"mavlink_comm.html#mavlinkcommand","title":"MavLinkCommand","text":"<p>This is the base class for a set of strongly typed command classes that are code generated by the MavLinkComGenerator project. This replaces the C definitions defined in the mavlink C API and provides a more object oriented way to send commands via the sendCommand method on MavLinkNode.  The MavLinkNode takes care of turning these into the underlying mavlink COMMAND_LONG message.</p>"},{"location":"mavlink_comm.html#mavlinkconnection","title":"MavLinkConnection","text":"<p>This class provides static helper methods for creating connections to remote MavLink nodes, over serial ports, as well as UDP, or TCP sockets. This class provides a way to subscribe to receive messages from that node in a pub/sub way so you can have multiple subscribers on the same connection.  MavLinkVehicle uses this to track various messages that define the overall vehicle state.</p>"},{"location":"mavlink_comm.html#mavlinkvehicle","title":"MavLinkVehicle","text":"<p>MavLinkVehicle is a MavLinkNode that tracks various messages that define the overall vehicle state and provides a VehicleState struct containing a snapshot of that state, including home position, current orientation, local position, global position, and so on. This class also provides a bunch of helper methods that wrap commonly used commands providing simple method calls to do things like arm, disarm, takeoff, land, go to a local coordinate, and fly under offbaord control either by position or velocity control.</p>"},{"location":"mavlink_comm.html#mavlinktcpserver","title":"MavLinkTcpServer","text":"<p>This helper class provides a way to setup a \"server\" that accepts MavLinkConnections from remote nodes.  You can use this class to get a connection that you can then give to MavLinkVideoServer to serve images over MavLink.</p>"},{"location":"mavlink_comm.html#mavlinkftpclient","title":"MavLinkFtpClient","text":"<p>This helper class takes a given MavLinkConnection and provides FTP client support for the MAVLINK_MSG_ID_FILE_TRANSFER_PROTOCOL for vehicles that support the FTP capability.  This class provides simple methods to list directory contents, and the get and put files.</p>"},{"location":"mavlink_comm.html#mavlinkvideoclient","title":"MavLinkVideoClient","text":"<p>This helper class takes a given MavLinkConnection and provides helper methods for requesting video from remote node and packaging up the MAVLINK_MSG_ID_DATA_TRANSMISSION_HANDSHAKE and MAVLINK_MSG_ID_ENCAPSULATED_DATA messages into simple to use MavLinkVideoFrames.</p>"},{"location":"mavlink_comm.html#mavlinkvideoserver","title":"MavLinkVideoServer","text":"<p>This helper class takes a given MavLinkConnection and provides the server side of the MavLinkVideoClient protocol, including helper methods for notifying when there is a video request to process (hasVideoRequest) and a method to send video frames (sendFrame) which  will generate the right MAVLINK_MSG_ID_DATA_TRANSMISSION_HANDSHAKE and MAVLINK_MSG_ID_ENCAPSULATED_DATA sequence.</p>"},{"location":"mavlink_comm.html#examples","title":"Examples","text":"<p>The following code from the UnitTest project shows how to connect to a Pixhawk flight controller over USB serial port, then wait for the first heartbeat message to be received:</p> <pre><code>auto connection = MavLinkConnection::connectSerial(\"drone\", \"/dev/ttyACM0\", 115200, \"sh /etc/init.d/rc.usb\\n\");\nMavLinkHeartbeat heartbeat;\nif (!waitForHeartbeat(10000, heartbeat)) {\n    throw std::runtime_error(\"Received no heartbeat from PX4 after 10 seconds\");\n}\n</code></pre> <p>The following code connects to serial port, and then forwards all messages to and from QGroundControl to that drone using another connection that is joined to the drone stream.</p> <pre><code>auto droneConnection = MavLinkConnection::connectSerial(\"drone\", \"/dev/ttyACM0\", 115200, \"sh /etc/init.d/rc.usb\\n\");\nauto proxyConnection = MavLinkConnection::connectRemoteUdp(\"qgc\", \"127.0.0.1\", \"127.0.0.1\", 14550);\ndroneConnection-&gt;join(proxyConnection);\n</code></pre> <p>The following code then takes that connection and turns on heartBeats and starts tracking vehicle information using local system id 166 and component id 1.</p> <pre><code>auto vehicle = std::make_shared&lt;MavLinkVehicle&gt;(166, 1);\nvehicle-&gt;connect(connection);\nvehicle-&gt;startHeartbeat();\n\nstd::this_thread::sleep_for(std::chrono::seconds(5));\n\nVehicleState state = vehicle-&gt;getVehicleState();\nprintf(\"Home position is %s, %f,%f,%f\\n\", state.home.is_set ? \"set\" : \"not set\", \n    state.home.global_pos.lat, state.home.global_pos.lon, state.home.global_pos.alt);\n</code></pre> <p>The following code uses the vehicle object to arm the drone and take off and wait for the takeoff altitude to be reached:</p> <pre><code>bool rc = false;\nif (!vehicle-&gt;armDisarm(true).wait(3000, &amp;rc) || !rc) {\n    printf(\"arm command failed\\n\");\n    return;\n}\nif (!vehicle-&gt;takeoff(targetAlt).wait(3000, &amp;rc) || !rc) {\n    printf(\"takeoff command failed\\n\");\n    return;\n}\nint version = vehicle-&gt;getVehicleStateVersion();\nwhile (true) {\n    int newVersion = vehicle-&gt;getVehicleStateVersion();\n    if (version != newVersion) {\n        VehicleState state = vehicle-&gt;getVehicleState();\n        float alt = state.local_est.pos.z;\n        if (alt &gt;= targetAlt - delta &amp;&amp; alt &lt;= targetAlt + delta)\n        {           \n            reached = true;\n            printf(\"Target altitude reached\\n\");\n            break;\n        }\n    } else {\n        std::this_thread::sleep_for(std::chrono::milliseconds(10));\n    }\n}\n</code></pre> <p>The following code uses offboard control to make the drone fly in a circle with camera pointed at the center. Here we use the subscribe method to check each new local position message to indicate so we can compute the new velocity vector as soon as that new position is received.  We request a high rate for those messages using setMessageInterval to ensure smooth circular orbit.</p> <p><pre><code>vehicle-&gt;setMessageInterval((int)MavLinkMessageIds::MAVLINK_MSG_ID_LOCAL_POSITION_NED, 30);\nvehicle-&gt;requestControl();\nint subscription = vehicle-&gt;getConnection()-&gt;subscribe(\n    [&amp;](std::shared_ptr&lt;MavLinkConnection&gt; connection, const MavLinkMessage&amp; m) {\n        if (m.msgid == (int)MavLinkMessageIds::MAVLINK_MSG_ID_LOCAL_POSITION_NED)\n        {\n            // convert generic msg to strongly typed message.\n            MavLinkLocalPositionNed localPos;\n            localPos.decode(msg); \n            float x = localPos.x;\n            float y = localPos.y;\n            float dx = x - cx;\n            float dy = y - cy;\n            float angle = atan2(dy, dx);\n            if (angle &lt; 0) angle += M_PI * 2;\n            float tangent = angle + M_PI_2;\n            double newvx = orbitSpeed * cos(tangent);\n            double newvy = orbitSpeed * sin(tangent);\n            float heading = angle + M_PI;\n            vehicle-&gt;moveByLocalVelocityWithAltHold(newvx, newvy, altitude, true, heading);\n        }\n    });\n</code></pre> The following code stops flying the drone in offboard mode and tells the drone to loiter at its current location. This version of the code shows how to use the AsyncResult without blocking on a wait call.</p> <pre><code>    vehicle-&gt;releaseControl();\n    if (vehicle-&gt;loiter().then([=](bool rc) {\n        printf(\"loiter command %s\\n\", rc ? \"succeeded\" : \"failed\");\n    }\n</code></pre> <p>The following code gets all configurable parameters from the drone and prints them:</p> <pre><code>    auto list = vehicle-&gt;getParamList();\n    auto end = list.end();\n    int count = 0;\n    for (auto iter = list.begin(); iter &lt; end; iter++)\n    {\n        count++;\n        MavLinkParameter p = *iter;\n        if (p.type == MAV_PARAM_TYPE_REAL32 || p.type == MAV_PARAM_TYPE_REAL64) {\n            printf(\"%s=%f\\n\", p.name.c_str(), p.value);\n        }\n        else {\n            printf(\"%s=%d\\n\", p.name.c_str(), static_cast&lt;int&gt;(p.value));\n        }\n    }\n</code></pre> <p>The following code sets a parameter on the Pixhawk to disable the USB safety check (this is handy if you are controlling the Pixhawk over USB using another onboard computer that is part of the drone itself).  You should NOT do this if you are connecting your PC or laptop to the drone over USB.</p> <pre><code>    MavLinkParameter p;\n    p.name = \"CBRK_USB_CHK\";\n    p.value = 197848;\n    if (!vehicle-&gt;setParameter(p).wait(3000,&amp;rc) || !rc) {\n        printf(\"Setting the CBRK_USB_CHK failed\");\n    }\n</code></pre> <p>MavLinkVehicle actually has a helper method for this called allowFlightControlOverUsb, so now you know how it is implemented :-) </p>"},{"location":"mavlink_comm.html#advanced-connections","title":"Advanced Connections","text":"<p>You can wire up different configurations of mavlink pipelines using the MavLinkConnection class \"join\" method as shown below.</p> <p>Example 1, we connect to PX4 over serial, and proxy those messages through to QGroundControl and the LogViewer who are listening on remote ports.  </p> <p></p> <p>Example 2: simulation can talk to jMavSim and jMavSim connects to PX4.  jMavSim can also manage multiple connections, so it can talk to unreal simulator.  Another MavLinkConnection can be joined to proxy connections that jMavSim doesn't support, like the LogViewer or a remote camera node.</p> <p></p> <p>Example 3: we use MavLinkConnection to connect to PX4 over serial, then join additional connections for all our remote nodes including jMavSim.</p> <p></p> <p>Example 4: We can also do distributed systems to control the drone remotely:</p> <p></p>"},{"location":"mavlink_logviewer.html","title":"Log Viewer","text":"<p>The <code>LogViewer</code> is a Windows WPF app that presents the MavLink streams that it is getting from the Unreal Simulator.  You can use this to monitor what is happening on the drone while it is flying. For example, the picture below shows  a real time graph of the x, y an z gyro sensor information being generated by the simulator.</p>"},{"location":"mavlink_logviewer.html#usage","title":"Usage","text":"<p>You can open a log file, it supports .mavlink and PX4 *.ulg files, then you will see the contents of the log in a tree view on the left, whatever metrics you select will be added to the right the right side. You can close each individual chart with the little close box in the top right of each chart and you can group charts so they share the same vertical axis using the group charts button on the top toolbar.</p> <p></p> <p>There is also a map option which will plot the GPS path the drone took.  You can also load multiple log files so you can compare the data from each.</p>"},{"location":"mavlink_logviewer.html#realtime","title":"Realtime","text":"<p>You can also get a realtime view if you connect the LogViewer <code>before</code> you run the simulation.</p> <p></p> <p>For this to work you need to configure the <code>settings.json</code> with the following settings:</p> <pre><code>{\n    \"SettingsVersion\": 1.2,\n    \"SimMode\": \"Multirotor\",\n    \"Vehicles\": {\n        \"PX4\": {\n            ...,\n            \"LogViewerHostIp\": \"127.0.0.1\",\n            \"LogViewerPort\": 14388,\n        }\n    }\n}\n</code></pre> <p>Note</p> <p>Do not use the \"Logs\" setting when you want realtime LogViewer logging. Logging to a file using \"Logs\" is mutually exclusive with LogViewer logging.</p> <p>Simply press the blue connector button on the top right corner of the window, select the Socket <code>tab</code>, enter the port number <code>14388</code>, and your <code>localhost</code> network.  If you are using WSL 2 on Windows then select <code>vEthernet (WSL)</code>.</p> <p>If you do choose <code>vEthernet (WSL)</code> then make sure you also set <code>LocalHostIp</code> and <code>LogViewerHostIp</code> to the matching WSL ethernet address, something like <code>172.31.64.1</code>.</p> <p>Then press the record button (triangle on the right hand side of the toolbar). Now start the simulator, and the data will start streaming into LogViewer.</p> <p>The drone view in Log Viewer shows the actual estimated position coming from the PX4, so that is a great way to check whether the PX4 is in sync with the simulator.  Sometimes you can see some drift here as the attitude estimation catches up with reality, this can become more visible after a bad crash.</p>"},{"location":"mavlink_logviewer.html#installation","title":"Installation","text":"<p>If you can't build the LogViewer.sln, there is also a click once installer.</p>"},{"location":"mavlink_logviewer.html#configuration","title":"Configuration","text":"<p>The magic port number 14388 can be configured in the simulator by editing the settings.json file.  If you change the port number in LogViewer connection dialog then be sure to make the matching changes in your <code>settings.json</code> file.</p>"},{"location":"mavlink_logviewer.html#debugging","title":"Debugging","text":"<p>See PX4 Logging for more information on how to use the LogViewer to debug situations you are setting.</p>"},{"location":"mavlink_mocap.html","title":"MAVLink Motion Capture (MavLinkMoCap)","text":"<p>This folder contains the <code>MavLinkMoCap</code> library which connects to an <code>OptiTrack</code> camera system for accurate and precise indoor location.</p>"},{"location":"mavlink_mocap.html#dependencies","title":"Dependencies:","text":"<ul> <li>OptiTrack Motive.</li> <li>MavLinkCom.</li> </ul>"},{"location":"mavlink_mocap.html#setup-rigidbody","title":"Setup RigidBody","text":"<p>First, you need to define a <code>RigidBody</code> named 'Quadrocopter' using <code>Motive</code>. See Rigid_Body_Tracking.</p>"},{"location":"mavlink_mocap.html#mavlinktest","title":"MavLinkTest","text":"<p>Use <code>MavLinkTest</code> to talk to your PX4 drone, with <code>-server:addr:port</code>, for example, when connected to drone wifi use: </p> <ul> <li><code>MavLinkMoCap -server:10.42.0.228:14590 \"-project:D:\\OptiTrack\\Motive Project 2016-12-19 04.09.42 PM.ttp\"</code> </li> </ul> <p>This publishes the <code>ATT_POS_MOCAP</code> messages and you can proxy those through to the PX4 by running MavLinkTest on the dronebrain using:</p> <ul> <li><code>MavLinkTest -serial:/dev/ttyACM0,115200 -proxy:10.42.0.228:14590</code></li> </ul> <p>Now the drone will get the <code>ATT_POS_MOCAP</code> and you should see the light turn green meaning it is now has a home position and is ready to fly.</p>"},{"location":"mavlink_playback.html","title":"MAVLink Command Playback","text":"<p><code>AutonomySim</code> supports playing back the high-level commands in a <code>.mavlink</code> log file that were recorded using the <code>MavLinkTest</code> application for the purpose of comparing real and simulated flight. The recording.mavlink is an example of a log file captured using a real drone using the following commands:</p> <pre><code>MavLinkTest -serial:/dev/ttyACM0,115200 -logdir:. \n</code></pre> <p>The log file contains the commands performed, which included several \"orbit\" commands. The resulting GPS map of the flight looks like this:</p> <p></p>"},{"location":"mavlink_playback.html#side-by-side-comparison","title":"Side-by-side comparison","text":"<p>Now we can copy the *.mavlink log file recorded by <code>MavLinkTest</code> to the PC running the Unreal simulator with AutonomySim plugin. When the Simulator is running and the drone is parked in a place in a map that has room to do the same maneuvers we can run this <code>MavLinkTest</code> command line:</p> <pre><code>MavLinkTest -server:127.0.0.1:14550\n</code></pre> <p>This should connect to the simulator. Now you can enter this command:</p> <pre><code>PlayLog recording.mavlink\n</code></pre> <p>The same commands you performed on the real drone will now play again in the simulator. You can then press <code>t</code> to see the trace, and it will show you the trace of the real drone and the simulated drone. Every time you press <code>t</code> again you can reset the lines so they are synched to the current position, this way I was able to capture a side-by-side trace of the \"orbit\" command performed in this recording, which generates the picture below.  The pink line is the simulated flight and the red line is the real flight:</p> <p></p> <p>Note</p> <p>We use the <code>;</code> key in the simulator to take control of camera position using keyboard to get this shot.</p>"},{"location":"mavlink_playback.html#parameters","title":"Parameters","text":"<p>It may help to set the simulator up with some of the same flight parameters that your real drone is using. For example, in my case, I was using a lower than normal cruise speed, slow takeoff speed, and it helps to tell the simulator to wait a long time before disarming (<code>COM_DISARM_LAND</code>) and to turn off the safety switches <code>NAV_RCL_ACT</code> and <code>NAV_DLL_ACT</code> (do not do that on a real drone).</p> <pre><code>param MPC_XY_CRUISE 2\nparam MPC_XY_VEL_MAX 2\nparam MPC_TKO_SPEED 1\nparam COM_DISARM_LAND 60\nparam NAV_RCL_ACT 0\nparam NAV_DLL_ACT 0\n</code></pre>"},{"location":"mesh_access.html","title":"Accessing Meshes","text":"<p><code>AutonomySim</code> supports the ability to access the static meshes that make up the scene.</p>"},{"location":"mesh_access.html#mesh-data-structure","title":"Mesh Data Structure","text":"<p>Each mesh is represented with the below struct:</p> <pre><code>struct MeshPositionVertexBuffersResponse {\n    Vector3r position;\n    Quaternionr orientation;\n    std::vector&lt;float&gt; vertices;\n    std::vector&lt;uint32_t&gt; indices;\n    std::string name;\n};\n</code></pre> <ul> <li>The position and orientation are in the Unreal coordinate system.</li> <li>The mesh itself is a triangular mesh represented by the vertices and the indices.</li> <li>The triangular mesh type is typically called a Face-Vertex Mesh. This means every triplet of indices hold the indexes of the vertices that make up the triangle/face.</li> <li>The x,y,z coordinates of the vertices are all stored in a single vector. This means the vertices vector is Nx3 where N is number of vertices. </li> <li>The position of the vertices are the global positions in the Unreal coordinate system. This means they have already been transformed by the position and orientation.</li> </ul>"},{"location":"mesh_access.html#methods","title":"Methods","text":"<p>The API to get the meshes in the scene is quite simple. However, one should note that the function call is very expensive and should very rarely be called. In general this is ok because this function only accesses the static meshes which for most applications are not changing during the duration of your program.</p> <p>Note that you will have to use a 3rdparty library or your own custom code to actually interact with the received meshes. Below I utilize the Python bindings of libigl to visualize the received meshes.</p> <pre><code>import AutonomySim\n\nAUTONOMYSIM_HOST_IP='127.0.0.1'\nclient = AutonomySim.VehicleClient(ip=AUTONOMYSIM_HOST_IP)\nclient.confirmConnection()\n\n# List of returned meshes are received via this function\nmeshes=client.simGetMeshPositionVertexBuffers()\n\nindex=0\nfor m in meshes:\n    # Finds one of the cube meshes in the Blocks environment\n    if 'cube' in m.name:\n\n        # Code from here on relies on libigl. Libigl uses pybind11 to wrap C++ code. So here the built pyigl.so\n        # library is in the same directory as this example code.\n        # This is here as code for your own mesh library should require something similar\n        from pyigl import *\n        from iglhelpers import *\n\n        # Convert the lists to numpy arrays\n        vertex_list=np.array(m.vertices,dtype=np.float32)\n        indices=np.array(m.indices,dtype=np.uint32)\n\n        num_vertices=int(len(vertex_list)/3)\n        num_indices=len(indices)\n\n        # Libigl requires the shape to be Nx3 where N is number of vertices or indices\n        # It also requires the actual type to be double(float64) for vertices and int64 for the triangles/indices\n        vertices_reshaped=vertex_list.reshape((num_vertices,3))\n        indices_reshaped=indices.reshape((int(num_indices/3),3))\n        vertices_reshaped=vertices_reshaped.astype(np.float64)\n        indices_reshaped=indices_reshaped.astype(np.int64)\n\n        # Libigl function to convert to internal Eigen format\n        v_eig=p2e(vertices_reshaped)\n        i_eig=p2e(indices_reshaped)\n\n        # View the mesh\n        viewer = igl.glfw.Viewer()\n        viewer.data().set_mesh(v_eig,i_eig)\n        viewer.launch()\n        break\n</code></pre>"},{"location":"object_detection.html","title":"Object Detection","text":""},{"location":"object_detection.html#about","title":"About","text":"<p>This feature lets you generate object detectors using existing cameras in <code>AutonomySim</code>, similar to deep neural network (DNN) object detectors. Using the API, you can control which object to detect by name and the radial distance from the camera. One can control the settings for each camera, image type, and vehicle combination separately.</p>"},{"location":"object_detection.html#api","title":"API","text":"<ul> <li> <p>Set the mesh name to detect in wildcard format:</p> <p><code>simAddDetectionFilterMeshName(camera_name, image_type, mesh_name, vehicle_name = '')</code></p> </li> <li> <p>Clear all mesh names previously added:</p> <p><code>simClearDetectionMeshNames(camera_name, image_type, vehicle_name = '')</code> </p> </li> <li> <p>Set the detection radius in centimeters:</p> <p><code>simSetDetectionFilterRadius(camera_name, image_type, radius_cm, vehicle_name = '')</code></p> </li> <li> <p>Get the detections:</p> <p><code>simGetDetections(camera_name, image_type, vehicle_name = '')</code></p> </li> </ul> <p>The return value of <code>simGetDetections</code> is a <code>DetectionInfo</code> array:</p> <pre><code>DetectionInfo\n    name = ''\n    geo_point = GeoPoint()\n    box2D = Box2D()\n    box3D = Box3D()\n    relative_pose = Pose()\n</code></pre>"},{"location":"object_detection.html#usage","title":"Usage","text":"<p>The Python script <code>detection.py</code> shows how to set detection parameters and shows the result in OpenCV capture.</p> <p>A minimal example using API with Blocks environment to detect Cylinder objects is below:</p> <pre><code>camera_name = \"0\"\nimage_type = AutonomySim.ImageType.Scene\n\nclient = AutonomySim.MultirotorClient()\nclient.confirmConnection()\n\nclient.simSetDetectionFilterRadius(camera_name, image_type, 80 * 100) # in [cm]\nclient.simAddDetectionFilterMeshName(camera_name, image_type, \"Cylinder_*\") \nclient.simGetDetections(camera_name, image_type)\ndetections = client.simClearDetectionMeshNames(camera_name, image_type)\n</code></pre> <p>Output result:</p> <pre><code>Cylinder: &lt;DetectionInfo&gt; {\n    'name': 'Cylinder9_2',\n    'geo_point': &lt;GeoPoint&gt; {\n        'altitude': 16.979999542236328,\n        'latitude': 32.28772183970703,\n        'longitude': 34.864785008379876\n    },\n    'box2D': &lt;Box2D&gt; {\n        'max': &lt;Vector2r&gt; {\n            'x_val': 617.025634765625,\n            'y_val': 583.5487060546875\n        },\n        'min': &lt;Vector2r&gt; {\n            'x_val': 485.74359130859375,\n            'y_val': 438.33465576171875\n        }\n    },\n    'box3D': &lt;Box3D&gt; {\n        'max': &lt;Vector3r&gt; {\n            'x_val': 4.900000095367432,\n            'y_val': 0.7999999523162842,\n            'z_val': 0.5199999809265137\n        },\n        'min': &lt;Vector3r&gt; {\n            'x_val': 3.8999998569488525,\n            'y_val': -0.19999998807907104,\n            'z_val': 1.5199999809265137\n        }\n    },\n    'relative_pose': &lt;Pose&gt; {\n        'orientation': &lt;Quaternionr&gt; {\n            'w_val': 0.9929741621017456,\n            'x_val': 0.0038591264747083187,\n            'y_val': -0.11333247274160385,\n            'z_val': 0.03381215035915375\n        },\n        'position': &lt;Vector3r&gt; {\n            'x_val': 4.400000095367432,\n            'y_val': 0.29999998211860657,\n            'z_val': 1.0199999809265137\n        }\n    }\n}\n</code></pre> <p> </p>"},{"location":"orbit.html","title":"Orbital Trajectories","text":"<p>Moved here from https://github.com/nervosys/AutonomySim/wiki/An-Orbit-Trajectory</p> <p>Have you ever wanted to fly a nice smooth circular orbit? This can be handy for capturing 3-D objects from all sides, especially if you get multiple orbits at different altitudes.</p> <p>The <code>PythonClient/multirotor</code> folder contains a script named Orbit that will do precisely that.</p> <p>See demo video</p> <p>The demo video was created by running this command line:</p> <pre><code>python orbit.py --radius 10 --altitude 5 --speed 1 --center \"0,1\" --iterations 1\n</code></pre> <p>This flies a 10-meter radius orbit around the center location at <code>(startpos + radius * [0,1])</code>. In other words, the center is located <code>radius</code> meters away in the direction of the provided center vector. This also keeps the front-facing camera on the drone always pointing at the center of the circle. If you watch the flight using <code>LogViewer</code>, you will see a nice circular pattern get traced out on the GPS map:</p> <p></p> <p>The core of the algorithm is uncomplicated. At each point on the circle, we look ahead by a small delta in degrees, called the <code>lookahead_angle</code>, with the angle computed based on our desired velocity. We then find the lookahead point on the circle using sine/cosine and make that our target point. Calculating the velocity is then easy; just subtract the current position from that point and feed this into the AutonomySim method, <code>moveByVelocityZ()</code>.</p>"},{"location":"performance_hardware.html","title":"Hardware Performance","text":"<p>It is not required, but we recommend running your Unreal Environment on a Solid State Drive (SSD). Between debugging, logging, and Unreal asset loading the hard drive can become your bottle neck.  It is normal that your hard drive will be slammed while Unreal is loading the environment, but if your hard drive performance looks like this while the Unreal game is running then you will probably not get a good flying experience.  </p> <p></p> <p>In fact, if the hard drive is this busy, chances are the drone will not fly properly at all. For some unknown reason this I/O bottle neck also interferes with the drone control loop and if that loop doesn't run at a high rate (300-500 Hz) then the drone will not fly.  Not surprising, the control loop inside the PX4 firmware that runs on a Pixhawk flight controller runs at 1000 Hz.</p>"},{"location":"performance_hardware.html#reducing-io","title":"Reducing I/O","text":"<p>If you can't whip off to Fry's Electronics and pick up an overpriced super fast SSD this weekend, then the following steps can be taken to reduce the hard drive I/O:</p> <ol> <li>First run the Unreal Environment using Cooked content outside of the UE Editor or any debugging environment, and package the content to your fastest SSD drive.  You can do that using this menu option:</li> </ol> <p></p> <ol> <li>If you must use the UE editor (because you are actively modifying game assets), then at least don't run that in a debugger.  If you are using Visual Studio use start without debugging.</li> <li>If you must debug the app, and you are using Visual Studio debugger, stop then Visual Studio from logging Intellitrace information. Go to Tools/Options/Debugging/Intellitrace, and turn off the main checkbox.</li> <li>Turn off any Unreal Analytics that your environment may have enabled, especially any file logging.</li> </ol>"},{"location":"performance_hardware.html#io-from-page-faults","title":"I/O from Page Faults","text":"<p>If your system is running out of RAM it may start paging memory to disk.  If your operating system has enabled paging to disk, make sure it is paging to your fastest SSD.  Or if you have enough RAM disable paging all together.  In fact, if you disable paging and the game stops working you will know for sure you are running out of RAM.</p> <p>Obviously, shutting down any other unnecessary apps should also free up memory so you don't run out.</p>"},{"location":"performance_hardware.html#ideal-runtime-performance","title":"Ideal Runtime performance","text":"<p>This is what my slow hard drive looks like when flying from UE editor.  You can see it's very busy, but the drone still flies ok:</p> <p></p> <p>This is what my fast SSD looks like when the drone is flying in an Unreal Cooked app (no UE editor, no debugger). Not surprisingly it is flying perfectly in this case:</p> <p></p>"},{"location":"performance_software.html","title":"Software Performance","text":"<p>Coming soon.</p>"},{"location":"project_binaries.html","title":"Project Binaries","text":"<p>You can simply download precompiled binaries and run to get started immediately. If you want to set up your own Unreal environment then please see these instructions.</p>"},{"location":"project_binaries.html#unreal-engine","title":"Unreal Engine","text":"<p>Windows, Linux</p> <p>Download the binaries for the environment of your choice from the latest release.</p> <p>Some pre-compiled environment binaries may include multiple files (i.e. City.zip.001, City.zip.002). Make sure to download both files before starting the environment. Use 7zip to unzip these files. On Linux, pass the first zip file name as argument and it should detect all the other parts as well - <code>7zz x TrapCamera.zip.001</code></p> <p>macOS</p> <p>You will need to build it yourself</p>"},{"location":"project_binaries.html#unity-experimental","title":"Unity (Experimental)","text":"<p>A free environment called Windridge City is available at Unity Asset Store as an experimental release of AutonomySim on Unity.</p> <p>Note</p> <p>This is an old release, and many of the features and APIs might not work.</p>"},{"location":"project_binaries.html#controlling-vehicles","title":"Controlling Vehicles","text":"<p>Most of our users typically use APIs to control the vehicles. However you can also control vehicles manually. You can drive the car using keyboard, gamepad or steering wheel. To fly drone manually, you will need either XBox controller or a remote control (feel free to contribute keyboard support). Please see remote control setup for more details. Alternatively you can use APIs for programmatic control or use so-called Computer Vision mode to move around in environment using the keyboard.</p>"},{"location":"project_binaries.html#dont-have-good-gpu","title":"Don't Have Good GPU?","text":"<p>The AutonomySim binaries, like CityEnviron, requires a beefy GPU to run smoothly. You can run them in low resolution mode by editing the <code>run.cmd</code> file (if it doesn't exist, create it with the following content) on Windows like this:</p> <pre><code>start CityEnviron -ResX=640 -ResY=480 -windowed\n</code></pre> <p>For Linux binaries, use the <code>Blocks.sh</code> or corresponding shell script as follows -</p> <pre><code>./Blocks.sh -ResX=640 -ResY=480 -windowed\n</code></pre> <p>Check out all the other command-line options</p> <p>UE 4.24 uses Vulkan drivers by default, but they can consume more GPU memory. If you get memory allocation errors, then you can try switching to OpenGL using <code>-opengl</code></p> <p>You can also limit the maximum FPS using the <code>simRunConsoleCommand()</code> API as follows-</p> <pre><code>&gt;&gt;&gt; import AutonomySim\n&gt;&gt;&gt; client = AutonomySim.VehicleClient()\n&gt;&gt;&gt; client.confirmConnection()\nConnected!\nClient Ver:1 (Min Req: 1), Server Ver:1 (Min Req: 1)\n\n&gt;&gt;&gt; client.simRunConsoleCommand(\"t.MaxFPS 10\")\nTrue\n</code></pre>"},{"location":"project_structure.html","title":"Project Structure","text":""},{"location":"project_structure.html#autonomylib","title":"AutonomyLib","text":"<p>The majority of the code is located in <code>AutonomyLib</code>, a self-contained library that can be compiled with any popular <code>C++11</code> compiler.</p> <p><code>AutonomyLib</code> consists of the following components:</p> <ol> <li>Physics engine: This is header-only physics engine. It is designed to be fast and extensible to implement different vehicles.</li> <li>Sensor models: These are header-only models for the barometer, IMU, GPS and magnetometer.</li> <li>Vehicle models: These are header-only models for vehicle configurations and models. Currently, we have implemented model for a multirotor and a configuration for a PX4 quadrotor in the <code>X config</code>. There are several different multirotor models defined in <code>MultirotorParams.hpp</code>, including a hexacopter as well.</li> <li>API-related files: This part of <code>AutonomyLib</code> provides abstract base classes for our APIs and concrete implementations for specific vehicle platforms such as <code>MavLink</code>. It also contains classes for the RPC client and server.</li> </ol> <p>Apart from these, all common utilities are defined in the <code>common/</code> subfolder. One important file here is <code>AutonomySimSettings.hpp</code>, which should be modified if any new fields are to be added in <code>settings.json</code>.</p> <p><code>AutonomySim</code> supports different firmwares for multirotors such as its own <code>SimpleFlight</code>, <code>PX4</code>, and <code>ArduPilot</code>. Files for communicating with each firmware are placed in their respective subfolders in <code>multirotor/firmwares</code>.</p> <p>Vehicle-specific APIs are defined in the <code>api/</code> subfolder along-with required data structures. The <code>AutonomyLib/src/</code> directory contains <code>.cpp</code> files with implementations of various methods defined in the <code>.hpp</code> files. For e.g. <code>MultirotorApiBase.cpp</code> contains the base implementation of the multirotor APIs, which can be overridden in the specific firmware files.</p>"},{"location":"project_structure.html#unrealpluginsautonomysim","title":"Unreal/Plugins/AutonomySim","text":"<p>This is the only portion of project which is dependent on Unreal engine. We have kept it isolated so we can implement simulator for other platforms as well. The Unreal code takes advantage of its UObject based classes including Blueprints. The <code>Source/</code> folder contains the C++ files, while the <code>Content/</code> folder has the blueprints and assets. Some main components are described below:</p> <ol> <li>SimMode_ classes: The SimMode classes help implement many different modes, such as pure Computer Vision mode, where there is no vehicle or simulation for a specific vehicle (currently car and multirotor). The vehicle classes are located in <code>Vehicles/</code></li> <li>PawnSimApi: This is the base class for all vehicle pawn visualizations. Each vehicle has their own child (Multirotor|Car|ComputerVision)Pawn class.</li> <li>UnrealSensors: Contains implementation of Distance and Lidar sensors.</li> <li>WorldSimApi: Implements most of the environment and vehicle-agnostic APIs</li> </ol> <p>Apart from these, <code>PIPCamera</code> contains the camera initialization, and <code>UnrealImageCapture</code> &amp; <code>RenderRequest</code> the image rendering code. <code>AutonomyBlueprintLib</code> has a lot of utility and wrapper methods used to interface with the UE4 engine.</p>"},{"location":"project_structure.html#mavlinkcom","title":"MavLinkCom","text":"<p>This is the library developed by our own team member Chris Lovett that provides C++ classes to talk to the MavLink devices. This library is stand alone and can be used in any project. See MavLinkCom for more info.</p>"},{"location":"project_structure.html#sample-programs","title":"Sample Programs","text":"<p>We have created a few sample programs to demonstrate how to use the API. See HelloDrone and DroneShell. DroneShell demonstrates how to connect to the simulator using UDP.  The simulator is running a server (similar to DroneServer).</p>"},{"location":"project_structure.html#python-client","title":"Python Client","text":"<p>PythonClient contains Python API wrapper files and sample programs demonstrating their uses.</p>"},{"location":"project_structure.html#unreal-engine","title":"Unreal Engine","text":"<p>The below figure illustrates how <code>AutonomySim</code> is loaded and invoked by the Unreal Engine:</p> <p></p>"},{"location":"project_structure.html#contributing","title":"Contributing","text":"<p>See Contribution Guidelines</p>"},{"location":"px4_build.html","title":"Building PX4","text":""},{"location":"px4_build.html#source-code","title":"Source code","text":"<p>Getting the PX4 source code is easy:</p> <pre><code>sudo apt-get install git\ngit clone https://github.com/PX4/PX4-Autopilot.git --recursive\nbash ./PX4-Autopilot/Tools/setup/ubuntu.sh --no-sim-tools\ncd PX4-Autopilot\n</code></pre> <p>To build it, you will need the right tools.</p>"},{"location":"px4_build.html#px4-build-tools","title":"PX4 Build tools","text":"<p>The full instructions are available on the dev.px4.io website, but we've copied the relevant subset of those instructions here for your convenience.</p> <p>(Note that WSL) can be used to build the PX4 firmware, just follow the BashOnWindows instructions at the bottom of this page) then proceed with the Ubuntu setup for PX4.</p>"},{"location":"px4_build.html#build-sitl-version","title":"Build SITL version","text":"<p>Now, you can make the SITL version that runs in POSIX from the Firmware folder you created above:</p> <pre><code>make px4_sitl_default none_iris\n</code></pre> <p>This build system is quite special, it knows how to update git submodules (and there's a lot of them), then it runs cmake (if necessary), then it runs the build itself. So in a way the root Makefile is a meta-meta makefile. You might see prompts like this:</p> <pre><code> *******************************************************************************\n *   IF YOU DID NOT CHANGE THIS FILE (OR YOU DON'T KNOW WHAT A SUBMODULE IS):  *\n *   Hit 'u' and &lt;ENTER&gt; to update ALL submodules and resolve this.            *\n *   (performs git submodule sync --recursive                                  *\n *    and git submodule update --init --recursive )                            *\n *******************************************************************************\n</code></pre> <p>Every time you see this prompt, press <code>u</code> on your keyboard. It should take about 2 minutes. If everything succeeds, the last line will link the <code>px4</code> application, which you can then run using the following command:</p> <pre><code>make px4_sitl_default none_iris\n</code></pre> <p>Now, you should see output that looks like this:</p> <pre><code>creating new parameters file\ncreating new dataman file\n\n______  __   __    ___ \n| ___ \\ \\ \\ / /   /   |\n| |_/ /  \\ V /   / /| |\n|  __/   /   \\  / /_| |\n| |     / /^\\ \\ \\___  |\n\\_|     \\/   \\/     |_/\n\npx4 starting.\n\n18446744073709551615 WARNING: setRealtimeSched failed (not run as root?)\nERROR [param] importing from 'rootfs/eeprom/parameters' failed (-1)\nCommand 'param' failed, returned 1\n  SYS_AUTOSTART: curr: 0 -&gt; new: 4010\n  SYS_MC_EST_GROUP: curr: 2 -&gt; new: 1\nINFO  [dataman] Unkown restart, data manager file 'rootfs/fs/microsd/dataman' size is 11797680 bytes\n  BAT_N_CELLS: curr: 0 -&gt; new: 3\n  CAL_GYRO0_ID: curr: 0 -&gt; new: 2293768\n  CAL_ACC0_ID: curr: 0 -&gt; new: 1376264\n  CAL_ACC1_ID: curr: 0 -&gt; new: 1310728\n  CAL_MAG0_ID: curr: 0 -&gt; new: 196616\n</code></pre> <p>The first run sets up the PX4 parameters for SITL mode. The second run has less output. This app is also an interactive console where you can type commands. Type 'help' to see what they are and just type ctrl-C to kill it. You can do that and restart it any time, that's a great way to reset any wonky state if you need to (it's equivalent to a Pixhawk hardware reboot).</p>"},{"location":"px4_build.html#arm-embedded-tools","title":"ARM embedded tools","text":"<p>If you plan to build the PX4 firmware for real Pixhawk hardware then you will need the <code>gcc</code> cross-compiler for ARM Cortex-M4 chipset. You can get this compiler by PX4 DevGuide, specifically this is in their <code>ubuntu_sim_nuttx.sh</code> setup script.</p> <p>After following those setup instructions you can verify the install by entering this command <code>arm-none-eabi-gcc --version</code>.  You should see the following output:</p> <pre><code>arm-none-eabi-gcc (GNU Tools for Arm Embedded Processors 7-2017-q4-major) 7.2.1 20170904 (release) [ARM/embedded-7-branch revision 255204]\nCopyright (C) 2017 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n</code></pre>"},{"location":"px4_build.html#build-px4-for-arm-hardware","title":"Build PX4 for ARM hardware","text":"<p>Now, you can build the PX4 firmware and run it on real Pixhawk hardware:</p> <pre><code>make px4_fmu-v4\n</code></pre> <p>This build will take a little longer because it is building a lot more including the NuttX real time OS, all the drivers for the sensors in the Pixhawk flight controller, and more.  It is also running the compiler in super size-squeezing mode so it can fit all that in a 1 megabyte ROM!</p> <p>One nice tid bit is you can plug in your pixhawk USB, and type <code>make px4fmu-v2_default upload</code> to flash the hardware with these brand new bits, so you don't need to use QGroundControl for that.</p>"},{"location":"px4_build.html#some-useful-parameters","title":"Some Useful Parameters","text":"<p>PX4 has many customizable parameters (over 700 of them, in fact) and to get best results with AutonomySim we have found the following parameters are handy:</p> <pre><code>// be sure to enable the new position estimator module:\nparam set SYS_MC_EST_GROUP 2\n\n// increase default limits on cruise speed so you can move around a large map more quickly.\nparam MPC_XY_CRUISE 10             \nparam MPC_XY_VEL_MAX 10\nparam MPC_Z_VEL_MAX_DN 2\n\n// increase timeout for auto-disarm on landing so that any long running app doesn't have to worry about it\nparam COM_DISARM_LAND 60\n\n// make it possible to fly without radio control attached (do NOT do this one on a real drone)\nparam NAV_RCL_ACT 0\n\n// enable new syslogger to get more information from PX4 logs\nparam set SYS_LOGGER 1\n</code></pre>"},{"location":"px4_build.html#using-bashonwindows","title":"Using BashOnWindows","text":"<p>See Bash on Windows Toolchain.</p>"},{"location":"px4_lockstep.html","title":"PX4 Lockstep Mode","text":"<p>The latest version of PX4 supports a new lockstep feature when communicating with the simulator over TCP. Lockstep is an important feature because it synchronizes PX4 and the simulator so that they use the same clock time. This makes PX4 behave normally even during unusually long delays in simulator response due to performance lags.</p> <p>Recommendation</p> <p>When you are running a lockstep enabled version of PX4 in software-in-the-loop (SITL) mode, set <code>AutonomySim</code> to use a <code>SteppableClock</code> and set <code>UseTcp</code> and <code>LockStep</code> to <code>true</code>.</p> <pre><code>{\n    \"SettingsVersion\": 1.2,\n    \"SimMode\": \"Multirotor\",\n    \"ClockType\": \"SteppableClock\",\n    \"Vehicles\": {\n        \"PX4\": {\n            \"VehicleType\": \"PX4Multirotor\",\n            \"UseTcp\": true,\n            \"LockStep\": true,\n            ...\n</code></pre> <p>This causes <code>AutonomySim</code> to not use a real-time clock, but instead to advance the clock in-step with each sensor update sent to PX4. This way, PX4 perceives time is progressing smoothly no matter how long it takes <code>AutonomySim</code> to process the update loop.</p> <p>This has the following advantages:</p> <ul> <li><code>AutonomySim</code> can be run on slow machines that cannot process updates quickly.</li> <li>You can debug <code>AutonomySim</code>, hit a breakpoint, and when you resume, PX4 will behave normally.</li> <li>You can enable very slow sensors such as LiDAR with a large number of simulated points and PX4 will still behave normally.</li> </ul> <p>However, the <code>lockstep</code> feature does have side effects. These include slower update loops caused by running <code>AutonomySim</code> on an underpowered machine or by expensive sensors (e.g., LiDAR), which can create visible jerkiness in the simulated flight, if you view the on-screen updates in real-time.</p>"},{"location":"px4_lockstep.html#disabling-lockstep-mode","title":"Disabling Lockstep Mode","text":"<p>If you are running PX4 in <code>Cygwin</code>, there is an open issue with lockstep. PX4 is configured to use lockstep by default. To disable this feature, first disable it in PX4:</p> <ol> <li>Navigate to <code>boards/px4/sitl/</code> in your local PX4 repository</li> <li>Edit <code>default.cmake</code> and find the following line:     <pre><code>set(ENABLE_LOCKSTEP_SCHEDULER yes)\n</code></pre></li> <li>Change this line to:     <pre><code>set(ENABLE_LOCKSTEP_SCHEDULER no)\n</code></pre></li> <li>Disable it in AutonomySim by setting <code>LockStep</code> to <code>false</code> and either removing any <code>\"ClockType\":  \"SteppableClock\"</code> setting or resetting <code>ClockType</code> back to default: <pre><code>    {\n        ...\n        \"ClockType\": \"\",\n        \"Vehicles\": {\n            \"PX4\": {\n                \"VehicleType\": \"PX4Multirotor\",\n                \"LockStep\": false,\n                ...\n</code></pre></li> <li>Now you can run PX4 SITL as you normally would (<code>make px4_sitl_default none_iris</code>) and it will use the host system time without waiting on <code>AutonomySim</code>.</li> </ol>"},{"location":"px4_logging.html","title":"PX4/MavLink Logging","text":"<p>Thanks to Chris Lovett for developing various tools for PX4/MavLink logging mentioned on this page!</p>"},{"location":"px4_logging.html#logging-mavlink-messages","title":"Logging MavLink Messages","text":"<p><code>AutonomySim</code> can capture mavlink log files if you add the following to the PX4 section of your <code>settings.json</code> file:</p> <pre><code>{\n    \"SettingsVersion\": 1.2,\n    \"SimMode\": \"Multirotor\",\n    \"Vehicles\": {\n        \"PX4\": {\n            ...,\n            \"Logs\": \"c:/temp/mavlink\"\n        }\n    }\n}\n</code></pre> <p><code>AutonomySim</code> will create a time-stamped log file in this folder for each \"armed/disarmed\" flight session.</p> <p>You will then see log files organized by date in <code>[DRIVE]:\\temp\\logs</code>, specifically <code>*input.mavlink</code> and <code>*output.mavlink</code> files.</p>"},{"location":"px4_logging.html#mavlink-logviewer","title":"MavLink LogViewer","text":"<p>For MavLink enabled drones, you can also use our Log Viewer to visualize the streams of data. If you enable this form of realtime logging you should not use the \"Logs\" setting above, these two forms of logging are mutually exclusive.</p>"},{"location":"px4_logging.html#px4-log-in-sitl-mode","title":"PX4 Log in SITL Mode","text":"<p>In SITL mode, please a log file is produced when drone is armed. The software-in-the-loop (SITL) terminal will contain the path to the log file, it should look something like this</p> <pre><code>INFO  [logger] Opened log file: rootfs/fs/microsd/log/2017-03-27/20_02_49.ulg\n</code></pre>"},{"location":"px4_logging.html#px4-log-in-hitl-mode","title":"PX4 Log in HITL Mode","text":"<p>If you are using Pixhawk hardware in hardware-in-the-loop (HITL) mode, then set parameter <code>SYS_LOGGER=1</code> using <code>QGroundControl</code>. PX4 will write log file on device which you can download at later date using QGroundControl.</p>"},{"location":"px4_logging.html#debugging-a-bad-flight","title":"Debugging a bad flight","text":"<p>You can use these <code>*.mavlink</code> log files to debug a bad flight using the LogViewer. For example, AutonomySim/PX4 flight may misbehave if you run it on an under powered computer. The following shows what might happen in that situation.</p> <p></p> <p>In this flight, we ran a simple <code>commander takeoff</code> test as performed by <code>PythonClient/multirotor/stability_test.py</code> and the flight started off fine, but then went crazy at the end and the drone crashed. So why is that?  What can the log file show?</p> <p>Here we've plotted the following 5 metrics:</p> <ul> <li><code>hil_gps.alt</code> - the simulated altitude sent from AutonomySim to PX4.</li> <li><code>telemetry.update_rate</code> - the rate AutonomySim is performing the critical drone update loop in updates per second.</li> <li><code>telemetry.update_time</code> - the average time taken inside AutonomySim performing the critical drone update loop.</li> <li><code>telemetry.actuation_delay</code> - this is a very interesting metric measuring how long it takes PX4 to send back updated actuator controls message (motor power).</li> <li><code>actuator_controls.0</code> - the actuator controls signal from PX4 for the first rotor.</li> </ul> <p>What we see then with these metrics is that things started off nicely, with nice flat altitude, high update rate in the 275 to 300 fps range, and a nice low update time inside <code>AutonomySim</code> around 113 microseconds, and a nice low actuation delay in the round trip from PX4. The actuator controls also stabilize quickly to a nice flat line.</p> <p>But then the <code>update_time</code> starts to climb, at the same time the <code>actuation_delay</code> climbs and we see a little tip in <code>actuator_controls</code>. This dip should not happen, the PX4 is panicking over loss of update rate but it recovers.</p> <p>But then we see actuator controls go crazy, a huge spike in actuation delay, and around this time we see a message from AutonomySim saying <code>lockstep disabled</code>.  A delay over 100 millisecond triggers AutonomySim into jumping out of lockstep mode and the PX4 goes nuts and the drone crashes.</p> <p>The button line is that if a simple <code>takeoff</code> cannot maintain steady smooth flight and you see these kinds of spikes and uneven update rates then it means you are running AutonomySim on a computer that does not have enough horsepower.</p> <p>This is what a simple takeoff, hover, and land (THL) should look like:</p> <p></p> <p>Here, you see the <code>update_rate</code> sticking the target of 333 updates per second. You also see the <code>update_time</code> a nice flat 39 microseconds and the <code>actuator_delay</code> somewhere between 1.1 and 1.7 milliseconds, and the resulting <code>actuator_controls</code> a lovely flat line.</p>"},{"location":"px4_multivehicle.html","title":"PX4 Multi-vehicle Configuration","text":"<p>The PX4 software-in-the-loop (SITL) stack comes with a <code>sitl_multiple_run.sh</code> shell script that runs multiple instances of the PX4 binary. This allows the SITL stack to listen to connections from multiple <code>AutonomySim</code> vehicles on multiple TCP ports starting from 4560. However, the provided script does not let us view the PX4 console. If you want to run the instances manually while being able to view each instance's console (Recommended) see this section.</p>"},{"location":"px4_multivehicle.html#running-multiple-px4-sitl-instances","title":"Running Multiple PX4 SITL Instances","text":"<p>Note</p> <p>You have to build PX4 with <code>make px4_sitl_default none_iris</code> as shown here before trying to run multiple PX4 instances.</p> <ol> <li> <p>From your bash (or <code>Cygwin</code>) terminal go to the PX4 Firmware directory and run the <code>sitl_multiple_run.sh</code> script while specifying the number of vehicles you need.     <pre><code>cd PX4-Autopilot\n./Tools/sitl_multiple_run.sh 2  # 2 = number of vehicles/instances \n</code></pre>     This starts multiple instances that listen to TCP ports 4560 to 4560+i where 'i' is the number of vehicles/instances specified.</p> </li> <li> <p>You should get a confirmation message that says that old instances have been stopped and new instances have been started.     <pre><code>killing running instances\nstarting instance 0 in /cygdrive/c/PX4/home/PX4/Firmware/build/px4_sitl_default/instance_0\nstarting instance 1 in /cygdrive/c/PX4/home/PX4/Firmware/build/px4_sitl_default/instance_1\n</code></pre></p> </li> <li> <p>Now edit AutonomySim settings file to make sure you have matching TCP port settings for the set number of vehicles and to make sure that both vehicles do not spawn on the same point. For example, these settings would spawn two PX4Multirotors where one of them would try to connect to PX4 SITL at port <code>4560</code> and the other at port <code>4561</code>. It also makes sure the vehicles spawn at <code>0,1,0</code> and <code>0,-1,0</code> to avoid collision:     <pre><code>{\n    \"SettingsVersion\": 1.2,\n    \"SimMode\": \"Multirotor\",\n    \"Vehicles\": {\n        \"Drone1\": {\n            \"VehicleType\": \"PX4Multirotor\",\n            \"UseSerial\": false,\n            \"UseTcp\": true,\n            \"TcpPort\": 4560,\n            \"ControlPortLocal\": 14540,\n            \"ControlPortRemote\": 14580,\n            \"X\": 0, \"Y\": 1, \"Z\": 0\n        },\n        \"Drone2\": {\n            \"VehicleType\": \"PX4Multirotor\",\n            \"UseSerial\": false,\n            \"UseTcp\": true,\n            \"TcpPort\": 4561,\n            \"ControlPortLocal\": 14541,\n            \"ControlPortRemote\": 14581,       \n            \"X\": 0, \"Y\": -1, \"Z\": 0\n        }\n    }\n  }\n</code></pre>     You can add more than two vehicles but you will need to make sure you adjust the TCP port for each (ie: vehicle 3's port would be <code>4562</code> and so on..) and adjust the spawn point.</p> </li> <li> <p>Now run your Unreal <code>AutonomySim</code> environment and it should connect to SITL PX4 via TCP. If you are running the instances with the PX4 console visible, you should see a bunch of messages from each SITL PX4 window. Specifically, the following messages tell you that AutonomySim is connected properly and GPS fusion is stable:     <pre><code>INFO  [simulator] Simulator connected on UDP port 14560\nINFO  [mavlink] partner IP: 127.0.0.1\nINFO  [ecl/EKF] EKF GPS checks passed (WGS-84 origin set)\nINFO  [ecl/EKF] EKF commencing GPS fusion\n</code></pre></p> </li> </ol> <p>If you do not see these messages then check your port settings.</p> <p>You should also be able to use <code>QGroundControl</code> in SITL mode. Make sure there is no Pixhawk hardware plugged in, otherwise QGroundControl will choose to use that instead.  Note that as we don't have a physical board, an RC cannot be connected directly to it. So the alternatives are either use <code>XBox 360 Controller</code> or connect your RC using USB (for example, in case of FrSky Taranis X9D Plus) or using trainer USB cable to your PC. This makes your RC look like a joystick. You will need to do extra set up in QGroundControl to use virtual joystick for RC control.  You do not need to do this unless you plan to fly a drone manually in <code>AutonomySim</code>. Autonomous flight using the Python API does not require RC, see <code>No Remote Control</code>.</p>"},{"location":"px4_multivehicle.html#starting-sitl-instances-with-px4-console","title":"Starting SITL instances with PX4 console","text":"<p>If you want to start your SITL instances while being able to view the PX4 console, you will need to run the shell scripts found here rather than <code>sitl_multiple_run.sh</code>.</p> <p>Here is how you would do so:</p> <p>Note</p> <p>This script also assumes PX4 is built with <code>make px4_sitl_default none_iris</code> as shown here before trying to run multiple PX4 instances.</p> <ol> <li>From your bash (or <code>Cygwin</code>) terminal go to the PX4 directory and get the scripts (place them in a subdirectory called Scripts win the PX4 directory as shown)     <pre><code>cd PX4\nmkdir -p Scripts\ncd Scripts\nwget https://github.com/nervosys/AutonomySim/raw/main/PX4Scripts/sitl_kill.sh\nwget https://github.com/nervosys/AutonomySim/raw/main/PX4Scripts/run_autonomysim_sitl.sh\n</code></pre> Note the shell scripts expect the <code>Scripts</code> and <code>Firmware</code> directories to be within the same parent directory. Also, you may need to make the scripts executable by running <code>chmod +x sitl_kill.sh</code> and <code>chmod +x run_autonomysim_sitl.sh</code>.</li> <li>Run the <code>sitl_kill.sh</code> script to kill all active PX4 SITL instances      <pre><code>./sitl_kill.sh\n</code></pre></li> <li> <p>Run the <code>run_autonomysim_sitl.sh</code> script while specifying which instance you would like to run in the current terminal window (the first instance would be numbered 0)     <pre><code>./run_autonomysim_sitl.sh 0 # first instance = 0\n</code></pre></p> <p>You should see the PX4 instance starting and waiting for AutonomySim's connection as it would with a single instance. <pre><code>______  __   __    ___\n| ___ \\ \\ \\ / /   /   |\n| |_/ /  \\ V /   / /| |\n|  __/   /   \\  / /_| |\n| |     / /^\\ \\ \\___  |\n\\_|     \\/   \\/     |_/\n\npx4 starting.\nINFO  [px4] Calling startup script: /bin/sh /cygdrive/c/PX4/home/PX4/Firmware/etc/init.d-posix/rcS 0\nINFO  [dataman] Unknown restart, data manager file './dataman' size is 11798680 bytes\nINFO  [simulator] Waiting for simulator to connect on TCP port 4560\n</code></pre> 4. Open a new terminal and go to the Scripts directory and start the next instance <pre><code>cd PX4\ncd Scripts\n./run_autonomysim_sitl.sh 1  # ,2,3,4,..,etc\n</code></pre> 5. Repeat Step 4 for as many instances as you would like to start 6. Run your Unreal AutonomySim environment and it should connect to SITL PX4 via TCP (assuming your settings.json file has the right ports).</p> </li> </ol>"},{"location":"px4_setup.html","title":"PX4 Setup","text":"<p>The PX4 software stack is an open-source popular flight controller with support for wide variety of boards and sensors as well as built-in capability for higher level tasks such as mission planning. Please visit px4.io for more information.</p> <p>Setting up PX4 can be challenging</p> <p>While all releases of <code>AutonomySim</code> are always tested with PX4 to ensure support, setting up PX4 is not a trivial task. Unless you have at least intermediate level of experience with PX4 stack, we recommend you use simple_flight, which is now a default in <code>AutonomySim</code>.</p>"},{"location":"px4_setup.html#supported-hardware","title":"Supported Hardware","text":"<p>The following Pixhawk hardware versions have been tested and verified to work with <code>AutonomySim</code>:</p> <ul> <li>Pixhawk PX4 2.4.8</li> <li>PixFalcon</li> <li>PixRacer</li> <li>Pixhawk 2.1</li> <li>Pixhawk 4 mini from Holybro</li> <li>Pixhawk 4 from Holybro</li> </ul> <p>PX4 firmware version 1.11.2 also works on the Pixhawk 4 devices.</p>"},{"location":"px4_setup.html#px4-hardware-in-the-loop-hitl-setup","title":"PX4 Hardware-in-the-loop (HITL) Setup","text":"<p>First, you will need one of the supported device listed above. For manual flight, you will also need an RC transmitter and receiver.</p> <ol> <li>Ensure your RC receiver is bound with its RC transmitter. Connect the RC transmitter to the flight controller's RC port. Refer to your RC manual and PX4 docs for more information.</li> <li>Download QGroundControl, launch it and connect your flight controller to the USB port.</li> <li>Use <code>QGroundControl</code> to flash the latest PX4 Flight Stack. See also initial firmware setup video.</li> <li>In <code>QGroundControl</code>, configure your Pixhawk for HITL simulation by selecting the HITL Quadrocopter X airframe. After PX4 reboots, check that <code>HIL Quadrocopter X</code> is selected.</li> <li>In <code>QGroundControl</code>, go to <code>Radio</code> tab and calibrate (make sure the remote control is on and the receiver is showing the indicator for the binding).</li> <li>Go to the <code>Flight Mode</code> tab and chose one of the remote control switches as <code>Mode Channel</code>. Then set (for example) <code>Stabilized</code> and <code>Attitude</code> flight modes for two positions of the switch.</li> <li>Go to the <code>Tuning</code> section of QGroundControl and set appropriate values. For example, for the Turnigy/FrSky/FlySky FS-TH9X RC transmitter, the following values give a more realistic feel:</li> <li>Hover Throttle = mid+1 mark</li> <li>Roll and pitch sensitivity = mid-3 mark</li> <li>Altitude and position control sensitivity = mid-2 mark</li> <li>In AutonomySim settings file, specify PX4 for your vehicle config like this:</li> </ol> <pre><code>    {\n        \"SettingsVersion\": 1.2,\n        \"SimMode\": \"Multirotor\",\n        \"ClockType\": \"SteppableClock\",\n        \"Vehicles\": {\n            \"PX4\": {\n                \"VehicleType\": \"PX4Multirotor\",\n                \"UseSerial\": true,\n                \"LockStep\": true,\n                \"Sensors\":{\n                    \"Barometer\":{\n                        \"SensorType\": 1,\n                        \"Enabled\": true,\n                        \"PressureFactorSigma\": 0.0001825\n                    }\n                },\n                \"Parameters\": {\n                    \"NAV_RCL_ACT\": 0,\n                    \"NAV_DLL_ACT\": 0,\n                    \"COM_OBL_ACT\": 1,\n                    \"LPE_LAT\": 47.641468,\n                    \"LPE_LON\": -122.140165\n                }\n            }\n        }\n    }\n</code></pre> <p>Note</p> <p>The PX4 <code>[simulator]</code> is communicating over TCP, so we need to set <code>\"UseTcp\": true,</code>. We have also enabled <code>LockStep</code> (see PX4 LockStep for more information). The <code>Barometer</code> setting keeps PX4 stable because the default <code>AutonomySim</code> barometer generates excessive noise. This setting improves the signal-to-noise ratio (SNR), producing faster GPS lock on PX4.</p> <p>With the above settings, you should be able to use a radio/remote controller (RC) in <code>AutonomySim</code>. You can usually arm the vehicle by bringing two RC sticks or 'pots' downward and inward toward one another. You do not need <code>QGroundControl</code> after the initial setup. Typically, the <code>Stabilized</code> rather than <code>Manual</code> mode provides new users or 'newbs' a better experience. See PX4 Basic Flying Guide.</p> <p>You can also control the drone from Python APIs.</p> <p>See Walkthrough Demo Video and  Unreal AutonomySim Setup  Video that shows you all the setup steps in this document.</p>"},{"location":"px4_setup.html#setting-up-px4-software-in-loop","title":"Setting up PX4 Software-in-Loop","text":"<p>The PX4 SITL mode doesn't require you to have separate device such as a Pixhawk or Pixracer. This is in fact the recommended way to use PX4 with simulators by PX4 team. However, this is indeed harder to set up. Please see this dedicated page for setting up PX4 in SITL mode.</p>"},{"location":"px4_setup.html#faq","title":"FAQ","text":""},{"location":"px4_setup.html#drone-doesnt-fly-properly-it-just-goes-crazy","title":"Drone doesn't fly properly, it just goes \"crazy\".","text":"<p>There are a few reasons that can cause this. First, make sure your drone doesn't fall down large distance when starting the simulator. This might happen if you have created a custom Unreal environment and Player Start is placed too high above the ground. It seems that when this happens internal calibration in PX4 gets confused.</p> <p>You should also use QGroundControl and make sure you can arm and takeoff in QGroundControl properly.</p> <p>Finally, this also can be a machine performance issue in some rare cases, check your hard drive performance.</p>"},{"location":"px4_setup.html#can-i-use-arducopter-or-other-mavlink-implementations","title":"Can I use Arducopter or other MavLink implementations?","text":"<p>Our code is tested with the PX4 firmware. We have not tested Arducopter or other mavlink implementations. Some of the flight API's do use the PX4 custom modes in the MAV_CMD_DO_SET_MODE messages (like PX4_CUSTOM_MAIN_MODE_AUTO)</p>"},{"location":"px4_setup.html#it-is-not-finding-my-pixhawk-hardware","title":"It is not finding my Pixhawk hardware","text":"<p>Check your settings.json file for this line \"SerialPort\":\"*,115200\".  The asterisk here means \"find any serial port that looks like a Pixhawk device, but this doesn't always work for all types of Pixhawk hardware. So on Windows you can find the actual COM port using Device Manager, look under \"Ports (COM &amp; LPT), plug the device in and see what new COM port shows up.  Let's say you see a new port named \"USB Serial Port (COM5)\". Well, then change the SerialPort setting to this: \"SerialPort\":\"COM5,115200\".</p> <p>On Linux, the device can be found by running \"ls /dev/serial/by-id\" if you see a device name listed that looks like this <code>usb-3D_Robotics_PX4_FMU_v2.x_0-if00</code> then you can use that name to connect, like this: <code>\"SerialPort\":\"/dev/serial/by-id/usb-3D_Robotics_PX4_FMU_v2.x_0-if00\"</code>. Note that this long name is actually a symbolic link to the real name, if you use <code>\"ls -l ...\"</code> you can find that symbolic link, it is usually something like <code>\"/dev/ttyACM0\"</code>, so this will also work <code>\"SerialPort\":\"/dev/ttyACM0,115200\"</code>.  But that mapping is similar to windows, it is automatically assigned and can change, whereas the long name will work even if the actual TTY serial device mapping changes.</p>"},{"location":"px4_setup.html#warn-commander-takeoff-denied-disarm-and-re-try","title":"WARN [commander] Takeoff denied, disarm and re-try","text":"<p>This happens if you try and take off when PX4 still has not computed the home position. PX4 will report the home position once it is happy with the GPS signal, and you will see these messages:</p> <pre><code>INFO  [commander] home: 47.6414680, -122.1401672, 119.99\nINFO  [tone_alarm] home_set\n</code></pre> <p>Up until this point in time, however, the PX4 will reject takeoff commands.</p>"},{"location":"px4_setup.html#when-i-tell-the-drone-to-do-something-it-always-lands","title":"When I tell the drone to do something it always lands","text":"<p>For example, you use DroneShell <code>moveToPosition -z -20 -x 50 -y 0</code> which it does, but when it gets to the target location the drone starts to land. This is the default behavior of PX4 when offboard mode completes. To set the drone to hover instead set this PX4 parameter:</p> <pre><code>param set COM_OBL_ACT 1\n</code></pre>"},{"location":"px4_setup.html#i-get-message-length-mismatches-errors","title":"I get message length mismatches errors","text":"<p>You might need to set MAV_PROTO_VER parameter in QGC to \"Always use version 1\". Please see this issue more details.</p>"},{"location":"px4_sitl.html","title":"Setting up PX4 Software-in-the-loop (SITL)","text":"<p>The PX4 software provides a software-in-the-loop (SITL) simulation mode that runs on Linux. If you are on Windows, you can use the Cygwin Toolchain or you can use the Windows subsystem for Linux and follow the PX4 Linux toolchain setup.</p> <p>If you are using WSL2, please read these additional instructions.</p> <p>Note</p> <p>Every time you stop Unreal, you have to restart <code>px4</code>.</p> <ol> <li> <p>From your bash terminal follow these steps for Linux and follow all the instructions under <code>NuttX based hardware</code> to install prerequisites. We've also included our own copy of the PX4 build instructions which is a bit more concise about what we need exactly.</p> </li> <li> <p>Get the PX4 source code and build the posix SITL version of PX4:     <pre><code>mkdir -p PX4\ncd PX4\ngit clone https://github.com/PX4/PX4-Autopilot.git --recursive\nbash ./PX4-Autopilot/Tools/setup/ubuntu.sh --no-nuttx --no-sim-tools\ncd PX4-Autopilot\n</code></pre>     And find the latest stable release from https://github.com/PX4/PX4-Autopilot/releases and checkout the source code matching that release, for example:     <pre><code>git checkout v1.11.3\n</code></pre></p> </li> <li> <p>Use following command to build and start PX4 firmware in SITL mode:     <pre><code>make px4_sitl_default none_iris\n</code></pre>    If you are using older version v1.8.* use this command instead: <code>make posix_sitl_ekf2 none_iris</code>.</p> </li> <li> <p>You should see a message saying the SITL PX4 app is waiting for the simulator (AutonomySim) to connect. You will also see information about which ports are configured for mavlink connection to the PX4 app. The default ports have changed recently, so check them closely to make sure AutonomySim settings are correct.     <pre><code>INFO  [simulator] Waiting for simulator to connect on TCP port 4560\nINFO  [init] Mixer: etc/mixers/quad_w.main.mix on /dev/pwm_output0\nINFO  [mavlink] mode: Normal, data rate: 4000000 B/s on udp port 14570 remote port 14550\nINFO  [mavlink] mode: Onboard, data rate: 4000000 B/s on udp port 14580 remote port 14540\n</code></pre>     This is an interactive PX4 console, type <code>help</code> to see the list of commands you can enter here. They are mostly low level PX4 commands, but some of them can be useful for debugging.</p> </li> <li> <p>Edit the AutonomySim settings file to make sure you have matching UDP and TCP port settings:     <pre><code>{\n    \"SettingsVersion\": 1.2,\n    \"SimMode\": \"Multirotor\",\n    \"ClockType\": \"SteppableClock\",\n    \"Vehicles\": {\n        \"PX4\": {\n            \"VehicleType\": \"PX4Multirotor\",\n            \"UseSerial\": false,\n            \"LockStep\": true,\n            \"UseTcp\": true,\n            \"TcpPort\": 4560,\n            \"ControlPortLocal\": 14540,\n            \"ControlPortRemote\": 14580,\n            \"Sensors\":{\n                \"Barometer\":{\n                    \"SensorType\": 1,\n                    \"Enabled\": true,\n                    \"PressureFactorSigma\": 0.0001825\n                }\n            },\n            \"Parameters\": {\n                \"NAV_RCL_ACT\": 0,\n                \"NAV_DLL_ACT\": 0,\n                \"COM_OBL_ACT\": 1,\n                \"LPE_LAT\": 47.641468,\n                \"LPE_LON\": -122.140165\n            }\n        }\n    }\n}\n</code></pre>     Notice the PX4 <code>[simulator]</code> is using TCP, which is why we need to add: <code>\"UseTcp\": true,</code>. Notice we are also enabling <code>LockStep</code>, see PX4 LockStep for more information. The <code>Barometer</code> setting keeps PX4 happy because the default AutonomySim barometer has a bit too much noise generation. This setting clamps that down a bit which allows PX4 to achieve GPS lock more quickly.</p> </li> <li> <p>Open incoming TCP port 4560 and incoming UDP port 14540 using your firewall configuration.</p> </li> <li> <p>Now run your Unreal AutonomySim environment and it should connect to SITL PX4 via TCP. You should see a bunch of messages from the SITL PX4 window. Specifically, the following messages tell you that AutonomySim is connected properly and GPS fusion is stable:     <pre><code>INFO  [simulator] Simulator connected on UDP port 14560\nINFO  [mavlink] partner IP: 127.0.0.1\nINFO  [ecl/EKF] EKF GPS checks passed (WGS-84 origin set)\nINFO  [ecl/EKF] EKF commencing GPS fusion\n</code></pre></p> <p>If you do not see these messages then check your port settings.</p> </li> <li> <p>You should also be able to use QGroundControl with SITL mode. Make sure there is no Pixhawk hardware plugged in, otherwise QGroundControl will choose to use that instead.  Note that as we don't have a physical board, an RC cannot be connected directly to it. So the alternatives are either use XBox 360 Controller or connect your RC using USB (for example, in case of FrSky Taranis X9D Plus) or using trainer USB cable to your PC. This makes your RC look like a joystick. You will need to do extra set up in QGroundControl to use virtual joystick for RC control. You do not need to do this unless you plan to fly a drone manually in AutonomySim. Autonomous flight using the Python API does not require RC, see <code>No Remote Control</code> below.</p> </li> </ol>"},{"location":"px4_sitl.html#setting-gps-origin","title":"Setting GPS origin","text":"<p>Notice the above settings are provided in the <code>params</code> section of the <code>settings.json</code> file:</p> <pre><code>    \"LPE_LAT\": 47.641468,\n    \"LPE_LON\": -122.140165,\n</code></pre> <p>PX4 SITL mode needs to be configured to get the home location correct. The home location needs to be set to the same coordinates defined in  OriginGeopoint. You can also run the following in the SITL PX4 console window to check that these values are set correctly:</p> <pre><code>param show LPE_LAT\nparam show LPE_LON\n</code></pre>"},{"location":"px4_sitl.html#smooth-offboard-transitions","title":"Smooth Offboard Transitions","text":"<p>Notice the above setting is provided in the <code>params</code> section of the <code>settings.json</code> file:</p> <pre><code>\"COM_OBL_ACT\": 1\n</code></pre> <p>This tells the drone automatically hover after each offboard control command finishes (the default setting is to land).  Hovering is a smoother transition between multiple offboard commands. You can check this setting by running the following PX4 console command:</p> <pre><code>param show COM_OBL_ACT\n</code></pre>"},{"location":"px4_sitl.html#check-the-home-position","title":"Check the Home Position","text":"<p>If you are using DroneShell to execute commands (arm, takeoff, etc) then you should wait until the Home position is set. You will see the PX4 SITL console output this message:</p> <pre><code>INFO  [commander] home: 47.6414680, -122.1401672, 119.99\nINFO  [tone_alarm] home_set\n</code></pre> <p>Now DroneShell 'pos' command should report this position and the commands should be accepted by PX4. If you attempt to takeoff without a home position you will see the message:</p> <pre><code>WARN  [commander] Takeoff denied, disarm and re-try\n</code></pre> <p>After home position is set check the local position reported by 'pos' command :</p> <pre><code>Local position: x=-0.0326988, y=0.00656854, z=5.48506\n</code></pre> <p>If the z coordinate is large like this then takeoff might not work as expected. Resetting the SITL and simulation should fix that problem.</p>"},{"location":"px4_sitl.html#wsl2","title":"WSL2","text":"<p>Windows Subsystem for Linux (WSL) version 2 operates in a virtual machine. This requires additional setup - see additional instructions.</p>"},{"location":"px4_sitl.html#no-remote-control","title":"No Remote Control","text":"<p>Notice the above setting is provided in the <code>params</code> section of the <code>settings.json</code> file:</p> <pre><code>\"NAV_RCL_ACT\": 0,\n\"NAV_DLL_ACT\": 0,\n</code></pre> <p>This is required if you plan to fly the SITL mode PX4 with no remote control, just using python scripts, for example.  These parameters stop the PX4 from triggering \"failsafe mode on\" every time a move command is finished.  You can use the following PX4 command to check these values are set correctly:</p> <pre><code>param show NAV_RCL_ACT\nparam show NAV_DLL_ACT\n</code></pre> <p>Note</p> <p>Do not do this on a real drone as it is too dangerous to fly without these failsafe measures.</p>"},{"location":"px4_sitl.html#manually-set-parameters","title":"Manually set parameters","text":"<p>You can also run the following in the PX4 console to set all these parameters manually:</p> <pre><code>param set NAV_RCL_ACT 0\nparam set NAV_DLL_ACT 0\n</code></pre>"},{"location":"px4_sitl.html#setting-up-multi-vehicle-simulation","title":"Setting up multi-vehicle simulation","text":"<p>You can simulate multiple drones in SITL mode using AutonomySim. However, this requires setting up multiple instances of the PX4 firmware simulator to be able to listen for each vehicle's connection on a separate TCP port (4560, 4561, etc). Please see this dedicated page for instructions on setting up multiple instances of PX4 in SITL mode.</p>"},{"location":"px4_sitl.html#using-virtualbox-ubuntu","title":"Using VirtualBox Ubuntu","text":"<p>If you want to run the above posix_sitl in a <code>VirtualBox Ubuntu</code> machine then it will have a different ip address from localhost. So in this case you need to edit the settings file and change the UdpIp and SitlIp to the ip address of your virtual machine set the LocalIpAddress to the address of your host machine running the Unreal engine.</p>"},{"location":"px4_sitl.html#remote-controller","title":"Remote Controller","text":"<p>There are several options for flying the simulated drone using a remote control or joystick like Xbox gamepad. See remote controllers</p>"},{"location":"px4_sitl_wsl2.html","title":"PX4 Software-in-the-loop (SITL) on WSL2","text":"<p>The Windows subsystem for Linux version 2 uses a virtual machine, which has a separate IP address from your Windows host machine. This means that PX4 cannot find <code>AutonomySim</code> on <code>localhost</code>, which is the default behavior for PX4.</p> <p>You will notice that on Windows <code>ipconfig</code> returns a new ethernet adapter for WSL like this (notice the vEthernet has <code>(WSL)</code> in the name:</p> <pre><code>Ethernet adapter vEthernet (WSL):\n\n   Connection-specific DNS Suffix  . :\n   Link-local IPv6 Address . . . . . : fe80::1192:f9a5:df88:53ba%44\n   IPv4 Address. . . . . . . . . . . : 172.31.64.1\n   Subnet Mask . . . . . . . . . . . : 255.255.240.0\n   Default Gateway . . . . . . . . . :\n</code></pre> <p>This address <code>172.31.64.1</code> is the address that WSL 2 can use to reach your Windows host machine.</p> <p>Starting with this PX4 Change Request (which correlates to version v1.12.0-beta1 or newer) PX4 in SITL mode can now connect to AutonomySim on a different (remote) IP address.  To enable this make sure you have a version of PX4 containing this fix and set the following environment variable in linux:</p> <pre><code>export PX4_SIM_HOST_ADDR=172.31.64.1\n</code></pre> <p>Note</p> <p>Be sure to update the above address <code>172.31.64.1</code> to match what you see from your <code>ipconfig</code> command.</p> <p>Open incoming TCP port 4560 and incoming UDP port 14540 using your firewall configuration.</p> <p>Now on the linux side run <code>ip address show</code> and copy the <code>eth0 inet</code> address, it should be something like <code>172.31.66.156</code>.  This is the address Windows needs to know in order to find PX4.</p> <p>Edit your AutonomySim settings file and add <code>LocalHostIp</code> to tell AutonomySim to use the WSL ethernet adapter address instead of the default <code>localhost</code>.  This will cause AutonomySim to open the TCP port on that adapter which is the address that the PX4 app will be looking for.  Also tell AutonomySim to connect the <code>ControlIp</code> UDP channel by setting <code>ControlIp</code> to the magic string <code>remote</code>. This resolves to the WSL 2 remote ip address found in the TCP socket.</p> <pre><code>{\n    \"SettingsVersion\": 1.2,\n    \"SimMode\": \"Multirotor\",\n    \"ClockType\": \"SteppableClock\",\n    \"Vehicles\": {\n        \"PX4\": {\n            \"VehicleType\": \"PX4Multirotor\",\n            \"UseSerial\": false,\n            \"LockStep\": true,\n            \"UseTcp\": true,\n            \"TcpPort\": 4560,\n            \"ControlIp\": \"remote\",\n            \"ControlPortLocal\": 14540,\n            \"ControlPortRemote\": 14580,\n            \"LocalHostIp\": \"172.31.64.1\",\n            \"Sensors\":{\n                \"Barometer\":{\n                    \"SensorType\": 1,\n                    \"Enabled\": true,\n                    \"PressureFactorSigma\": 0.0001825\n                }\n            },\n            \"Parameters\": {\n                \"NAV_RCL_ACT\": 0,\n                \"NAV_DLL_ACT\": 0,\n                \"COM_OBL_ACT\": 1,\n                \"LPE_LAT\": 47.641468,\n                \"LPE_LON\": -122.140165\n            }\n        }\n    }\n}\n</code></pre> <p>See PX4 LockStep for more information. The \"Barometer\" setting keeps PX4 happy because the default AutonomySim barometer has a bit too much noise generation.  This setting clamps that down a bit.</p> <p>If your local repo does not include this PX4 commit, please edit the Linux file in <code>ROMFS/px4fmu_common/init.d-posix/rcS</code> and make sure it is looking for the <code>PX4_SIM_HOST_ADDR</code> environment variable and is passing that through to the PX4 simulator like this:</p> <pre><code># If PX4_SIM_HOST_ADDR environment variable is empty use localhost.\nif [ -z \"${PX4_SIM_HOST_ADDR}\" ]; then\n    echo \"PX4 SIM HOST: localhost\"\n    simulator start -c $simulator_tcp_port\nelse\n    echo \"PX4 SIM HOST: $PX4_SIM_HOST_ADDR\"\n    simulator start -t $PX4_SIM_HOST_ADDR $simulator_tcp_port\nfi\n</code></pre> <p>Note</p> <p>This code might already be there depending on the version of PX4 you are using.</p> <p>Please be patient when waiting for the message:</p> <pre><code>INFO  [simulator] Simulator connected on TCP port 4560.\n</code></pre> <p>It can take a little longer to establish the remote connection than it does with <code>localhost</code>.</p> <p>Now you can proceed with the steps shown in Setting up PX4 Software-in-Loop.</p>"},{"location":"real_vehicles.html","title":"Deploying AutonomyLib to Real Vehicles","text":"<p>The <code>AutonomyLib</code> library can be compiled and deployed on the companion computer on a real robotic system. For our testing, we connected a <code>Gigabyte Brix BXi7-5500</code> companion computer to a Pixhawk/PX4 flight controller over USB on a drone. The Gigabyte PC runs Ubuntu, so we are able to SSH into it over Wi-Fi:</p> <p></p> <p>Once connected, you can run <code>MavLinkTest</code> with this command line:</p> <pre><code>MavLinkTest -serial:/dev/ttyACM0,115200 -logdir:. \n</code></pre> <p>This will produce a log file of the flight which can then be used for playback in the simulator.</p> <p>You can also add <code>-proxy:192.168.1.100:14550</code> to connect <code>MavLinkTest</code> to a remote computer where you can run QGroundControl or our PX4 Log Viewer, which is another handy way to see what is going on with your drone.</p> <p><code>MavLinkTest</code> has simple commands for testing your drone, such as the following example:</p> <pre><code>arm\ntakeoff 5\norbit 10 2\n</code></pre> <p>This will arm the drone, ascend to 5 meters above-ground-level (AGL), and perform an orbit pattern with a radius of 10 meters at 2 m/s. Type '?' to find all available commands.</p> <p>Note: Some commands (for example, <code>orbit</code>) are named differently and have different syntax in <code>MavLinkTest</code> and <code>DroneShell</code> (for example, <code>circlebypath -radius 10 -velocity 21</code>).</p> <p>When you land the drone you can stop <code>MavLinkTest</code> and copy the *.mavlink log file that was generated.</p>"},{"location":"real_vehicles.html#droneserver-and-droneshell","title":"DroneServer and DroneShell","text":"<p>Once you are happy that the <code>MavLinkTest</code> is working, you can also run <code>DroneServer</code> and <code>DroneShell</code> as follows. First, run <code>MavLinkTest</code> with a local proxy to send everything to <code>DroneServer</code>:</p> <pre><code>MavLinkTest -serial:/dev/ttyACM0,115200 -logdir:. -proxy:127.0.0.1:14560\n</code></pre> <p>Change <code>~/Documents/AutonomySim/settings.json</code> to say \"serial\": false, because we want <code>DroneServer</code> to look for this UDP connection.</p> <pre><code>DroneServer 0\n</code></pre> <p>Lastly, you can now connect <code>DroneShell</code> to this instance of <code>DroneServer</code> and use the commands to fly your drone:</p> <pre><code>DroneShell\n==||=&gt;\n        Welcome to DroneShell 1.0.\n        Type ? for help.\n        Nervosys, LLC (c) 2024.\n\nWaiting for drone to report a valid GPS location...\n==||=&gt; requestcontrol\n==||=&gt; arm\n==||=&gt; takeoff\n==||=&gt; circlebypath -radius 10 -velocity 2\n</code></pre>"},{"location":"real_vehicles.html#px4-specific-tools","title":"PX4 Specific Tools","text":"<p>You can run the <code>MavlinkCom</code> library and MavLinkTest app to test the connection between your companion computer and flight controller.  </p>"},{"location":"real_vehicles.html#how-does-this-work","title":"How Does This Work?","text":"<p><code>AutonomySim</code> uses <code>MavLinkCom</code> component developed by @lovettchris. The <code>MavLinkCom</code> has a proxy architecture where you can open a connection to PX4 either using serial or UDP and then other components share this connection. When PX4 sends MavLink message, all components receive that message. If any component sends a message then it's received by PX4 only. This allows you to connect any number of components to PX4 This code opens a connection for LogViewer and QGC. You can add something more if you like.</p> <p>If you want to use QGC and AutonomySim together than you will need QGC to let own the serial port. QGC opens up TCP connection that acts as a proxy so any other component can connect to QGC and send <code>MavLinkMessage</code> to QGC and then QGC forwards that message to PX4. So you tell AutonomySim to connect to QGC and let QGC own serial port.</p> <p>For companion board, the way we did it earlier was to have Gigabyte Brix on the drone. This x86 full-fledged computer that will connect to PX4 through USB. We had Ubuntu on Brix and ran DroneServer. The <code>DroneServer</code> created an API endpoint that we can talk to via C++ client code (or Python code) and it translated API calls to MavLink messages. That way you can write your code against the same API, test it in the simulator and then run the same code on an actual vehicle. So the companion computer has DroneServer running along with client code. </p>"},{"location":"reinforcement_learning.html","title":"Reinforcement Learning (RL)","text":"<p>Below, we describe how to implement a deep Q network (DQN) in <code>AutonomySim</code>. We use a <code>Gynamsium</code> (previously <code>OpenAI Gym</code>) wrapper around the <code>AutomomySim</code> API together with the OpenAI <code>Stable Baselines</code> implementations of standard RL algorithms. We recommend installing stable-baselines3 from the DLR Institute of Robotics and Mechatronics to run these examples.</p> <p>Warning</p> <p>This is still in active development. The below is a framework that can be extended and tweaked to obtain better performance.</p>"},{"location":"reinforcement_learning.html#wrapping-gymnasium-or-gym","title":"Wrapping Gymnasium or Gym","text":"<p>In order to use <code>AutonomySim</code> as a gym environment, we extend and reimplement the base methods such as <code>step</code>, <code>_get_obs</code>, <code>_compute_reward</code> and <code>reset</code> specific to <code>AutonomySim</code> and the task of interest. The sample environments used in these examples for cars and drones can be seen here: <code>PythonClient/reinforcement_learning/*_env.py</code></p>"},{"location":"reinforcement_learning.html#learning-to-control-rovers","title":"Learning to Control Rovers","text":"<p>Note</p> <p>The source code is here</p> <p>This example works with the <code>Neighborhood</code> environment available in releases.</p> <p>First, we need to capture and transform images from the simulation. Below, we show how a depth image can be obtained from the <code>ego</code> camera and transformed to an 84x84 tensor for input into the network.</p> <p>Note</p> <p>You can use other sensor modalities and sensor inputs as well, but you'll have to modify the code accordingly.</p> <pre><code>responses = client.simGetImages([ImageRequest(0, AutonomySimImageType.DepthPerspective, True, False)])\ncurrent_state = transform_input(responses)\n</code></pre> <p>We define six actions (i.e., brake, straight with throttle, full-left with throttle, full-right with throttle, half-left with throttle, half-right with throttle) that an agent can execute. This is done via the function <code>interpret_action</code>:</p> <pre><code>def interpret_action(action):\n    car_controls.brake = 0              # throttle (initial)\n    car_controls.throttle = 1           # ...\n    if action == 0:                     # \n        car_controls.throttle = 0       # brake\n        car_controls.brake = 1          # ...\n    elif action == 1:                   #\n        car_controls.steering = 0       # steer center\n    elif action == 2:                   #\n        car_controls.steering = 0.5     # steer right\n    elif action == 3:                   #\n        car_controls.steering = -0.5    # steer left\n    elif action == 4:                   #\n        car_controls.steering = 0.25    # steer half-right\n    else:                               #\n        car_controls.steering = -0.25   # steer half-left\n    return car_controls\n</code></pre> <p>We then define the reward function in <code>_compute_reward</code> as a convex combination of how fast the vehicle is travelling and how much it deviates from the center line. The agent gets a high reward when moving fast and staying in the center of the lane.</p> <pre><code>def _compute_reward(car_state):\n    MAX_SPEED = 300\n    MIN_SPEED = 10\n    thresh_dist = 3.5\n    beta = 3\n\n    z = 0\n    pts = [np.array([0, -1, z]), np.array([130, -1, z]), np.array([130, 125, z]), np.array([0, 125, z]), np.array([0, -1, z]), np.array([130, -1, z]), np.array([130, -128, z]), np.array([0, -128, z]), np.array([0, -1, z])]\n    pd = car_state.position\n    car_pt = np.array(list(pd.values()))\n\n    dist = 10000000\n    for i in range(0, len(pts)-1):\n        dist = min(dist, np.linalg.norm(np.cross((car_pt - pts[i]), (car_pt - pts[i+1])))/np.linalg.norm(pts[i]-pts[i+1]))\n\n    # print(dist)\n    if dist &gt; thresh_dist:\n        reward = -3\n    else:\n        reward_dist = (math.exp(-beta*dist) - 0.5)\n        reward_speed = (((car_state.speed - MIN_SPEED)/(MAX_SPEED - MIN_SPEED)) - 0.5)\n        reward = reward_dist + reward_speed\n\n    return reward\n</code></pre> <p>The computed reward function also subsequently determines if the episode has terminated (e.g., due to collision). We look at the speed of the vehicle and if it is less than a threshold, the episode is considered terminated.</p> <pre><code>done = 0\nif reward &lt; -1:\n    done = 1\nif car_controls.brake == 0:\n    if car_state.speed &lt;= 5:\n        done = 1\nreturn done\n</code></pre> <p>The main loop then sequences through obtaining the image, computing the action to take according to the current policy, getting a reward, and so forth. If the episode terminates, we reset the vehicle to the original state via <code>reset()</code>:</p> <pre><code>client.reset()\nclient.enableApiControl(True)\nclient.armDisarm(True)\ncar_control = interpret_action(1) // Reset position and drive straight for one second\nclient.setCarControls(car_control)\ntime.sleep(1)\n</code></pre> <p>Once a Gym environment wrapper is defined as in <code>car_env.py</code>, we make use of <code>stable-baselines3</code> to run a DQN training loop. The DQN training can be configured as seen in <code>dqn_car.py</code>:</p> <pre><code>model = DQN(\n    \"CnnPolicy\",\n    env,\n    learning_rate=0.00025,\n    verbose=1,\n    batch_size=32,\n    train_freq=4,\n    target_update_interval=10000,\n    learning_starts=200000,\n    buffer_size=500000,\n    max_grad_norm=10,\n    exploration_fraction=0.1,\n    exploration_final_eps=0.01,\n    device=\"cuda\",\n    tensorboard_log=\"./tb_logs/\",\n)\n</code></pre> <p>Training and evaluation environments can be defined (see <code>EvalCallback</code> in <code>dqn_car.py</code>). The evaluation environoment can be different from the one used for training, with different termination conditions or scene configuration. A <code>tensorboard</code> log directory is defined in the DQN parameters. Finally, <code>model.learn()</code> starts the DQN training loop. Implementations of other RL algorithms such as proximal policy optimization (PPO), asynchronous advantage actor-critic (A3C), and others, can be found in <code>stable-baselines3</code>.</p> <p>Note</p> <p>The simulation needs to be up and running before you execute <code>dqn_car.py</code>. The video below shows first few episodes of DQN training.</p> <p></p>"},{"location":"reinforcement_learning.html#learning-to-control-drones","title":"Learning to Control Drones","text":"<p>Source code</p> <p>This example works with the <code>MountainLandscape</code> environment available in releases.</p> <p>We can also apply RL to learn drone control policies. Below is an example of training a quadrotor to follow along high-tension power lines (e.g., for autonomous inspections). We specify a discrete action space with seven actions: six directions and one hovering action.</p> <pre><code>def interpret_action(self, action):\n    if action == 0:\n        quad_offset = (self.step_length, 0, 0)  # step x-axis positive\n    elif action == 1:\n        quad_offset = (0, self.step_length, 0)  # step y-axis positive\n    elif action == 2:\n        quad_offset = (0, 0, self.step_length)  # step z-axis positive\n    elif action == 3:\n        quad_offset = (-self.step_length, 0, 0) # step x-axis negative\n    elif action == 4:\n        quad_offset = (0, -self.step_length, 0) # step y-axis negative\n    elif action == 5:\n        quad_offset = (0, 0, -self.step_length) # step z-axis negative\n    else:\n        quad_offset = (0, 0, 0)                 # center coordinates\n</code></pre> <p>Again, the reward is a joint function of the quadrotor speed and distance from the power lines.</p> <pre><code>def compute_reward(quad_state, quad_vel, collision_info):\n    thresh_dist = 7\n    beta = 1\n\n    z = -10\n    pts = [np.array([-0.55265, -31.9786, -19.0225]),np.array([48.59735, -63.3286, -60.07256]),np.array([193.5974, -55.0786, -46.32256]),np.array([369.2474, 35.32137, -62.5725]),np.array([541.3474, 143.6714, -32.07256]),]\n\n    quad_pt = np.array(list((self.state[\"position\"].x_val, self.state[\"position\"].y_val,self.state[\"position\"].z_val,)))\n\n    if self.state[\"collision\"]:\n        reward = -100\n    else:\n        dist = 10000000\n        for i in range(0, len(pts) - 1):\n            dist = min(dist, np.linalg.norm(np.cross((quad_pt - pts[i]), (quad_pt - pts[i + 1]))) / np.linalg.norm(pts[i] - pts[i + 1]))\n\n        if dist &gt; thresh_dist:\n            reward = -10\n        else:\n            reward_dist = math.exp(-beta * dist) - 0.5\n            reward_speed = (np.linalg.norm([self.state[\"velocity\"].x_val, self.state[\"velocity\"].y_val, self.state[\"velocity\"].z_val,])- 0.5)\n            reward = reward_dist + reward_speed\n</code></pre> <p>We consider an episode to terminate if the quadrotor drifts too far from the power lines. We then reset the drone to its starting position.</p> <p>Once a Gym environment wrapper is defined as in <code>drone_env.py</code>, we use <code>stable-baselines3</code> to run a DQN training loop. The DQN training can be configured as seen in <code>dqn_drone.py</code>:</p> <pre><code>model = DQN(\n    \"CnnPolicy\",\n    env,\n    learning_rate=0.00025,\n    verbose=1,\n    batch_size=32,\n    train_freq=4,\n    target_update_interval=10000,\n    learning_starts=10000,\n    buffer_size=500000,\n    max_grad_norm=10,\n    exploration_fraction=0.1,\n    exploration_final_eps=0.01,\n    device=\"cuda\",\n    tensorboard_log=\"./tb_logs/\",\n)\n</code></pre> <p>Training and evaluation environments can be defined (see <code>EvalCallback</code> in <code>dqn_car.py</code>). The evaluation environoment can be different from the one used for training, with different termination conditions or scene configuration. A <code>tensorboard</code> log directory is defined in the DQN parameters. Finally, <code>model.learn()</code> starts the DQN training loop. Implementations of other RL algorithms such as proximal policy optimization (PPO), asynchronous advantage actor-critic (A3C), and others, can be found in <code>stable-baselines3</code>.</p> <p>Here is the video of first few episodes during the training.</p> <p></p>"},{"location":"reinforcement_learning.html#related","title":"Related","text":"<p>Also see The Autonomous Driving Cookbook by the Microsoft Deep Learning and Robotics Garage Chapter.</p>"},{"location":"ros_pkgs.html","title":"ROS Interface","text":"<p><code>autonomysim_ros_pkgs</code>: a C++ <code>ROS</code> wrapper over the <code>AutonomySim</code> client library.</p>"},{"location":"ros_pkgs.html#setup","title":"Setup","text":"<p>The below steps are for Linux. If you're running <code>AutonomySim</code> on Windows, you can use Windows Subsystem for Linux (WSL) to run the ROS wrapper. See the instructions below. If you are unable or prefer not to install ROS and related tools on your host Linux machine, you can also try it using Docker. See the steps below.</p> <ul> <li>If your default <code>gcc</code> version is less than 8 (see <code>gcc --version</code> output)</li> <li>Install <code>gcc</code> &gt;= 8.0.0: <code>sudo apt-get install gcc-8 g++-8</code></li> <li>Verify installation by <code>gcc-8 --version</code></li> <li>Ubuntu 16.04<ul> <li>Install ROS kinetic</li> <li>Install <code>tf2-sensor</code> and <code>mavros</code> packages: <code>sudo apt-get install ros-kinetic-tf2-sensor-msgs ros-kinetic-tf2-geometry-msgs ros-kinetic-mavros*</code></li> </ul> </li> <li>Ubuntu 18.04<ul> <li>Install ROS melodic</li> <li>Install <code>tf2-sensor</code> and <code>mavros</code> packages: <code>sudo apt-get install ros-melodic-tf2-sensor-msgs ros-melodic-tf2-geometry-msgs ros-melodic-mavros*</code></li> </ul> </li> <li> <p>Ubuntu 20.04 | 22.04</p> <ul> <li>Install ROS noetic</li> <li>Install <code>tf2-sensor</code> and <code>mavros</code> packages: <code>sudo apt-get install ros-noetic-tf2-sensor-msgs ros-noetic-tf2-geometry-msgs ros-noetic-mavros*</code></li> </ul> </li> <li> <p>Install catkin_tools <code>sudo apt-get install python-catkin-tools</code> or <code>pip install catkin_tools</code>. If using Ubuntu 20.04 use <code>pip install \"git+https://github.com/catkin/catkin_tools.git#egg=catkin_tools\"</code></p> </li> </ul>"},{"location":"ros_pkgs.html#build","title":"Build","text":"<p>Build <code>AutonomySim</code>:</p> <pre><code>git clone https://github.com/nervosys/AutonomySim.git;\ncd AutonomySim;\n./setup.sh;\n./build.sh;\n</code></pre> <p>Ensure that you have setup the environment variables for ROS, as mentioned in the installation pages above. Add the <code>source</code> command to your <code>.bashrc</code> for convenience (replace <code>melodic</code> with specfic version name):</p> <pre><code>echo \"source /opt/ros/melodic/setup.bash\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>Build the ROS package:</p> <pre><code>cd ros;\ncatkin build;  # or catkin_make\n</code></pre> <p>If your default <code>gcc</code> version is less than 8 (see <code>gcc --version</code> output), compilation will fail. In that case, use <code>gcc-8</code> explicitly as follows:</p> <pre><code>catkin build -DCMAKE_C_COMPILER=gcc-8 -DCMAKE_CXX_COMPILER=g++-8\n</code></pre>"},{"location":"ros_pkgs.html#running-autonomysim_ros_pkgs","title":"Running <code>autonomysim_ros_pkgs</code>","text":"<pre><code>source devel/setup.bash;\nroslaunch autonomysim_ros_pkgs autonomysim_node.launch;\nroslaunch autonomysim_ros_pkgs rviz.launch;\n</code></pre> <p>Note</p> <p>If you get an error running <code>roslaunch autonomysim_ros_pkgs autonomysim_node.launch</code>, run <code>catkin clean</code> and try again</p>"},{"location":"ros_pkgs.html#using-the-autonomysim-ros-wrapper","title":"Using the AutonomySim ROS Wrapper","text":"<p>The ROS wrapper is composed of two ROS nodes:</p> <ol> <li>A wrapper over the AutonomySim multirotor C++ client library.</li> <li>A simple proportional derivative (PD) or proportional integral derivative (PID) position controller.</li> </ol> <p>Let's look at the ROS API for both nodes below.</p>"},{"location":"ros_pkgs.html#ros-wrapper-node","title":"ROS Wrapper Node","text":""},{"location":"ros_pkgs.html#publishers","title":"Publishers","text":"<ul> <li> <p><code>/autonomysim_node/origin_geo_point</code> autonomysim_ros_pkgs/GPSYaw GPS coordinates corresponding to global NED frame. This is set in the AutonomySim's settings.json file under the <code>OriginGeopoint</code> key.</p> </li> <li> <p><code>/autonomysim_node/VEHICLE_NAME/global_gps</code> sensor_msgs/NavSatFix This the current GPS coordinates of the drone in AutonomySim.</p> </li> <li> <p><code>/autonomysim_node/VEHICLE_NAME/odom_local_ned</code> nav_msgs/Odometry Odometry in NED frame (default name: odom_local_ned, launch name and frame type are configurable) wrt take-off point.</p> </li> <li> <p><code>/autonomysim_node/VEHICLE_NAME/CAMERA_NAME/IMAGE_TYPE/camera_info</code> sensor_msgs/CameraInfo</p> </li> <li> <p><code>/autonomysim_node/VEHICLE_NAME/CAMERA_NAME/IMAGE_TYPE</code> sensor_msgs/Image   RGB or float image depending on image type requested in settings.json.</p> </li> <li> <p><code>/tf</code> tf2_msgs/TFMessage</p> </li> <li> <p><code>/autonomysim_node/VEHICLE_NAME/altimeter/SENSOR_NAME</code> autonomysim_ros_pkgs/Altimeter This the current altimeter reading for altitude, pressure, and QNH</p> </li> <li> <p><code>/autonomysim_node/VEHICLE_NAME/imu/SENSOR_NAME</code> sensor_msgs::Imu IMU sensor data</p> </li> <li> <p><code>/autonomysim_node/VEHICLE_NAME/magnetometer/SENSOR_NAME</code> sensor_msgs::MagneticField   Meausrement of magnetic field vector/compass</p> </li> <li> <p><code>/autonomysim_node/VEHICLE_NAME/distance/SENSOR_NAME</code> sensor_msgs::Range   Meausrement of distance from an active ranger, such as infrared or IR</p> </li> <li> <p><code>/autonomysim_node/VEHICLE_NAME/lidar/SENSOR_NAME</code> sensor_msgs::PointCloud2   LIDAR pointcloud</p> </li> </ul>"},{"location":"ros_pkgs.html#subscribers","title":"Subscribers","text":"<ul> <li> <p><code>/autonomysim_node/vel_cmd_body_frame</code> autonomysim_ros_pkgs/VelCmd   Ignore <code>vehicle_name</code> field, leave it to blank. We will use <code>vehicle_name</code> in future for multiple drones.</p> </li> <li> <p><code>/autonomysim_node/vel_cmd_world_frame</code> autonomysim_ros_pkgs/VelCmd   Ignore <code>vehicle_name</code> field, leave it to blank. We will use <code>vehicle_name</code> in future for multiple drones.</p> </li> <li> <p><code>/gimbal_angle_euler_cmd</code> autonomysim_ros_pkgs/GimbalAngleEulerCmd   Gimbal set point in euler angles.</p> </li> <li> <p><code>/gimbal_angle_quat_cmd</code> autonomysim_ros_pkgs/GimbalAngleQuatCmd   Gimbal set point in quaternion.</p> </li> <li> <p><code>/autonomysim_node/VEHICLE_NAME/car_cmd</code> autonomysim_ros_pkgs/CarControls Throttle, brake, steering and gear selections for control. Both automatic and manual transmission control possible, see the <code>car_joy.py</code> script for use.</p> </li> </ul>"},{"location":"ros_pkgs.html#services","title":"Services","text":"<ul> <li> <p><code>/autonomysim_node/VEHICLE_NAME/land</code> autonomysim_ros_pkgs/Takeoff</p> </li> <li> <p><code>/autonomysim_node/takeoff</code> autonomysim_ros_pkgs/Takeoff</p> </li> <li> <p><code>/autonomysim_node/reset</code> autonomysim_ros_pkgs/Reset resets all drones</p> </li> </ul>"},{"location":"ros_pkgs.html#parameters","title":"Parameters","text":"<ul> <li> <p><code>/autonomysim_node/world_frame_id</code> [string]   Set in: <code>$(autonomysim_ros_pkgs)/launch/autonomysim_node.launch</code>   Default: world_ned   Set to \"world_enu\" to switch to ENU frames automatically</p> </li> <li> <p><code>/autonomysim_node/odom_frame_id</code> [string]   Set in: <code>$(autonomysim_ros_pkgs)/launch/autonomysim_node.launch</code>   Default: odom_local_ned   If you set world_frame_id to \"world_enu\", the default odom name will instead default to \"odom_local_enu\"</p> </li> <li> <p><code>/autonomysim_node/coordinate_system_enu</code> [boolean]   Set in: <code>$(autonomysim_ros_pkgs)/launch/autonomysim_node.launch</code>   Default: false   If you set world_frame_id to \"world_enu\", this setting will instead default to true</p> </li> <li> <p><code>/autonomysim_node/update_autonomysim_control_every_n_sec</code> [double]   Set in: <code>$(autonomysim_ros_pkgs)/launch/autonomysim_node.launch</code>   Default: 0.01 seconds.   Timer callback frequency for updating drone odom and state from AutonomySim, and sending in control commands.   The current RPClib interface to unreal engine maxes out at 50 Hz.   Timer callbacks in ROS run at maximum rate possible, so it's best to not touch this parameter.</p> </li> <li> <p><code>/autonomysim_node/update_autonomysim_img_response_every_n_sec</code> [double]   Set in: <code>$(autonomysim_ros_pkgs)/launch/autonomysim_node.launch</code>   Default: 0.01 seconds.   Timer callback frequency for receiving images from all cameras in AutonomySim.   The speed will depend on number of images requested and their resolution.   Timer callbacks in ROS run at maximum rate possible, so it's best to not touch this parameter.</p> </li> <li> <p><code>/autonomysim_node/publish_clock</code> [double]   Set in: <code>$(autonomysim_ros_pkgs)/launch/autonomysim_node.launch</code>   Default: false   Will publish the ros /clock topic if set to true.</p> </li> </ul>"},{"location":"ros_pkgs.html#position-controller-node","title":"Position Controller Node","text":""},{"location":"ros_pkgs.html#parameters_1","title":"Parameters","text":"<ul> <li>PD controller parameters:</li> <li> <p><code>/pd_position_node/kd_x</code> [double],     <code>/pd_position_node/kp_y</code> [double],     <code>/pd_position_node/kp_z</code> [double],     <code>/pd_position_node/kp_yaw</code> [double]     Proportional gains</p> </li> <li> <p><code>/pd_position_node/kd_x</code> [double],     <code>/pd_position_node/kd_y</code> [double],     <code>/pd_position_node/kd_z</code> [double],     <code>/pd_position_node/kd_yaw</code> [double]     Derivative gains</p> </li> <li> <p><code>/pd_position_node/reached_thresh_xyz</code> [double]     Threshold euler distance (meters) from current position to setpoint position</p> </li> <li> <p><code>/pd_position_node/reached_yaw_degrees</code> [double]     Threshold yaw distance (degrees) from current position to setpoint position</p> </li> <li> <p><code>/pd_position_node/update_control_every_n_sec</code> [double]   Default: 0.01 seconds</p> </li> </ul>"},{"location":"ros_pkgs.html#services_1","title":"Services","text":"<ul> <li> <p><code>/autonomysim_node/VEHICLE_NAME/gps_goal</code> [Request: srv/SetGPSPosition]   Target gps position + yaw.   In absolute altitude.</p> </li> <li> <p><code>/autonomysim_node/VEHICLE_NAME/local_position_goal</code> [Request: srv/SetLocalPosition]   Target local position + yaw in global NED frame.</p> </li> </ul>"},{"location":"ros_pkgs.html#subscribers_1","title":"Subscribers","text":"<ul> <li> <p><code>/autonomysim_node/origin_geo_point</code> autonomysim_ros_pkgs/GPSYaw   Listens to home geo coordinates published by <code>autonomysim_node</code>.</p> </li> <li> <p><code>/autonomysim_node/VEHICLE_NAME/odom_local_ned</code> nav_msgs/Odometry   Listens to odometry published by <code>autonomysim_node</code></p> </li> </ul>"},{"location":"ros_pkgs.html#publishers_1","title":"Publishers","text":"<ul> <li><code>/vel_cmd_world_frame</code> autonomysim_ros_pkgs/VelCmd   Sends velocity command to <code>autonomysim_node</code></li> </ul>"},{"location":"ros_pkgs.html#globals","title":"Globals","text":"<ul> <li>Dynamic constraints. These can be changed in <code>dynamic_constraints.launch</code>:</li> <li> <p><code>/max_vel_horz_abs</code> [double]   Maximum horizontal velocity of the drone (meters/second)</p> </li> <li> <p><code>/max_vel_vert_abs</code> [double]   Maximum vertical velocity of the drone (meters/second)</p> </li> <li> <p><code>/max_yaw_rate_degree</code> [double]   Maximum yaw rate (degrees/second)</p> </li> </ul>"},{"location":"ros_pkgs.html#miscellaneous","title":"Miscellaneous","text":""},{"location":"ros_pkgs.html#configuring-the-build-environment-on-wsl","title":"Configuring the Build Environment on WSL","text":"<p>These setup instructions describe how to setup the <code>Windows Subsystem for Linux</code> version 2 (WSL2). It involves enabling WSL2, installing a Linux OS image, and installing the build environment as if it were a native Linux system. Upon completion, you will be able to build and run the ROS wrapper as with any Linux machine.</p> <p>Note</p> <p>We recommend using the native file system in <code>/home/...</code> rather than Windows-mounted folders under <code>/mnt/...</code>, as the former is significantly faster.</p>"},{"location":"ros_pkgs.html#setup-on-wsl","title":"Setup on WSL","text":"<ol> <li>Follow the instructions here. Ensure that the ROS version you want to use is supported by the Linux/Ubuntu version you want to install.</li> <li>You now have a working Ubuntu subsystem under Windows, you can now go to Ubuntu 16/18/20/22 instructions and then Running AutonomySim with ROS on WSL.</li> </ol> <p>Note</p> <p>You can run XWindows applications (including SITL) by installing VcXsrv  on Windows. To use it find and run <code>XLaunch</code> from the Windows start menu. Select <code>Multiple Windows</code> in first popup, <code>Start no client</code> in second popup, only <code>Clipboard</code> in third popup. Do not select <code>Native Opengl</code> (and if you are not able to connect select <code>Disable access control</code>). You will need to set the DISPLAY variable to point to your display: in WSL it is <code>127.0.0.1:0</code>, in WSL2 it will be the ip address of the PC's network port and can be set by using the code below. Also in WSL2 you may have to disable the firewall for public networks, or create an exception in order for VcXsrv to communicate with WSL2:</p> <p><code>export DISPLAY=$(cat /etc/resolv.conf | grep nameserver | awk '{print $2}'):0</code></p> <p>Tip</p> <ul> <li>If you add this line to your ~/.bashrc file you won't need to run this command again</li> <li>For code editing you can install VSCode inside WSL.</li> <li>Windows 10 includes \"Windows Defender\" virus scanner. It will slow down WSL quite a bit. Disabling it greatly improves disk performance but increases your risk to viruses so disable at your own risk. Here is one of many resources/videos that show you how to disable it: How to Disable or Enable Windows Defender on Windows 10</li> </ul>"},{"location":"ros_pkgs.html#file-system-access-between-wsl-and-windows-1011","title":"File System Access between WSL and Windows 10/11","text":"<p>Inside WSL, Windows drives are referenced in the <code>/mnt</code> directory. For example, in order to list documents within your () documents folder: <p><code>ls /mnt/c/'Documents and Settings'/&lt;username&gt;/Documents</code> or <code>ls /mnt/c/Users/&lt;username&gt;/Documents</code></p> <p>Inside Windows, the WSL distribution files are located at:</p> <p><code>\\\\wsl$\\&lt;distribution name&gt;</code> e.g., <code>\\\\wsl$\\Ubuntu-22.04</code></p>"},{"location":"ros_pkgs.html#running-autonomysim-with-ros-on-wsl","title":"Running AutonomySim with ROS on WSL","text":"<p>WSL1: <code>export WSL_HOST_IP=127.0.0.1</code> WSL2: <code>export WSL_HOST_IP=$(cat /etc/resolv.conf | grep nameserver | awk '{print $2}')</code></p> <p>Now, as in the running on Linux section, execute the following:</p> <pre><code>source devel/setup.bash\nroslaunch autonomysim_ros_pkgs autonomysim_node.launch output:=screen host:=$WSL_HOST_IP\nroslaunch autonomysim_ros_pkgs rviz.launch\n</code></pre>"},{"location":"ros_pkgs.html#using-ros-with-docker","title":"Using ROS with Docker","text":"<p>A <code>Dockerfile</code> is present in the <code>docker</code> directory. To build the <code>autonomysim-ros</code> image, run the following:</p> <pre><code>cd tools\ndocker build -t autonomysim-ros -f Dockerfile-ROS .\n</code></pre> <p>To run, replace the path of the <code>AutonomySim</code> folder below:</p> <pre><code>docker run --rm -it --net=host -v &lt;your-AutonomySim-folder-path&gt;:/home/testuser/AutonomySim AutonomySim-ros:latest bash\n</code></pre> <p>The above command mounts the AutonomySim directory to the home directory inside the container. Any changes you make in the source files from your host will be visible inside the container, which is useful for development and testing. Now follow the steps from Build to compile and run the ROS wrapper.</p>"},{"location":"ros_pkgs_tutorial.html","title":"ROS Packages Tutorial","text":"<p>This is a set of example <code>settings.json</code>, <code>roslaunch</code>, and <code>rviz</code> files to provide a starting point for using <code>AutonomySim</code> with <code>ROS</code>. See autonomysim_ros_pkgs for the ROS API.</p> <p><code>tf</code>: ROS transforms</p> <p>We use the abbreviation <code>tf</code> herein to refer to ROS transforms, as provided by the ROS tf2 library.</p>"},{"location":"ros_pkgs_tutorial.html#setup","title":"Setup","text":"<p>Make sure that the autonomysim_ros_pkgs setup has been completed and the prerequisites installed.</p> <pre><code>cd PATH_TO/AutonomySim/ros\ncatkin build autonomysim_tutorial_pkgs\n</code></pre> <p>If your default <code>gcc</code> is less than or equal to version 8 (see <code>gcc --version</code> output), compilation will fail. In that case, use <code>gcc-8</code> explicitly as follows:</p> <pre><code>catkin build autonomysim_tutorial_pkgs -DCMAKE_C_COMPILER=gcc-8 -DCMAKE_CXX_COMPILER=g++-8\n</code></pre> <p>Note</p> <p>When running examples and opening a new terminal, sourcing the <code>setup.bash</code> file is necessary. If you're using the ROS wrapper frequently, it might be helpful to add the <code>source PATH_TO/AutonomySim/ros/devel/setup.bash</code> to your <code>~/.profile</code> or <code>~/.bashrc</code> to avoid needing to run this command every time a new terminal is opened.</p>"},{"location":"ros_pkgs_tutorial.html#examples","title":"Examples","text":""},{"location":"ros_pkgs_tutorial.html#single-drone-with-monocular-and-depth-cameras-and-lidar","title":"Single drone with monocular and depth cameras, and LiDAR","text":"<ul> <li><code>settings.json</code> front_stereo_and_center_mono.json</li> </ul> <pre><code>source PATH_TO/AutonomySim/ros/devel/setup.bash\nroscd autonomysim_tutorial_pkgs\ncp settings/front_stereo_and_center_mono.json ~/Documents/AutonomySim/settings.json\n\n# Start your unreal package or binary here\nroslaunch autonomysim_ros_pkgs autonomysim_node.launch;\n\n# in a new pane / terminal\nsource PATH_TO/AutonomySim/ros/devel/setup.bash\nroslaunch autonomysim_tutorial_pkgs front_stereo_and_center_mono.launch\n</code></pre> <p>The above would start <code>rviz</code> with <code>tf</code>s, registered RGB-D cloud using depth_image_proc using the <code>depth_to_pointcloud</code> launch file, and the LiDAR point cloud. </p>"},{"location":"ros_pkgs_tutorial.html#multi-drone-with-cameras-lidars-and-imus","title":"Multi-drone with cameras, LiDARs, and IMUs","text":"<p>Where <code>N = 2</code> in this case.</p> <ul> <li><code>settings.json</code>: two_drones_camera_lidar_imu.json </li> </ul> <pre><code>source PATH_TO/AutonomySim/ros/devel/setup.bash\nroscd autonomysim_tutorial_pkgs\ncp settings/two_drones_camera_lidar_imu.json ~/Documents/AutonomySim/settings.json\n\n# Start your unreal package or binary here\nroslaunch autonomysim_ros_pkgs autonomysim_node.launch;\nroslaunch autonomysim_ros_pkgs rviz.launch\n</code></pre> <p>You can view the <code>tf</code>s in <code>rviz</code>. And do a <code>rostopic list</code> and <code>rosservice list</code> to inspect the services avaiable.    </p>"},{"location":"ros_pkgs_tutorial.html#twenty-five-drones-in-a-square-pattern","title":"Twenty-five drones in a square pattern","text":"<ul> <li><code>settings.json</code>: twenty_five_drones.json </li> </ul> <pre><code>source PATH_TO/AutonomySim/ros/devel/setup.bash\nroscd autonomysim_tutorial_pkgs\ncp settings/twenty_five_drones.json ~/Documents/AutonomySim/settings.json\n\n# Start your unreal package or binary here\nroslaunch autonomysim_ros_pkgs autonomysim_node.launch;\nroslaunch autonomysim_ros_pkgs rviz.launch\n</code></pre> <p>You can view the <code>tf</code>s in <code>rviz</code>. And do a <code>rostopic list</code> and <code>rosservice list</code> to inspect the services avaiable.</p>"},{"location":"sensors.html","title":"Sensors","text":"<p><code>AutonomySim</code> currently supports the following sensors. Each sensor is associated with a integer enum specifying its sensor type.</p> <ul> <li>Camera</li> <li>Barometer = 1</li> <li>Imu = 2</li> <li>Gps = 3</li> <li>Magnetometer = 4</li> <li>Distance Sensor = 5</li> <li>Lidar = 6</li> </ul> <p>Note</p> <p>Cameras are configured differently than the other sensors and do not have an enum associated with them. Look at general settings and image API for camera config and API.</p>"},{"location":"sensors.html#default-sensors","title":"Default sensors","text":"<p>If no sensors are specified in the <code>settings.json</code>, then the following sensors are enabled by default based on the sim mode.</p>"},{"location":"sensors.html#multirotor","title":"Multirotor","text":"<ul> <li>Imu</li> <li>Magnetometer</li> <li>Gps</li> <li>Barometer</li> </ul>"},{"location":"sensors.html#car","title":"Car","text":"<ul> <li>Gps</li> </ul>"},{"location":"sensors.html#computer-vision","title":"Computer Vision","text":"<ul> <li>None</li> </ul> <p>Behind the scenes, <code>createDefaultSensorSettings</code> method in AutonomySimSettings.hpp sets up the above sensors with their default parameters, depending on the sim mode specified in the <code>settings.json</code> file.</p>"},{"location":"sensors.html#configuring-the-default-sensor-list","title":"Configuring the default sensor list","text":"<p>The default sensor list can be configured in settings json:</p> <pre><code>\"DefaultSensors\": {\n    \"Barometer\": {\n        \"SensorType\": 1,\n        \"Enabled\" : true,\n        \"PressureFactorSigma\": 0.001825,\n        \"PressureFactorTau\": 3600,\n        \"UncorrelatedNoiseSigma\": 2.7,\n        \"UpdateLatency\": 0,\n        \"UpdateFrequency\": 50,\n        \"StartupDelay\": 0\n\n    },\n    \"Imu\": {\n        \"SensorType\": 2,\n        \"Enabled\" : true,\n        \"AngularRandomWalk\": 0.3,\n        \"GyroBiasStabilityTau\": 500,\n        \"GyroBiasStability\": 4.6,\n        \"VelocityRandomWalk\": 0.24,\n        \"AccelBiasStabilityTau\": 800,\n        \"AccelBiasStability\": 36\n    },\n    \"Gps\": {\n        \"SensorType\": 3,\n        \"Enabled\" : true,\n        \"EphTimeConstant\": 0.9,\n        \"EpvTimeConstant\": 0.9,\n        \"EphInitial\": 25,\n        \"EpvInitial\": 25,\n        \"EphFinal\": 0.1,\n        \"EpvFinal\": 0.1,\n        \"EphMin3d\": 3,\n        \"EphMin2d\": 4,\n        \"UpdateLatency\": 0.2,\n        \"UpdateFrequency\": 50,\n        \"StartupDelay\": 1\n    },\n    \"Magnetometer\": {\n        \"SensorType\": 4,\n        \"Enabled\" : true,\n        \"NoiseSigma\": 0.005,\n        \"ScaleFactor\": 1,\n        \"NoiseBias\": 0,\n        \"UpdateLatency\": 0,\n        \"UpdateFrequency\": 50,\n        \"StartupDelay\": 0\n    },\n    \"Distance\": {\n        \"SensorType\": 5,\n        \"Enabled\" : true,\n        \"MinDistance\": 0.2,\n        \"MaxDistance\": 40,\n        \"X\": 0, \"Y\": 0, \"Z\": -1,\n        \"Yaw\": 0, \"Pitch\": 0, \"Roll\": 0,\n        \"DrawDebugPoints\": false\n    },\n    \"Lidar2\": {\n        \"SensorType\": 6,\n        \"Enabled\" : true,\n        \"NumberOfChannels\": 16,\n        \"RotationsPerSecond\": 10,\n        \"PointsPerSecond\": 100000,\n        \"X\": 0, \"Y\": 0, \"Z\": -1,\n        \"Roll\": 0, \"Pitch\": 0, \"Yaw\" : 0,\n        \"VerticalFOVUpper\": -15,\n        \"VerticalFOVLower\": -25,\n        \"HorizontalFOVStart\": -20,\n        \"HorizontalFOVEnd\": 20,\n        \"DrawDebugPoints\": true,\n        \"DataFrame\": \"SensorLocalFrame\"\n    }\n}\n</code></pre>"},{"location":"sensors.html#configuring-vehicle-specific-sensor-list","title":"Configuring vehicle-specific sensor list","text":"<p>A vehicle can override a subset of the default sensors listed above. A Lidar and Distance sensor are not added to a vehicle by default, so those you need to add this way. Each sensor must have a valid \"SensorType\" and a subset of the properties can be defined that override the default values shown above and you can set Enabled to false to disable a specific type of sensor.</p> <pre><code>\"Vehicles\": {\n    \"Drone1\": {\n        \"VehicleType\": \"SimpleFlight\",\n        \"AutoCreate\": true,\n        ...\n        \"Sensors\": {\n            \"Barometer\":{\n                \"SensorType\": 1,\n                \"Enabled\": true,\n                \"PressureFactorSigma\": 0.0001825\n            },\n            \"MyLidar1\": {\n                \"SensorType\": 6,\n                \"Enabled\" : true,\n                \"NumberOfChannels\": 16,\n                \"PointsPerSecond\": 10000,\n                \"X\": 0, \"Y\": 0, \"Z\": -1,\n                \"DrawDebugPoints\": true\n            },\n            \"MyLidar2\": {\n                \"SensorType\": 6,\n                \"Enabled\" : true,\n                \"NumberOfChannels\": 4,\n                \"PointsPerSecond\": 10000,\n                \"X\": 0, \"Y\": 0, \"Z\": -1,\n                \"DrawDebugPoints\": true\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"sensors.html#sensor-specific-settings","title":"Sensor-specific settings","text":"<p>For detailed information on the meaning of these sensor settings see the following pages:</p> <ul> <li>Lidar sensor settings</li> <li>Distance sensor settings</li> </ul>"},{"location":"sensors.html#server-side-visualization-for-debugging","title":"Server side visualization for debugging","text":"<p>Be default, the points hit by distance sensor are not drawn on the viewport. To enable the drawing of hit points on the viewport, please enable setting <code>DrawDebugPoints</code> via settings json. E.g. -</p> <pre><code>\"Distance\": {\n    \"SensorType\": 5,\n    \"Enabled\" : true,\n    ...\n    \"DrawDebugPoints\": true\n}\n</code></pre>"},{"location":"sensors.html#sensor-apis","title":"Sensor APIs","text":"<p>Jump straight to <code>hello_drone.py</code> or <code>hello_drone.cpp</code> for example usage, or see follow below for the full API.</p>"},{"location":"sensors.html#barometer","title":"Barometer","text":"<pre><code>nervosys::autonomylib::BarometerBase::Output getBarometerData(const std::string&amp; barometer_name, const std::string&amp; vehicle_name);\n</code></pre> <pre><code>barometer_data = client.getBarometerData(barometer_name = \"\", vehicle_name = \"\")\n</code></pre>"},{"location":"sensors.html#imu","title":"IMU","text":"<pre><code>nervosys::autonomylib::ImuBase::Output getImuData(const std::string&amp; imu_name = \"\", const std::string&amp; vehicle_name = \"\");\n</code></pre> <pre><code>imu_data = client.getImuData(imu_name = \"\", vehicle_name = \"\")\n</code></pre>"},{"location":"sensors.html#gps","title":"GPS","text":"<pre><code>nervosys::autonomylib::GpsBase::Output getGpsData(const std::string&amp; gps_name = \"\", const std::string&amp; vehicle_name = \"\");\n</code></pre> <pre><code>gps_data = client.getGpsData(gps_name = \"\", vehicle_name = \"\")\n</code></pre>"},{"location":"sensors.html#magnetometer","title":"Magnetometer","text":"<pre><code>nervosys::autonomylib::MagnetometerBase::Output getMagnetometerData(const std::string&amp; magnetometer_name = \"\", const std::string&amp; vehicle_name = \"\");\n</code></pre> <pre><code>magnetometer_data = client.getMagnetometerData(magnetometer_name = \"\", vehicle_name = \"\")\n</code></pre>"},{"location":"sensors.html#distance-sensor","title":"Distance sensor","text":"<pre><code>nervosys::autonomylib::DistanceSensorData getDistanceSensorData(const std::string&amp; distance_sensor_name = \"\", const std::string&amp; vehicle_name = \"\");\n</code></pre> <pre><code>distance_sensor_data = client.getDistanceSensorData(distance_sensor_name = \"\", vehicle_name = \"\")\n</code></pre>"},{"location":"sensors.html#lidar","title":"LiDAR","text":"<p>See the LiDAR page for the LiDAR API.</p>"},{"location":"sensors_image.html","title":"Image Sensors","text":"<p><code>AutonomySim</code> provides Python-based neuromorphic vision sensor (NVS) and event-based vision sensor (EVS) camera simulators, optimized for performance and the ability to run in real-time alongside the simulator. Currently, only the EVS simulator is publicly available.</p>"},{"location":"sensors_image.html#types","title":"Types","text":"<p>The following image sensor types are supported or planned:</p> <ul> <li>RGB imager (see)</li> <li>Multispectral imager (MSI)</li> <li>Hyperspectral imager (HSI)</li> <li>Short-wave infrared (SWIR) thermal imager</li> <li>Long-wave infrared (LWIR) thermal imager</li> <li>Event-based vision sensor (EVS)</li> <li>Neuromorphic vision sensor (NVS)</li> </ul>"},{"location":"sensors_image.html#rgb-sensor","title":"RGB Sensor","text":"<p>See the visible-spectrum RGB image sensor section here.</p>"},{"location":"sensors_image.html#multi-and-hyper-spectral-sensors","title":"Multi- and Hyper-spectral Sensors","text":"<p>Coming soon.</p>"},{"location":"sensors_image.html#thermal-infrared-sensor-tirs","title":"Thermal Infrared Sensor (TIRS)","text":"<p>This is a tutorial for generating simulated thermal infrared (IR) images using AutonomySim and the AutonomySim Africa environment. </p> <p>The pre-compiled Africa Environment can be downloaded from the Releases tab of this Github repo: Windows Pre-compiled binary</p> <p>To generate data, you may use two python files: create_ir_segmentation_map.py and capture_ir_segmentation.py.</p> <ul> <li>create_ir_segmentation_map.py uses temperature, emissivity, and camera response information to estimate the thermal digital count that could be expected for the objects in the environment, and then reassigns the segmentation IDs in AutonomySim to match these digital counts. It should be run before starting to capture thermal IR data. Otherwise, digital counts in the IR images will be incorrect. The camera response, temperature, and emissivity data are all included for the Africa environment.</li> <li>capture_ir_segmentation.py is run after the segmentation IDs have been reassigned. It tracks objects of interest and records the infrared and scene images from the multirotor. It uses Computer Vision mode.</li> </ul> <p>Details on how temperatures were estimated for plants and animals in the Africa environment, et cetera, can be found in Bondi et al. (2018).</p>"},{"location":"sensors_image.html#neuromorphic-vision-sensor-nvs","title":"Neuromorphic Vision Sensor (NVS)","text":"<p>Coming soon.</p>"},{"location":"sensors_image.html#event-based-vision-sensor-evs","title":"Event-based Vision Sensor (EVS)","text":"<p>An event-based vision sensor (EVS) camera is a special camera that measures changes in logarithmic brightness and only reports the changes as events. Each event is a set of four values that gets generated every time the absolute change in the logarithmic brightness exceeds a certain threshold. An event contains the timestamp of the measurement, pixel location (x and y coordinates) and the polarity: which is either +1/-1 based on whether the logarithmic brightness has increased or decreased. Most event cameras have a temporal resolution of the order of microseconds, making them significantly faster than RGB sensors, and also demonstrate a high dynamic range and low motion blur.</p> <p>More details about event cameras can be found in this tutorial from RPG-UZH</p>"},{"location":"sensors_image.html#evs-camera-simulator","title":"EVS Camera Simulator","text":"<p>The <code>AutonomySim</code> event simulator uses two consecutive RGB images (converted to grayscale), and computes \"past events\" that would have occurred during the transition based on the change in log luminance between the images. These events are reported as a stream of bytes, following this format:</p> <p><code>&lt;x&gt; &lt;y&gt; &lt;timestamp&gt; &lt;pol&gt;</code></p> <p><code>x</code> and <code>y</code> are the pixel locations of the event firing, <code>timestamp</code> is the global timestamp in microseconds and <code>pol</code> is either +1/-1 depending on whether the brightness increased or decreased. Along with this bytestream, an accumulation of events over a 2D frame is also constructed, known as an 'event image' that visualizes +1 events as red and -1 as blue pixels. An example event image is shown below:</p> <p></p>"},{"location":"sensors_image.html#algorithm","title":"Algorithm","text":"<p>The inner workings of the event simulator loosely follows this set of operations:</p> <ol> <li>Take the difference between the log intensities of the current and previous frames.  </li> <li>Iterating over all pixels, calculate the polarity for each each pixel based on a threshold of change in log intensity.  </li> <li>Determine the number of events to be fired per pixel, based on extent of intensity change over the threshold. Let \\(N_{max}\\) be the maximum number of events that can occur at a single pixel, then the total number of firings to be simulated at pixel location \\(u\\) would be \\(N_e(u) = min(N_{max}, \\frac{\\Delta L(u)}{TOL})\\).  </li> <li>Determine the timestamps for each interpolated event by interpolating between the amount of time that has elapsed between the captures of the previous and current images. \\(t = t_{prev} + \\frac{\\Delta T}{N_e(u)}\\) </li> <li>Generate the output bytestream by simulating events at every pixel and sort by timestamp.</li> </ol>"},{"location":"sensors_image.html#usage","title":"Usage","text":"<p>An example script to run the event simulator alongside AutonomySim is located at https://github.com/nervosys/AutonomySim/blob/master/PythonClient/eventcamera_sim/test_event_sim.py. The following optional command-line arguments can be passed to this script.</p> <pre><code>args.width, args.height (float): Simulated event camera resolution\nargs.save (bool): whether to save the event data to a file\nargs.debug (bool): Whether to display the simulated events as an image\n</code></pre> <p>The implementation of the actual event simulation, written in Python and numba, is at https://github.com/nervosys/AutonomySim/blob/master/PythonClient/eventcamera_sim/event_simulator.py. The event simulator is initialized as follows, with the arguments controlling the resolution of the camera.</p> <pre><code>from event_simulator import *\nev_sim = EventSimulator(W, H)\n</code></pre> <p>The actual computation of the events is triggered through an <code>image_callback</code> function, which is executed every time a new RGB image is obtained. The first time this function is called, due to the lack of a 'previous' image, it acts as an initialization of the event sim. </p> <pre><code>event_img, events = ev_sim.image_callback(img, ts_delta)\n</code></pre> <p>This function, which behaves similar to a callback (called every time a new image is received) returns an event image as a one dimensional array of +1/-1 values, thus indicating only whether events were seen at each pixel, but not the timing/number of events. This one dimensional array can be converted into the red/blue event image as seen in the function <code>convert_event_img_rgb</code>. <code>events</code> is a numpy array of events, each of format <code>&lt;x&gt; &lt;y&gt; &lt;timestamp&gt; &lt;pol&gt;</code>.</p> <p>Through this function, the event sim computes the difference between the past and the current image, and computes a stream of events which is then returned as a numpy array. This can then be appended to a file.</p> <p>There are quite a few parameters that can be tuned to achieve a level of visual fidelity/performance of the event simulation. The main factors to tune are the following:</p> <ol> <li>The resolution of the camera.</li> <li>The log luminance threshold <code>TOL</code> that determines whether or not a detected change counts as an event.</li> </ol> <p>Note</p> <p>There is also currently a max limit on the number of events generated per pair of images, which can also be tuned.</p>"},{"location":"settings.html","title":"Settings","text":""},{"location":"settings.html#where-are-settings-stored","title":"Where are Settings Stored?","text":"<p><code>AutonomySim</code> searches for the settings definition in the following order. The first match will be used:</p> <ol> <li> <p>Looking at the (absolute) path specified by the <code>-settings</code> command line argument. For example, in Windows: <code>AutonomySim.exe -settings=\"C:\\path\\to\\settings.json\"</code> In Linux <code>./Blocks.sh -settings=\"/home/$USER/path/to/settings.json\"</code></p> </li> <li> <p>Looking for a json document passed as a command line argument by the <code>-settings</code> argument. For example, in Windows: <code>AutonomySim.exe -settings={\"foo\":\"bar\"}</code> In Linux <code>./Blocks.sh -settings={\"foo\":\"bar\"}</code></p> </li> <li> <p>Looking in the folder of the executable for a file called <code>settings.json</code>. This will be a deep location where the actual executable of the Editor or binary is stored. For e.g. with the Blocks binary, the location searched is <code>&lt;path-of-binary&gt;/LinuxNoEditor/Blocks/Binaries/Linux/settings.json</code>.</p> </li> <li> <p>Searching for <code>settings.json</code> in the folder from where the executable is launched</p> <p>This is a top-level directory containing the launch script or executable. For e.g. Linux: <code>&lt;path-of-binary&gt;/LinuxNoEditor/settings.json</code>, Windows: <code>&lt;path-of-binary&gt;/WindowsNoEditor/settings.json</code></p> <p>Note that this path changes depending on where its invoked from. On Linux, if executing the <code>Blocks.sh</code> script from inside LinuxNoEditor folder like <code>./Blocks.sh</code>, then the previous mentioned path is used. However, if launched from outside LinuxNoEditor folder such as <code>./LinuxNoEditor/Blocks.sh</code>, then <code>&lt;path-of-binary&gt;/settings.json</code> will be used.</p> </li> <li> <p>Looking in the AutonomySim subfolder for a file called <code>settings.json</code>. The AutonomySim subfolder is located at <code>Documents\\AutonomySim</code> on Windows and <code>~/Documents/AutonomySim</code> on Linux systems.</p> </li> </ol> <p>The file is in usual json format. On first startup AutonomySim would create <code>settings.json</code> file with no settings at the users home folder. To avoid problems, always use ASCII format to save json file.</p>"},{"location":"settings.html#how-to-chose-between-car-and-multirotor","title":"How to Chose Between Car and Multirotor?","text":"<p>The default is to use multirotor. To use car simple set <code>\"SimMode\": \"Car\"</code> like this:</p> <pre><code>{\n  \"SettingsVersion\": 1.2,\n  \"SimMode\": \"Car\"\n}\n</code></pre> <p>To choose multirotor, set <code>\"SimMode\": \"Multirotor\"</code>. If you want to prompt user to select vehicle type then use <code>\"SimMode\": \"\"</code>.</p>"},{"location":"settings.html#available-settings-and-their-defaults","title":"Available Settings and Their Defaults","text":"<p>Below are complete list of settings available along with their default values. If any of the settings is missing from json file, then default value is used. Some default values are simply specified as <code>\"\"</code> which means actual value may be chosen based on the vehicle you are using. For example, <code>ViewMode</code> setting has default value <code>\"\"</code> which translates to <code>\"FlyWithMe\"</code> for drones and <code>\"SpringArmChase\"</code> for cars.</p> <p>WARNING: Do not copy paste all of below in your settings.json. We strongly recommend adding only those settings that you don't want default values. Only required element is <code>\"SettingsVersion\"</code>.</p> <pre><code>{\n  \"SimMode\": \"\",\n  \"ClockType\": \"\",\n  \"ClockSpeed\": 1,\n  \"LocalHostIp\": \"127.0.0.1\",\n  \"ApiServerPort\": 41451,\n  \"RecordUIVisible\": true,\n  \"LogMessagesVisible\": true,\n  \"ShowLosDebugLines\": false,\n  \"ViewMode\": \"\",\n  \"RpcEnabled\": true,\n  \"EngineSound\": true,\n  \"PhysicsEngineName\": \"\",\n  \"SpeedUnitFactor\": 1.0,\n  \"SpeedUnitLabel\": \"m/s\",\n  \"Wind\": { \"X\": 0, \"Y\": 0, \"Z\": 0 },\n  \"CameraDirector\": {\n    \"FollowDistance\": -3,\n    \"X\": NaN, \"Y\": NaN, \"Z\": NaN,\n    \"Pitch\": NaN, \"Roll\": NaN, \"Yaw\": NaN\n  },\n  \"Recording\": {\n    \"RecordOnMove\": false,\n    \"RecordInterval\": 0.05,\n    \"Folder\": \"\",\n    \"Enabled\": false,\n    \"Cameras\": [\n        { \"CameraName\": \"0\", \"ImageType\": 0, \"PixelsAsFloat\": false,  \"VehicleName\": \"\", \"Compress\": true }\n    ]\n  },\n  \"CameraDefaults\": {\n    \"CaptureSettings\": [\n      {\n        \"ImageType\": 0,\n        \"Width\": 256,\n        \"Height\": 144,\n        \"FOV_Degrees\": 90,\n        \"AutoExposureSpeed\": 100,\n        \"AutoExposureBias\": 0,\n        \"AutoExposureMaxBrightness\": 0.64,\n        \"AutoExposureMinBrightness\": 0.03,\n        \"MotionBlurAmount\": 0,\n        \"TargetGamma\": 1.0,\n        \"ProjectionMode\": \"\",\n        \"OrthoWidth\": 5.12\n      }\n    ],\n    \"NoiseSettings\": [\n      {\n        \"Enabled\": false,\n        \"ImageType\": 0,\n\n        \"RandContrib\": 0.2,\n        \"RandSpeed\": 100000.0,\n        \"RandSize\": 500.0,\n        \"RandDensity\": 2,\n\n        \"HorzWaveContrib\":0.03,\n        \"HorzWaveStrength\": 0.08,\n        \"HorzWaveVertSize\": 1.0,\n        \"HorzWaveScreenSize\": 1.0,\n\n        \"HorzNoiseLinesContrib\": 1.0,\n        \"HorzNoiseLinesDensityY\": 0.01,\n        \"HorzNoiseLinesDensityXY\": 0.5,\n\n        \"HorzDistortionContrib\": 1.0,\n        \"HorzDistortionStrength\": 0.002\n      }\n    ],\n    \"Gimbal\": {\n      \"Stabilization\": 0,\n      \"Pitch\": NaN, \"Roll\": NaN, \"Yaw\": NaN\n    },\n    \"X\": NaN, \"Y\": NaN, \"Z\": NaN,\n    \"Pitch\": NaN, \"Roll\": NaN, \"Yaw\": NaN,\n    \"UnrealEngine\": {\n      \"PixelFormatOverride\": [\n        {\n          \"ImageType\": 0,\n          \"PixelFormat\": 0\n        }\n      ]\n    }\n  },\n  \"OriginGeopoint\": {\n    \"Latitude\": 47.641468,\n    \"Longitude\": -122.140165,\n    \"Altitude\": 122\n  },\n  \"TimeOfDay\": {\n    \"Enabled\": false,\n    \"StartDateTime\": \"\",\n    \"CelestialClockSpeed\": 1,\n    \"StartDateTimeDst\": false,\n    \"UpdateIntervalSecs\": 60\n  },\n  \"SubWindows\": [\n    {\"WindowID\": 0, \"CameraName\": \"0\", \"ImageType\": 3, \"VehicleName\": \"\", \"Visible\": false, \"External\": false},\n    {\"WindowID\": 1, \"CameraName\": \"0\", \"ImageType\": 5, \"VehicleName\": \"\", \"Visible\": false, \"External\": false},\n    {\"WindowID\": 2, \"CameraName\": \"0\", \"ImageType\": 0, \"VehicleName\": \"\", \"Visible\": false, \"External\": false}\n  ],\n  \"SegmentationSettings\": {\n    \"InitMethod\": \"\",\n    \"MeshNamingMethod\": \"\",\n    \"OverrideExisting\": true\n  },\n  \"PawnPaths\": {\n    \"BareboneCar\": {\"PawnBP\": \"Class'/AutonomySim/VehicleAdv/Vehicle/VehicleAdvPawn.VehicleAdvPawn_C'\"},\n    \"DefaultCar\": {\"PawnBP\": \"Class'/AutonomySim/VehicleAdv/SUV/SuvCarPawn.SuvCarPawn_C'\"},\n    \"DefaultQuadrotor\": {\"PawnBP\": \"Class'/AutonomySim/Blueprints/BP_FlyingPawn.BP_FlyingPawn_C'\"},\n    \"DefaultComputerVision\": {\"PawnBP\": \"Class'/AutonomySim/Blueprints/BP_ComputerVisionPawn.BP_ComputerVisionPawn_C'\"}\n  },\n  \"Vehicles\": {\n    \"SimpleFlight\": {\n      \"VehicleType\": \"SimpleFlight\",\n      \"DefaultVehicleState\": \"Armed\",\n      \"AutoCreate\": true,\n      \"PawnPath\": \"\",\n      \"EnableCollisionPassthrogh\": false,\n      \"EnableCollisions\": true,\n      \"AllowAPIAlways\": true,\n      \"EnableTrace\": false,\n      \"RC\": {\n        \"RemoteControlID\": 0,\n        \"AllowAPIWhenDisconnected\": false\n      },\n      \"Cameras\": {\n        //same elements as CameraDefaults above, key as name\n      },\n      \"X\": NaN, \"Y\": NaN, \"Z\": NaN,\n      \"Pitch\": NaN, \"Roll\": NaN, \"Yaw\": NaN\n    },\n    \"PhysXCar\": {\n      \"VehicleType\": \"PhysXCar\",\n      \"DefaultVehicleState\": \"\",\n      \"AutoCreate\": true,\n      \"PawnPath\": \"\",\n      \"EnableCollisionPassthrogh\": false,\n      \"EnableCollisions\": true,\n      \"RC\": {\n        \"RemoteControlID\": -1\n      },\n      \"Cameras\": {\n        \"MyCamera1\": {\n          //same elements as elements inside CameraDefaults above\n        },\n        \"MyCamera2\": {\n          //same elements as elements inside CameraDefaults above\n        },\n      },\n      \"X\": NaN, \"Y\": NaN, \"Z\": NaN,\n      \"Pitch\": NaN, \"Roll\": NaN, \"Yaw\": NaN\n    }\n  },\n  \"ExternalCameras\": {\n    \"FixedCamera1\": {\n        // same elements as in CameraDefaults above\n    },\n    \"FixedCamera2\": {\n        // same elements as in CameraDefaults above\n    }\n  }\n}\n</code></pre>"},{"location":"settings.html#simmode","title":"SimMode","text":"<p>SimMode determines which simulation mode will be used. Below are currently supported values: * <code>\"\"</code>: prompt user to select vehicle type multirotor or car * <code>\"Multirotor\"</code>: Use multirotor simulation * <code>\"Car\"</code>: Use car simulation * <code>\"ComputerVision\"</code>: Use only camera, no vehicle or physics</p>"},{"location":"settings.html#viewmode","title":"ViewMode","text":"<p>The ViewMode determines which camera to use as default and how camera will follow the vehicle. For multirotors, the default ViewMode is <code>\"FlyWithMe\"</code> while for cars the default ViewMode is <code>\"SpringArmChase\"</code>.</p> <ul> <li><code>FlyWithMe</code>: Chase the vehicle from behind with 6 degrees of freedom</li> <li><code>GroundObserver</code>: Chase the vehicle from 6' above the ground but with full freedom in XY plane.</li> <li><code>Fpv</code>: View the scene from front camera of vehicle</li> <li><code>Manual</code>: Don't move camera automatically. Use arrow keys and ASWD keys for move camera manually.</li> <li><code>SpringArmChase</code>: Chase the vehicle with camera mounted on (invisible) arm that is attached to the vehicle via spring (so it has some latency in movement).</li> <li><code>NoDisplay</code>: This will freeze rendering for main screen however rendering for subwindows, recording and APIs remain active. This mode is useful to save resources in \"headless\" mode where you are only interested in getting images and don't care about what gets rendered on main screen. This may also improve FPS for recording images.</li> </ul>"},{"location":"settings.html#timeofday","title":"TimeOfDay","text":"<p>This setting controls the position of Sun in the environment. By default <code>Enabled</code> is false which means Sun's position is left at whatever was the default in the environment and it doesn't change over the time. If <code>Enabled</code> is true then Sun position is computed using longitude, latitude and altitude specified in <code>OriginGeopoint</code> section for the date specified in <code>StartDateTime</code> in the string format as %Y-%m-%d %H:%M:%S, for example, <code>2018-02-12 15:20:00</code>. If this string is empty then current date and time is used. If <code>StartDateTimeDst</code> is true then we adjust for day light savings time. The Sun's position is then continuously updated at the interval specified in <code>UpdateIntervalSecs</code>. In some cases, it might be desirable to have celestial clock run faster or slower than simulation clock. This can be specified using <code>CelestialClockSpeed</code>, for example, value 100 means for every 1 second of simulation clock, Sun's position is advanced by 100 seconds so Sun will move in sky much faster.</p> <p>Also see Time of Day API.</p>"},{"location":"settings.html#origingeopoint","title":"OriginGeopoint","text":"<p>This setting specifies the latitude, longitude and altitude of the Player Start component placed in the Unreal environment. The vehicle's home point is computed using this transformation. Note that all coordinates exposed via APIs are using NED system in SI units which means each vehicle starts at (0, 0, 0) in NED system. Time of Day settings are computed for geographical coordinates specified in <code>OriginGeopoint</code>.</p>"},{"location":"settings.html#subwindows","title":"SubWindows","text":"<p>This setting determines what is shown in each of 3 subwindows which are visible when you press 1,2,3 keys. </p> <ul> <li><code>WindowID</code>: Can be 0 to 2</li> <li><code>CameraName</code>: is any available camera on the vehicle or external camera</li> <li><code>ImageType</code>: integer value determines what kind of image gets shown according to ImageType enum.</li> <li><code>VehicleName</code>: string allows you to specify the vehicle to use the camera from, used when multiple vehicles are specified in the settings. First vehicle's camera will be used if there are any mistakes such as incorrect vehicle name, or only a single vehicle.</li> <li><code>External</code>: Set it to <code>true</code> if the camera is an external camera. If true, then the <code>VehicleName</code> parameter is ignored</li> </ul> <p>For example, for a single car vehicle, below shows driver view, front bumper view and rear view as scene, depth and surface normals respectively.</p> <pre><code>  \"SubWindows\": [\n    {\"WindowID\": 0, \"ImageType\": 0, \"CameraName\": \"3\", \"Visible\": true},\n    {\"WindowID\": 1, \"ImageType\": 3, \"CameraName\": \"0\", \"Visible\": true},\n    {\"WindowID\": 2, \"ImageType\": 6, \"CameraName\": \"4\", \"Visible\": true}\n  ]\n</code></pre> <p>In case of multiple vehicles, different vehicles can be specified as follows-</p> <pre><code>    \"SubWindows\": [\n        {\"WindowID\": 0, \"CameraName\": \"0\", \"ImageType\": 3, \"VehicleName\": \"Car1\", \"Visible\": false},\n        {\"WindowID\": 1, \"CameraName\": \"0\", \"ImageType\": 5, \"VehicleName\": \"Car2\", \"Visible\": false},\n        {\"WindowID\": 2, \"CameraName\": \"0\", \"ImageType\": 0, \"VehicleName\": \"Car1\", \"Visible\": false}\n    ]\n</code></pre>"},{"location":"settings.html#recording","title":"Recording","text":"<p>The recording feature allows you to record data such as position, orientation, velocity along with the captured image at specified intervals. You can start recording by pressing red Record button on lower right or the R key. The data is stored in the <code>Documents\\AutonomySim</code> folder (or the folder specified using <code>Folder</code>), in a time stamped subfolder for each recording session, as tab separated file.</p> <ul> <li><code>RecordInterval</code>: specifies minimal interval in seconds between capturing two images.</li> <li><code>RecordOnMove</code>: specifies that do not record frame if there was vehicle's position or orientation hasn't changed.</li> <li><code>Folder</code>: Parent folder where timestamped subfolder with recordings are created. Absolute path of the directory must be specified. If not used, then <code>Documents/AutonomySim</code> folder will be used. E.g. <code>\"Folder\": \"/home/&lt;user&gt;/Documents\"</code></li> <li><code>Enabled</code>: Whether Recording should start from the beginning itself, setting to <code>true</code> will start recording automatically when the simulation starts. By default, it's set to <code>false</code></li> <li><code>Cameras</code>: this element controls which cameras are used to capture images. By default scene image from camera 0 is recorded as compressed png format. This setting is json array so you can specify multiple cameras to capture images, each with potentially different image types. <ul> <li>When <code>PixelsAsFloat</code> is true, image is saved as pfm file instead of png file.</li> <li><code>VehicleName</code> option allows you to specify separate cameras for individual vehicles. If the <code>Cameras</code> element isn't present, <code>Scene</code> image from the default camera of each vehicle will be recorded.</li> <li>If you don't want to record any images and just the vehicle's physics data, then specify the <code>Cameras</code> element but leave it empty, like this: <code>\"Cameras\": []</code></li> <li>External cameras are currently not supported in recording</li> </ul> </li> </ul> <p>For example, the <code>Cameras</code> element below records scene &amp; segmentation images for <code>Car1</code> &amp; scene for <code>Car2</code>-</p> <pre><code>\"Cameras\": [\n    { \"CameraName\": \"0\", \"ImageType\": 0, \"PixelsAsFloat\": false, \"VehicleName\": \"Car1\", \"Compress\": true },\n    { \"CameraName\": \"0\", \"ImageType\": 5, \"PixelsAsFloat\": false, \"VehicleName\": \"Car1\", \"Compress\": true },\n    { \"CameraName\": \"0\", \"ImageType\": 0, \"PixelsAsFloat\": false, \"VehicleName\": \"Car2\", \"Compress\": true }\n]\n</code></pre> <p>Check out Modifying Recording Data for details on how to modify the kinematics data being recorded.</p>"},{"location":"settings.html#clockspeed","title":"ClockSpeed","text":"<p>This setting allows you to set the speed of simulation clock with respect to wall clock. For example, value of 5.0 would mean simulation clock has 5 seconds elapsed when wall clock has 1 second elapsed (i.e. simulation is running faster). The value of 0.1 means that simulation clock is 10X slower than wall clock. The value of 1 means simulation is running in real time. It is important to realize that quality of simulation may decrease as the simulation clock runs faster. You might see artifacts like object moving past obstacles because collision is not detected. However slowing down simulation clock (i.e. values &lt; 1.0) generally improves the quality of simulation.</p>"},{"location":"settings.html#segmentation-settings","title":"Segmentation Settings","text":"<p>The <code>InitMethod</code> determines how object IDs are initialized at startup to generate segmentation. The value \"\" or \"CommonObjectsRandomIDs\" (default) means assign random IDs to each object at startup. This will generate segmentation view with random colors assign to each object. The value \"None\" means don't initialize object IDs. This will cause segmentation view to have single solid colors. This mode is useful if you plan to set up object IDs using APIs and it can save lot of delay at startup for large environments like CityEnviron.</p> <p>If <code>OverrideExisting</code> is false then initialization does not alter non-zero object IDs already assigned otherwise it does.</p> <p>If <code>MeshNamingMethod</code> is \"\" or \"OwnerName\" then we use mesh's owner name to generate random hash as object IDs. If it is \"StaticMeshName\" then we use static mesh's name to generate random hash as object IDs. Note that it is not possible to tell individual instances of the same static mesh apart this way, but the names are often more intuitive.</p>"},{"location":"settings.html#wind-settings","title":"Wind Settings","text":"<p>This setting specifies the wind speed in World frame, in NED direction. Values are in m/s. By default, speed is 0, i.e. no wind.</p>"},{"location":"settings.html#camera-director-settings","title":"Camera Director Settings","text":"<p>This element specifies the settings used for the camera following the vehicle in the ViewPort.</p> <ul> <li><code>FollowDistance</code>: Distance at which camera follows the vehicle, default is -8 (8 meters) for Car, -3 for others.</li> <li><code>X, Y, Z, Yaw, Roll, Pitch</code>: These elements allows you to specify the position and orientation of the camera relative to the vehicle. Position is in NED coordinates in SI units with origin set to Player Start location in Unreal environment. The orientation is specified in degrees.</li> </ul>"},{"location":"settings.html#camera-settings","title":"Camera Settings","text":"<p>The <code>CameraDefaults</code> element at root level specifies defaults used for all cameras. These defaults can be overridden for individual camera in <code>Cameras</code> element inside <code>Vehicles</code> as described later.</p>"},{"location":"settings.html#note-on-imagetype-element","title":"Note on ImageType element","text":"<p>The <code>ImageType</code> element in JSON array determines which image type that settings applies to. The valid values are described in ImageType section. In addition, we also support special value <code>ImageType: -1</code> to apply the settings to external camera (i.e. what you are looking at on the screen).</p> <p>For example, <code>CaptureSettings</code> element is json array so you can add settings for multiple image types easily.</p>"},{"location":"settings.html#capturesettings","title":"CaptureSettings","text":"<p>The <code>CaptureSettings</code> determines how different image types such as scene, depth, disparity, surface normals and segmentation views are rendered. The Width, Height and FOV settings should be self explanatory. The AutoExposureSpeed decides how fast eye adaptation works. We set to generally high value such as 100 to avoid artifacts in image capture. Similarly we set MotionBlurAmount to 0 by default to avoid artifacts in ground truth images. The <code>ProjectionMode</code> decides the projection used by the capture camera and can take value \"perspective\" (default) or \"orthographic\". If projection mode is \"orthographic\" then <code>OrthoWidth</code> determines width of projected area captured in meters.</p> <p>For explanation of other settings, please see this article.</p>"},{"location":"settings.html#noisesettings","title":"NoiseSettings","text":"<p>The <code>NoiseSettings</code> allows to add noise to the specified image type with a goal of simulating camera sensor noise, interference and other artifacts. By default no noise is added, i.e., <code>Enabled: false</code>. If you set <code>Enabled: true</code> then following different types of noise and interference artifacts are enabled, each can be further tuned using setting. The noise effects are implemented as shader created as post processing material in Unreal Engine called CameraSensorNoise.</p> <p>Demo of camera noise and interference simulation:</p> <p></p>"},{"location":"settings.html#random-noise","title":"Random noise","text":"<p>This adds random noise blobs with following parameters. * <code>RandContrib</code>: This determines blend ratio of noise pixel with image pixel, 0 means no noise and 1 means only noise. * <code>RandSpeed</code>: This determines how fast noise fluctuates, 1 means no fluctuation and higher values like 1E6 means full fluctuation. * <code>RandSize</code>: This determines how coarse noise is, 1 means every pixel has its own noise while higher value means more than 1 pixels share same noise value. * <code>RandDensity</code>: This determines how many pixels out of total will have noise, 1 means all pixels while higher value means lesser number of pixels (exponentially).</p>"},{"location":"settings.html#horizontal-bump-distortion","title":"Horizontal bump distortion","text":"<p>This adds horizontal bumps / flickering / ghosting effect. * <code>HorzWaveContrib</code>: This determines blend ratio of noise pixel with image pixel, 0 means no noise and 1 means only noise. * <code>HorzWaveStrength</code>: This determines overall strength of the effect. * <code>HorzWaveVertSize</code>: This determines how many vertical pixels would be effected by the effect. * <code>HorzWaveScreenSize</code>: This determines how much of the screen is effected by the effect.</p>"},{"location":"settings.html#horizontal-noise-lines","title":"Horizontal noise lines","text":"<p>This adds regions of noise on horizontal lines. * <code>HorzNoiseLinesContrib</code>: This determines blend ratio of noise pixel with image pixel, 0 means no noise and 1 means only noise. * <code>HorzNoiseLinesDensityY</code>: This determines how many pixels in horizontal line gets affected. * <code>HorzNoiseLinesDensityXY</code>: This determines how many lines on screen gets affected.</p>"},{"location":"settings.html#horizontal-line-distortion","title":"Horizontal line distortion","text":"<p>This adds fluctuations on horizontal line. * <code>HorzDistortionContrib</code>: This determines blend ratio of noise pixel with image pixel, 0 means no noise and 1 means only noise. * <code>HorzDistortionStrength</code>: This determines how large is the distortion.</p>"},{"location":"settings.html#gimbal","title":"Gimbal","text":"<p>The <code>Gimbal</code> element allows to freeze camera orientation for pitch, roll and/or yaw. This setting is ignored unless <code>ImageType</code> is -1. The <code>Stabilization</code> is defaulted to 0 meaning no gimbal i.e. camera orientation changes with body orientation on all axis. The value of 1 means full stabilization. The value between 0 to 1 acts as a weight for fixed angles specified (in degrees, in world-frame) in <code>Pitch</code>, <code>Roll</code> and <code>Yaw</code> elements and orientation of the vehicle body. When any of the angles is omitted from json or set to NaN, that angle is not stabilized (i.e. it moves along with vehicle body).</p>"},{"location":"settings.html#unrealengine","title":"UnrealEngine","text":"<p>This element contains settings specific to the Unreal Engine. These will be ignored in the Unity project. * <code>PixelFormatOverride</code>: This contains a list of elements that have both a <code>ImageType</code> and <code>PixelFormat</code> setting. Each element allows you to override the default pixel format of the UTextureRenderTarget2D object instantiated for the capture specified by the <code>ImageType</code> setting. Specifying this element allows you to prevent crashes caused by unexpected pixel formats (see #4120 and #4339 for examples of these crashes). A full list of pixel formats can be viewed here.</p>"},{"location":"settings.html#external-cameras","title":"External Cameras","text":"<p>This element allows specifying cameras which are separate from the cameras attached to the vehicle, such as a CCTV camera. These are fixed cameras, and don't move along with the vehicles. The key in the element is the name of the camera, and the value i.e. settings are the same as <code>CameraDefaults</code> described above. All the camera APIs work with external cameras, including capturing images, changing the pose, etc by passing the parameter <code>external=True</code> in the API call.</p>"},{"location":"settings.html#vehicles-settings","title":"Vehicles Settings","text":"<p>Each simulation mode will go through the list of vehicles specified in this setting and create the ones that has <code>\"AutoCreate\": true</code>. Each vehicle specified in this setting has key which becomes the name of the vehicle. If <code>\"Vehicles\"</code> element is missing then this list is populated with default car named \"PhysXCar\" and default multirotor named \"SimpleFlight\".</p>"},{"location":"settings.html#common-vehicle-setting","title":"Common Vehicle Setting","text":"<ul> <li><code>VehicleType</code>: This could be any one of the following - <code>PhysXCar</code>, <code>SimpleFlight</code>, <code>PX4Multirotor</code>, <code>ComputerVision</code>, <code>ArduCopter</code> &amp; <code>ArduRover</code>. There is no default value therefore this element must be specified.</li> <li><code>PawnPath</code>: This allows to override the pawn blueprint to use for the vehicle. For example, you may create new pawn blueprint derived from ACarPawn for a warehouse robot in your own project outside the AutonomySim code and then specify its path here. See also PawnPaths. Note that you have to specify your custom pawn blueprint class path inside the global <code>PawnPaths</code> object using your proprietarily defined object name, and quote that name inside the <code>Vehicles</code> setting. For example,</li> </ul> <pre><code>    {\n      ...\n      \"PawnPaths\": {\n        \"CustomPawn\": {\"PawnBP\": \"Class'/Game/Assets/Blueprints/MyPawn.MyPawn_C'\"}\n      },\n      \"Vehicles\": {\n        \"MyVehicle\": {\n          \"VehicleType\": ...,\n          \"PawnPath\": \"CustomPawn\",\n          ...\n        }\n      }\n    }\n</code></pre> <ul> <li><code>DefaultVehicleState</code>: Possible value for multirotors is <code>Armed</code> or <code>Disarmed</code>.</li> <li><code>AutoCreate</code>: If true then this vehicle would be spawned (if supported by selected sim mode).</li> <li><code>RC</code>: This sub-element allows to specify which remote controller to use for vehicle using <code>RemoteControlID</code>. The value of -1 means use keyboard (not supported yet for multirotors). The value &gt;= 0 specifies one of many remote controllers connected to the system. The list of available RCs can be seen in Game Controllers panel in Windows, for example.</li> <li><code>X, Y, Z, Yaw, Roll, Pitch</code>: These elements allows you to specify the initial position and orientation of the vehicle. Position is in NED coordinates in SI units with origin set to Player Start location in Unreal environment. The orientation is specified in degrees.</li> <li><code>IsFpvVehicle</code>: This setting allows to specify which vehicle camera will follow and the view that will be shown when ViewMode is set to Fpv. By default, AutonomySim selects the first vehicle in settings as FPV vehicle.</li> <li><code>Sensors</code>: This element specifies the sensors associated with the vehicle, see Sensors page for details.</li> <li><code>Cameras</code>: This element specifies camera settings for vehicle. The key in this element is name of the available camera and the value is same as <code>CameraDefaults</code> as described above. For example, to change FOV for the front center camera to 120 degrees, you can use this for <code>Vehicles</code> setting:</li> </ul> <pre><code>\"Vehicles\": {\n    \"FishEyeDrone\": {\n      \"VehicleType\": \"SimpleFlight\",\n      \"Cameras\": {\n        \"front-center\": {\n          \"CaptureSettings\": [\n            {\n              \"ImageType\": 0,\n              \"FOV_Degrees\": 120\n            }\n          ]\n        }\n      }\n    }\n}\n</code></pre>"},{"location":"settings.html#using-px4","title":"Using PX4","text":"<p>By default we use simple_flight so you don't have to do separate HITL or SITL setups. We also support \"PX4\" for advanced users. To use PX4 with AutonomySim, you can use the following for <code>Vehicles</code> setting:</p> <pre><code>\"Vehicles\": {\n    \"PX4\": {\n      \"VehicleType\": \"PX4Multirotor\",\n    }\n}\n</code></pre>"},{"location":"settings.html#additional-px4-settings","title":"Additional PX4 Settings","text":"<p>The defaults for PX4 is to enable hardware-in-loop setup. There are various other settings available for PX4 as follows with their default values:</p> <pre><code>\"Vehicles\": {\n    \"PX4\": {\n      \"VehicleType\": \"PX4Multirotor\",\n      \"Lockstep\": true,\n      \"ControlIp\": \"127.0.0.1\",\n      \"ControlPortLocal\": 14540,\n      \"ControlPortRemote\": 14580,\n      \"LogViewerHostIp\": \"127.0.0.1\",\n      \"LogViewerPort\": 14388,\n      \"OffboardCompID\": 1,\n      \"OffboardSysID\": 134,\n      \"QgcHostIp\": \"127.0.0.1\",\n      \"QgcPort\": 14550,\n      \"SerialBaudRate\": 115200,\n      \"SerialPort\": \"*\",\n      \"SimCompID\": 42,\n      \"SimSysID\": 142,\n      \"TcpPort\": 4560,\n      \"UdpIp\": \"127.0.0.1\",\n      \"UdpPort\": 14560,\n      \"UseSerial\": true,\n      \"UseTcp\": false,\n      \"VehicleCompID\": 1,\n      \"VehicleSysID\": 135,\n      \"Model\": \"Generic\",\n      \"LocalHostIp\": \"127.0.0.1\",\n      \"Logs\": \"d:\\\\temp\\\\mavlink\",\n      \"Sensors\": {\n        ...\n      }\n      \"Parameters\": {\n        ...\n      }\n    }\n}\n</code></pre> <p>These settings define the MavLink SystemId and ComponentId for the Simulator (SimSysID, SimCompID), and for the vehicle (VehicleSysID, VehicleCompID) and the node that allows remote control of the drone from another app this is called the offboard node (OffboardSysID, OffboardCompID).</p> <p>If you want the simulator to also forward mavlink messages to your ground control app (like QGroundControl) you can also set the UDP address for that in case you want to run that on a different machine (QgcHostIp, QgcPort).  The default is local host so QGroundControl should \"just work\" if it is running on the same machine.</p> <p>You can connect the simulator to the LogViewer app, provided in this repo, by setting the UDP address for that (LogViewerHostIp, LogViewerPort).</p> <p>And for each flying drone added to the simulator there is a named block of additional settings. In the above you see the default name \"PX4\". You can change this name from the Unreal Editor when you add a new BP_FlyingPawn asset. You will see these properties grouped under the category \"MavLink\". The MavLink node for this pawn can be remote over UDP or it can be connected to a local serial port. If serial then set UseSerial to true, otherwise set UseSerial to false. For serial connections you also need to set the appropriate SerialBaudRate. The default of 115200 works with Pixhawk version 2 over USB.</p> <p>When communicating with the PX4 drone over serial port both the HIL_ messages and vehicle control messages share the same serial port. When communicating over UDP or TCP PX4 requires two separate channels. If UseTcp is false, then UdpIp, UdpPort are used to send HIL_ messages, otherwise the TcpPort is used. TCP support in PX4 was added in 1.9.2 with the <code>lockstep</code> feature because the guarantee of message delivery that TCP provides is required for the proper functioning of lockstep. AutonomySim becomes a TCP server in that case, and waits for a connection from the PX4 app. The second channel for controlling the vehicle is defined by (ControlIp, ControlPort) and is always a UDP channel.</p> <p>The <code>Sensors</code> section can provide customized settings for simulated sensors, see Sensors. The <code>Parameters</code> section can set PX4 parameters during initialization of the PX4 connection. See Setting up PX4 Software-in-Loop for an example.</p>"},{"location":"settings.html#using-ardupilot","title":"Using ArduPilot","text":"<p>ArduPilot Copter &amp; Rover vehicles are supported in latest AutonomySim main branch &amp; releases <code>v1.3.0</code> and later. For settings and how to use, please see ArduPilot SITL with AutonomySim</p>"},{"location":"settings.html#other-settings","title":"Other Settings","text":""},{"location":"settings.html#enginesound","title":"EngineSound","text":"<p>To turn off the engine sound use setting <code>\"EngineSound\": false</code>. Currently this setting applies only to car.</p>"},{"location":"settings.html#pawnpaths","title":"PawnPaths","text":"<p>This allows you to specify your own vehicle pawn blueprints, for example, you can replace the default car in AutonomySim with your own car. Your vehicle BP can reside in Content folder of your own Unreal project (i.e. outside of AutonomySim plugin folder). For example, if you have a car BP located in file <code>Content\\MyCar\\MySedanBP.uasset</code> in your project then you can set <code>\"DefaultCar\": {\"PawnBP\":\"Class'/Game/MyCar/MySedanBP.MySedanBP_C'\"}</code>. The <code>XYZ.XYZ_C</code> is a special notation required to specify class for BP <code>XYZ</code>. Please note that your BP must be derived from CarPawn class. By default this is not the case but you can re-parent the BP using the \"Class Settings\" button in toolbar in UE editor after you open the BP and then choosing \"Car Pawn\" for Parent Class settings in Class Options. It is also a good idea to disable \"Auto Possess Player\" and \"Auto Possess AI\" as well as set AI Controller Class to None in BP details. Please make sure your asset is included for cooking in packaging options if you are creating binary.</p>"},{"location":"settings.html#physicsenginename","title":"PhysicsEngineName","text":"<p>For cars, we support only PhysX for now (regardless of value in this setting). For multirotors, we support <code>\"FastPhysicsEngine\"</code> and <code>\"ExternalPhysicsEngine\"</code>. <code>\"ExternalPhysicsEngine\"</code> allows the drone to be controlled via setVehiclePose (), keeping the drone in place until the next call. It is especially useful for moving the AutonomySim drone using an external simulator or on a saved path.</p>"},{"location":"settings.html#localhostip-setting","title":"LocalHostIp Setting","text":"<p>Now when connecting to remote machines you may need to pick a specific Ethernet adapter to reach those machines, for example, it might be over Ethernet or over Wi-Fi, or some other special virtual adapter or a VPN.  Your PC may have multiple networks, and those networks might not be allowed to talk to each other, in which case the UDP messages from one network will not get through to the others.</p> <p>So the LocalHostIp allows you to configure how you are reaching those machines.  The default of 127.0.0.1 is not able to reach external machines, this default is only used when everything you are talking to is contained on a single PC.</p>"},{"location":"settings.html#apiserverport","title":"ApiServerPort","text":"<p>This setting determines the server port that used by AutonomySim clients, default port is 41451. By specifying different ports, the user can run multiple environments in parallel to accelerate data collection process.</p>"},{"location":"settings.html#speedunitfactor","title":"SpeedUnitFactor","text":"<p>Unit conversion factor for speed related to <code>m/s</code>, default is 1. Used in conjunction with SpeedUnitLabel. This may be only used for display purposes for example on-display speed when car is being driven. For example, to get speed in <code>miles/hr</code> use factor 2.23694.</p>"},{"location":"settings.html#speedunitlabel","title":"SpeedUnitLabel","text":"<p>Unit label for speed, default is <code>m/s</code>.  Used in conjunction with SpeedUnitFactor.</p>"},{"location":"settings_upgrading.html","title":"Upgrading the Settings","text":"<p>The settings schema in <code>AutonomySim</code> 1.2 is changed for more flexibility and cleaner interface. If you have older settings.json file then you can either delete it and restart AutonomySim or use this guide to make manual upgrade.</p>"},{"location":"settings_upgrading.html#quicker-way","title":"Quicker Way","text":"<p>We recommend simply deleting the settings.json and add back the settings you need. Please see the doc for complete information on available settings.</p>"},{"location":"settings_upgrading.html#changes","title":"Changes","text":""},{"location":"settings_upgrading.html#usagescenario","title":"UsageScenario","text":"<p>Previously we used <code>UsageScenario</code> to specify the <code>ComputerVision</code> mode. Now we use <code>\"SimMode\": \"ComputerVision\"</code> instead.</p>"},{"location":"settings_upgrading.html#cameradefaults-and-changing-camera-settings","title":"CameraDefaults and Changing Camera Settings","text":"<p>Previously we had <code>CaptureSettings</code> and <code>NoiseSettings</code> in root. Now these are combined in new <code>CameraDefaults</code> element. The schema for this element is later used to configure cameras on vehicle.</p>"},{"location":"settings_upgrading.html#gimbal","title":"Gimbal","text":"<p>The Gimbal element (instead of old Gimble element) is now moved out of <code>CaptureSettings</code>.</p>"},{"location":"settings_upgrading.html#cameraid-to-cameraname","title":"CameraID to CameraName","text":"<p>All settings now reference cameras by name instead of ID.</p>"},{"location":"settings_upgrading.html#using-px4","title":"Using PX4","text":"<p>The new Vehicles element allows to specify which vehicles to create. To use PX4, please see this section.</p>"},{"location":"settings_upgrading.html#additionalcameras","title":"AdditionalCameras","text":"<p>The old <code>AdditionalCameras</code> setting is now replaced by Cameras element within vehicle setting.</p>"},{"location":"simple_flight.html","title":"Simple Flight Controller","text":"<p><code>AutonomySim</code> has a built-in flight controller called <code>simple_flight</code> that is used by default. You do not need to do anything to use or configure it. AutonomySim also supports PX4 as another flight controller for advanced users. In the future, we also plan to support ROSFlight and Hackflight.</p> <p>For background information, see what is a flight controller?.</p>"},{"location":"simple_flight.html#advantages","title":"Advantages","text":"<p>The advantage of using <code>simple_flight</code> is zero additional setup you need to do and it \"just works\". Also, <code>simple_flight</code> uses a steppable clock which means you can pause the simulation and things are not at mercy of a high variance low precision clock that the operating system provides. Furthermore, <code>simple_flight</code> is simple, cross platform and consists of 100% header-only dependency-free C++ code which means you can literally switch between the simulator and the flight controller code within same codebase.</p>"},{"location":"simple_flight.html#design","title":"Design","text":"<p>Normally flight controllers are designed to run on actual hardware of vehicles and their support for running in simulator varies widely. They are often fairly difficult to configure for non-expert users and typically have a complex build, usually lacking cross platform support. All these problems have played a significant part in the design of simple_flight.</p> <p><code>simple_flight</code> is designed from ground up as library with clean a interface that can work onboard the vehicle as well as in the simulator. The core principle is that the flight controller has no way to specify a special simulation mode and therefore it has no way to know if it is running as a simulation or as a real vehicle. We thus view flight controllers simply as a collection of algorithms packaged in a library. Another key emphasis is to develop this code as dependency-free header-only pure standard C++11 code. This means there is no special build required to compile <code>simple_flight</code>. You just copy its source code to any project you wish and it just works.</p>"},{"location":"simple_flight.html#control","title":"Control","text":"<p><code>simple_flight</code> can control vehicles by taking in the desired input as angle rate, angle level, velocity or position. Each axis of control can be specified with one of these modes. Internally, <code>simple_flight</code> uses a cascade of PID controllers to finally generate actuator signals. This means that the position PID drives the velocity PID, which in turn drives the angle level PID which finally drives the angle rate PID.</p>"},{"location":"simple_flight.html#state-estimation","title":"State Estimation","text":"<p>In the current release, we are using the ground truth from the simulator for our state estimation. We plan to add a complimentary filter-based state estimator for angular velocity and orientation using 2 sensors (gyroscope, accelerometer) in the near future. In a more longer term, we plan to integrate another library to perform velocity and position estimation using 4 sensors (gyroscope, accelerometer, magnetometer and barometer) using an Extended Kalman Filter (EKF). If you have experience in this area, we encourage you to engage with us and contribute!</p>"},{"location":"simple_flight.html#supported-boards","title":"Supported Boards","text":"<p>Currently, we have implemented <code>simple_flight</code> interfaces for the simulated board. We plan to implement it for the Pixhawk V2 board and possibly the Naze32 board. We expect all our code to remain unchanged and the implementation would mainly involve adding drivers for various sensors, handling ISRs and managing other board specific details.</p> <p>Note</p> <p>If you have experience in this area, we encourage you to engage with us and contribute.</p>"},{"location":"simple_flight.html#configuration","title":"Configuration","text":"<p>To have <code>AutonomySim</code> use simple_flight, you can specify it in settings.json as shown below. Note that this is default, so you don't have to do it explicitly.</p> <pre><code>\"Vehicles\": {\n    \"SimpleFlight\": {\n      \"VehicleType\": \"SimpleFlight\",\n    }\n}\n</code></pre> <p>By default, a vehicle using simple_flight is already armed which is why you would see its propellers spinning. However, if you don't want that then set <code>DefaultVehicleState</code> to <code>Inactive</code> like this:</p> <pre><code>\"Vehicles\": {\n    \"SimpleFlight\": {\n      \"VehicleType\": \"SimpleFlight\",\n      \"DefaultVehicleState\": \"Inactive\"\n    }\n}\n</code></pre> <p>In this case, you will need to either manually arm by placing the RC sticks in the down-inward position or using the APIs.</p> <p>For safety reasons, flight controllers disallow API control unless a human operator has consented its use using a switch on his/her RC. Also, when RC control is lost, the vehicle should disable API control and enter hover mode for safety reasons. To simplify things a bit, simple_flight enables API control without human consent using RC and even when RC is not detected by default. However you can change this using the following setting:</p> <pre><code>\"Vehicles\": {\n    \"SimpleFlight\": {\n      \"VehicleType\": \"SimpleFlight\",\n\n      \"AllowAPIAlways\": true,\n      \"RC\": {\n        \"RemoteControlID\": 0,      \n        \"AllowAPIWhenDisconnected\": true\n      }\n    }\n}\n</code></pre> <p>Finally, <code>simple_flight</code> uses a steppable clock by default which means that the clock advances when the simulator tells it to advance (unlike the wall clock which advances strictly according to the passage of time). This means the clock can be paused, for example, if code hits a breakpoint and there is zero variance in the clock (clock APIs provided by operating systems might have significant variance unless it is a \"real time\" OS). If you want <code>simple_flight</code> to use a wall clock instead then use following settings:</p> <pre><code>\"ClockType\": \"ScalableClock\"\n</code></pre>"},{"location":"surveying.html","title":"Surveying","text":"<p>Moved here from https://github.com/nervosys/AutonomySim/wiki/Implementing-a-Drone-Survey-script</p> <p>Ever wanted to capture a bunch of top-down pictures of a certain location? Well, the Python API makes this really simple. See the code available here.</p> <p></p> <p>Let's assume we want the following variables:</p> Variable Description boxsize The overall size of the square box to survey stripewidth How far apart to drive the swim lanes, this can depend on the type of camera lens, for example. altitude The height to fly the survey. speed The speed of the survey can depend on how fast your camera can snap shots. <p>So with these we can compute a square path box using this code:</p> <pre><code>path = []\ndistance = 0\nwhile x &lt; self.boxsize:\n    distance += self.boxsize\n    path.append(Vector3r(x, self.boxsize, z))\n    x += self.stripewidth\n    distance += self.stripewidth\n    path.append(Vector3r(x, self.boxsize, z))\n    distance += self.boxsize\n    path.append(Vector3r(x, -self.boxsize, z))\n    x += self.stripewidth\n    distance += self.stripewidth\n    path.append(Vector3r(x, -self.boxsize, z))\n    distance += self.boxsize\n</code></pre> <p>Assuming we start in the corner of the box, increment <code>x</code> by the stripe width, then fly the full <code>y</code>-dimension of <code>-boxsize</code> to <code>+boxsize</code>, so in this case, <code>boxsize</code> is half the size of the actual box we will be covering.</p> <p>Once we have this list of <code>Vector3r</code> objects, we can fly this path very simply with the following call:</p> <pre><code>result = self.client.moveOnPath(\n    path, self.velocity, trip_time, DrivetrainType.ForwardOnly, YawMode(False,0), lookahead, 1)\n</code></pre> <p>We can compute an appropriate <code>trip_time</code> timeout by dividing the distance of the path and the speed we are flying.</p> <p>The <code>lookahead</code> needed here for smooth path interpolation can be computed from the velocity using <code>self.velocity + (self.velocity/2)</code>.  The more lookahead, the smoother the turns.  This is why you see in the screenshot that the ends of each swimland are smooth turns rather than a square box pattern.  This can result in a smoother video from your camera also.</p> <p>That's it. Pretty simple, huh?</p> <p>Of course, we can add a lot more intelligence to this, make it avoid known obstacles on the map, make it climb up and down a hillside so you can survey a slope, etc. This is only the tip of the iceberg in terms of what is possible.</p>"},{"location":"texture_swapping.html","title":"Runtime Texture Swapping","text":""},{"location":"texture_swapping.html#how-to-make-an-actor-retexturable","title":"How to Make An Actor Retexturable","text":"<p>To be made texture-swappable, an actor must derive from the parent class <code>TextureShuffleActor</code>. The parent class can be set via the settings tab in the actor's blueprint.</p> <p></p> <p>After setting the parent class to TextureShuffActor, the object gains the member <code>DynamicMaterial</code>. <code>DynamicMaterial</code> needs to be set--on all actor instances in the scene--to <code>TextureSwappableMaterial</code>.</p> <p>Warning</p> <p>Statically setting the Dynamic Material in the blueprint class may cause rendering errors. It seems to work better to set it on all the actor instances in the scene, using the details panel.</p> <p></p>"},{"location":"texture_swapping.html#how-to-define-the-sets-of-textures-to-choose-from","title":"How to Define the Set(s) of Textures to Choose From","text":"<p>Typically, certain subsets of actors will share a set of texture options with each other (e.g., walls that are part of the same building). It's easy to set up these groupings by using Unreal Engine's group editing functionality. Select all the instances that should have the same texture selection, and add the textures to all of them simultaneously via the Details panel. Use the same technique to add descriptive tags to groups of actors, which will be used to address them in the API.</p> <p></p> <p>It's ideal to work from larger groupings to smaller groupings, simply deselecting actors to narrow down the grouping as you go, and applying any individual actor properties last.</p> <p></p>"},{"location":"texture_swapping.html#how-to-swap-textures-from-the-api","title":"How to Swap Textures from the API","text":"<p>The following API is available in C++ and python. (C++ shown)</p> <pre><code>std::vector&lt;std::string&gt; simSwapTextures(const std::string&amp; tags, int tex_id);\n</code></pre> <p>The string of \",\" or \", \" delimited tags identifies on which actors to perform the swap. The tex_id indexes the array of textures assigned to each actor undergoing a swap. The function will return the list of objects which matched the provided tags and had the texture swap perfomed. If <code>tex_id</code> is out-of-bounds for some object's texture set, it will be taken modulo the number of textures that were available.</p> <p>Demo (Python):</p> <pre><code>import autonomysim\nimport time\n\nc = autonomysim.client.MultirotorClient()\nprint(c.simSwapTextures(\"furniture\", 0))\ntime.sleep(2)\nprint(c.simSwapTextures(\"chair\", 1))\ntime.sleep(2)\nprint(c.simSwapTextures(\"table\", 1))\ntime.sleep(2)\nprint(c.simSwapTextures(\"chair, right\", 0))\n</code></pre> <p>Results:</p> <pre><code>['RetexturableChair', 'RetexturableChair2', 'RetexturableTable']\n['RetexturableChair', 'RetexturableChair2']\n['RetexturableTable']\n['RetexturableChair2']\n</code></pre> <p></p> <p>Note that in this example, different textures were chosen on each actor for the same index value.</p> <p>You can also use the <code>simSetObjectMaterial</code> and <code>simSetObjectMaterialFromTexture</code> APIs to set an object's material to any material asset or filepath of a texture. For more information on using these APIs, see Texture APIs.</p>"},{"location":"unreal_blocks.html","title":"Blocks Environment","text":"<p>The <code>Blocks</code> environment is available in the folder <code>Unreal/Environments/Blocks</code> and is designed to be lightweight in size. That means its very basic but fast. Below are quick steps to get Blocks environment up and running.</p>"},{"location":"unreal_blocks.html#windows","title":"Windows","text":"<ol> <li>Ensure you have installed Unreal and built AutonomySim.</li> <li>Navigate to folder <code>AutonomySim\\Unreal\\Environments\\Blocks</code>, double click on Blocks.sln file to open in Visual Studio. By default, this project is configured for Visual Studio 2019. However, if you want to generate this project for Visual Studio 2022, go to 'Edit-&gt;Editor Preferences-&gt;Source Code' inside the Unreal Editor and select 'Visual Studio 2022' for the 'Source Code Editor' setting.</li> <li>Make sure <code>Blocks</code> project is the startup project, build configuration is set to <code>DebugGame_Editor</code> and <code>Win64</code>. Hit F5 to run.</li> <li>Press the Play button in Unreal Editor and you will see something like in below video. Also see how to use AutonomySim.</li> </ol>"},{"location":"unreal_blocks.html#changing-code-and-rebuilding","title":"Changing Code and Rebuilding","text":"<p>For Windows, you can just change the code in Visual Studio, press F5 and re-run. There are few batch files available in folder <code>AutonomySim\\Unreal\\Environments\\Blocks</code> that lets you sync code, clean etc.</p>"},{"location":"unreal_blocks.html#linux","title":"Linux","text":"<ol> <li>Make sure you have built the Unreal Engine and AutonomySim.</li> <li>Navigate to your UnrealEngine repo folder and run <code>Engine/Binaries/Linux/UE4Editor</code> which will start Unreal Editor.</li> <li>On first start you might not see any projects in UE4 editor. Click on Projects tab, Browse button and then navigate to <code>AutonomySim/Unreal/Environments/Blocks/Blocks.uproject</code>. </li> <li>If you get prompted for incompatible version and conversion, select In-place conversion which is usually under \"More\" options. If you get prompted for missing modules, make sure to select No so you don't exit. </li> <li>Finally, when prompted with building AutonomySim, select Yes. Now it might take a while so go get some coffee :).</li> <li>Press the Play button in Unreal Editor and you will see something like in below video. Also see how to use AutonomySim.</li> </ol>"},{"location":"unreal_blocks.html#changing-code-and-rebuilding_1","title":"Changing Code and Rebuilding","text":"<p>For Linux, make code changes in AutonomyLib or Unreal/Plugins folder and then run <code>./build.sh</code> to rebuild. This step also copies the build output to Blocks sample project. You can then follow above steps again to re-run.</p>"},{"location":"unreal_blocks.html#chosing-your-vehicle-car-or-multirotor","title":"Chosing Your Vehicle: Car or Multirotor","text":"<p>By default AutonomySim spawns multirotor. You can easily change this to car and use all of AutonomySim goodies. Please see using car guide.</p>"},{"location":"unreal_blocks.html#faq","title":"FAQ","text":""},{"location":"unreal_blocks.html#i-see-warnings-about-like-_buitdata-file-is-missing","title":"I see warnings about like <code>_BuitData</code> file is missing","text":"<p>These are intermediate files and you can safely ignore it.</p>"},{"location":"unreal_envs.html","title":"Unreal Environments","text":"<p>This page contains the complete instructions start to finish for setting up Unreal environment with <code>AutonomySim</code>. The <code>Unreal Marketplace</code> has several environments available that you can start using in just few minutes. It is also possible to use environments available on websites such as turbosquid.com or cgitrader.com with bit more effort (here's tutorial video). In addition there also several free environments available.</p> <p>Below we will use a freely downloadable environment from Unreal Marketplace called Landscape Mountain but the steps are same for any other environments.</p>"},{"location":"unreal_envs.html#note-for-linux-users","title":"Note for Linux Users","text":"<p>There is no <code>Epic Games Launcher</code> for Linux which means that if you need to create custom environment, you will need Windows machine to do that. Once you have Unreal project folder, just copy it over to your Linux machine.</p>"},{"location":"unreal_envs.html#step-by-step-instructions","title":"Step by Step Instructions","text":"<ol> <li>Ensure <code>AutonomySim</code> is built and Unreal 4.27 is installed as described in build instructions.</li> <li> <p>In <code>Epic Games Launcher</code> click the Learn tab then scroll down and find <code>Landscape Mountains</code>. Click the <code>Create Project</code> and download this content (~2GB download).</p> <p></p> </li> <li> <p>Open <code>LandscapeMountains.uproject</code>, it should launch the Unreal Editor.</p> <p></p> <p>Note</p> <p>The Landscape Mountains project is supported up to Unreal Engine version 4.24. If you do not have 4.24 installed, you should see a dialog titled <code>Select Unreal Engine Version</code> with a dropdown to select from installed versions. Select 4.27 or greater to migrate the project to a supported engine version. If you have 4.24 installed, you can manually migrate the project by navigating to the corresponding .uproject file in Windows Explorer, right-clicking it, and selecting the <code>Switch Unreal Engine version...</code> option. </p> </li> <li> <p>From the <code>File menu</code> select <code>New C++ class</code>, leave default <code>None</code> on the type of class, click <code>Next</code>, leave default name <code>MyClass</code>, and click <code>Create Class</code>. We need to do this because Unreal requires at least one source file in project. It should trigger compile and open up Visual Studio solution <code>LandscapeMountains.sln</code>.</p> </li> <li> <p>Go to your folder for AutonomySim repo and copy <code>Unreal\\Plugins</code> folder in to your <code>LandscapeMountains</code> folder. This way now your own Unreal project has AutonomySim plugin.</p> <p>Note</p> <p>If the AutonomySim installation is fresh, i.e, hasn't been built before, make sure that you run <code>build.cmd</code> from the root directory once before copying <code>Unreal\\Plugins</code> folder so that <code>AutonomyLib</code> files are also included. If you have made some changes in the Blocks environment, make sure to run <code>update_to_git.cmd</code> from <code>Unreal\\Environments\\Blocks</code> to update the files in <code>Unreal\\Plugins</code>.</p> </li> <li> <p>Edit the <code>LandscapeMountains.uproject</code> so that it looks like this</p> <pre><code>{\n    \"FileVersion\": 3,\n    \"EngineAssociation\": \"4.27\",\n    \"Category\": \"Samples\",\n    \"Description\": \"\",\n    \"Modules\": [\n        {\n            \"Name\": \"LandscapeMountains\",\n            \"Type\": \"Runtime\",\n            \"LoadingPhase\": \"Default\",\n            \"AdditionalDependencies\": [\n                \"AutonomySim\"\n            ]\n        }\n    ],\n    \"TargetPlatforms\": [\n        \"MacNoEditor\",\n        \"WindowsNoEditor\"\n    ],\n    \"Plugins\": [\n        {\n            \"Name\": \"AutonomySim\",\n            \"Enabled\": true\n        }\n    ]\n}\n</code></pre> </li> <li> <p>Edit the <code>Config\\DefaultGame.ini</code> to add the following line at the end:</p> <pre><code>+MapsToCook=(FilePath=\"/AutonomySim/AutonomySimAssets\")\n</code></pre> <p>Doing this forces Unreal to include all necessary AutonomySim content in packaged builds of your project.</p> </li> <li> <p>Close Visual Studio and the  <code>Unreal Editor</code> and right click the LandscapeMountains.uproject in Windows Explorer and select <code>Generate Visual Studio Project Files</code>.  This step detects all plugins and source files in your Unreal project and generates <code>.sln</code> file for Visual Studio.</p> <p></p> <p>Tip</p> <p>If the <code>Generate Visual Studio Project Files</code> option is missing you may need to reboot your machine for the Unreal Shell extensions to take effect.  If it is still missing then open the LandscapeMountains.uproject in the Unreal Editor and select <code>Refresh Visual Studio Project</code> from the <code>File</code> menu.</p> </li> <li> <p>Reopen <code>LandscapeMountains.sln</code> in Visual Studio, and make sure \"DebugGame Editor\" and \"Win64\" build configuration is the active build configuration.</p> <p></p> </li> <li> <p>Press <code>F5</code> to <code>run</code>. This will start the Unreal Editor. The Unreal Editor allows you to edit the environment, assets and other game related settings. First thing you want to do in your environment is set up <code>PlayerStart</code> object. In Landscape Mountains environment, <code>PlayerStart</code> object already exist and you can find it in the <code>World Outliner</code>. Make sure its location is setup as shown. This is where AutonomySim plugin will create and place the vehicle. If its too high up then vehicle will fall down as soon as you press play giving potentially random behavior</p> <p></p> </li> <li> <p>In <code>Window/World Settings</code> as shown below, set the <code>GameMode Override</code> to <code>AutonomySimGameMode</code>:</p> <p></p> </li> <li> <p>Go to 'Edit-&gt;Editor Preferences' in Unreal Editor, in the 'Search' box type 'CPU' and ensure that the 'Use Less CPU when in Background' is unchecked. If you don't do this then UE will be slowed down dramatically when UE window loses focus.</p> </li> <li> <p>Be sure to <code>Save</code> these edits. Hit the Play button in the Unreal Editor. See how to use AutonomySim.</p> </li> </ol> <p>Congratulations! You are now running AutonomySim in your own Unreal environment.</p>"},{"location":"unreal_envs.html#choosing-your-vehicle-car-or-multirotor","title":"Choosing Your Vehicle: Car or Multirotor","text":"<p>By default AutonomySim prompts user for which vehicle to use. You can easily change this by setting SimMode. Please see using car guide.</p>"},{"location":"unreal_envs.html#updating-your-environment-to-latest-version-of-autonomysim","title":"Updating Your Environment to Latest Version of AutonomySim","text":"<p>Once you have your environment using above instructions, you should frequently update your local AutonomySim code to latest version from GitHub. Below are the instructions to do this:</p> <ol> <li>First put clean.cmd (or clean.sh for Linux users) in the root folder of your environment. Run this file to clean up all intermediate files in your Unreal project.</li> <li>Do <code>git pull</code> in your AutonomySim repo followed by <code>build.cmd</code> (or <code>./build.sh</code> for Linux users).</li> <li>Replace [your project]/Plugins folder with AutonomySim/Unreal/Plugins folder.</li> <li>Right click on your .uproject file and chose \"Generate Visual Studio project files\" option. This is not required for Linux.</li> </ol>"},{"location":"unreal_envs.html#faq","title":"FAQ","text":""},{"location":"unreal_envs.html#what-are-other-cool-environments","title":"What are other cool environments?","text":"<p>Unreal Marketplace has dozens of prebuilt extra-ordinarily detailed environments ranging from Moon to Mars and everything in between. The one we have used for testing is called Modular Neighborhood Pack but you can use any environment. Another free environment is Infinity Blade series. Alternatively, if you look under the Learn tab in Epic Game Launcher, you will find many free samples that you can use. One of our favorites is \"A Boy and His Kite\" which is a 100 square miles of highly detailed environment (caution: you will need very beefy PC to run it!).</p>"},{"location":"unreal_envs.html#when-i-press-play-button-some-kind-of-video-starts-instead-of-my-vehicle","title":"When I press Play button some kind of video starts instead of my vehicle.","text":"<p>If the environment comes with MatineeActor, delete it to avoid any startup demo sequences. There might be other ways to remove it as well, for example, click on Blueprints button, then Level Blueprint and then look at Begin Play event in Event Graph. You might want to disconnect any connections that may be starting \"matinee\".</p>"},{"location":"unreal_envs.html#is-there-easy-way-to-sync-code-in-my-unreal-project-with-code-in-autonomysim-repo","title":"Is there easy way to sync code in my Unreal project with code in AutonomySim repo?","text":"<p>Sure, there is! You can find bunch of <code>.cmd</code> files (for linux, <code>.sh</code>) in <code>AutonomySim\\Unreal\\Environments\\Blocks</code>. Just copy them over to your own Unreal project. Most of these are quite simple and self explanatory.</p>"},{"location":"unreal_envs.html#i-get-some-error-about-map","title":"I get some error about map.","text":"<p>You might have to set default map for your project. For example, if you are using Modular Neighborhood Pack, set the Editor Starter Map as well as Game Default Map to Demo_Map in Project Settings &gt; Maps &amp; Modes.</p>"},{"location":"unreal_envs.html#i-see-add-to-project-option-for-environment-but-not-create-project-option","title":"I see \"Add to project\" option for environment but not \"Create project\" option.","text":"<p>In this case, create a new blank C++ project with no Starter Content and add your environment in to it.</p>"},{"location":"unreal_envs.html#i-already-have-my-own-unreal-project-how-do-i-use-autonomysim-with-it","title":"I already have my own Unreal project. How do I use AutonomySim with it?","text":"<p>Copy the <code>Unreal\\Plugins</code> folder from the build you did in the above section into the root of your Unreal project's folder. In your Unreal project's .uproject file, add the key <code>AdditionalDependencies</code> to the \"Modules\" object as we showed in the <code>LandscapeMountains.uproject</code> above.</p> <pre><code>\"AdditionalDependencies\": [\n    \"AutonomySim\"\n]\n</code></pre> <p>and the <code>Plugins</code> section to the top level object:</p> <pre><code>\"Plugins\": [\n    {\n        \"Name\": \"AutonomySim\",\n        \"Enabled\": true\n    }\n]\n</code></pre>"},{"location":"unreal_plugins.html","title":"Unreal Plugin Contents","text":"<p>Plugin contents are not shown in <code>Unreal</code> projects by default. To view plugin content, you need to click on few semi-hidden buttons:</p> <p></p> <p>Caution</p> <p>Changes you make in content folders are changes to binary files, so be careful!</p>"},{"location":"unreal_projects.html","title":"Unreal Projects","text":""},{"location":"unreal_projects.html#setting-up-the-unreal-project","title":"Setting Up the Unreal Project","text":""},{"location":"unreal_projects.html#option-1-built-in-blocks-environment","title":"Option 1: Built-in Blocks Environment","text":"<p>To get up and running fast, you can use the <code>Blocks</code> project that already comes with <code>AutonomySim</code>. This is not very highly detailed environment to keep the repo size reasonable but we use it for various testing all the times and it is the easiest way to get your feet wet in this strange land. </p> <p>Follow these quick steps.</p>"},{"location":"unreal_projects.html#option-2-create-your-own-unreal-environment","title":"Option 2: Create Your Own Unreal Environment","text":"<p>If you want to setup photo-realistic high quality environments, then you will need to create your own Unreal project. This is little bit more involved but worthwhile! </p> <p>Follow this step-by-step guide. </p>"},{"location":"unreal_projects.html#changing-code-and-development-workflow","title":"Changing Code and Development Workflow","text":"<p>To see how you can change and test <code>AutonomySim</code> code, please read our recommended development workflow.</p>"},{"location":"unreal_upgrading.html","title":"Upgrading Unreal Engine","text":"<p>These instructions apply if you are already using <code>AutonomySim</code> on <code>Unreal Engine</code> 4.25. If you have never installed <code>AutonomySim</code>, please see How to get it.</p> <p>Caution</p> <p>The below steps will delete any of your unsaved work in AutonomySim or Unreal folder.</p>"},{"location":"unreal_upgrading.html#first-steps","title":"First Steps","text":""},{"location":"unreal_upgrading.html#windows","title":"Windows","text":"<ol> <li>Install Visual Studio 2022 with VC++, Python and C#.</li> <li>Install UE 4.27 through Epic Games Launcher.</li> <li>Start <code>x64 Native Tools Command Prompt for VS 2022</code> and navigate to AutonomySim repo.</li> <li>Run <code>clean_rebuild.cmd</code> to remove all unchecked/extra stuff and rebuild everything.</li> <li>See also Build AutonomySim on Windows for more information.</li> </ol>"},{"location":"unreal_upgrading.html#linux","title":"Linux","text":"<ol> <li>From your AutonomySim repo folder, run 'clean_rebuild.sh`.</li> <li>Rename or delete your existing folder for Unreal Engine.</li> <li>Follow step 1 and 2 to install Unreal Engine 4.27.</li> <li>See also Build AutonomySim on Linux for more information.</li> </ol>"},{"location":"unreal_upgrading.html#upgrading-your-custom-unreal-project","title":"Upgrading Your Custom Unreal Project","text":"<p>If you have your own Unreal project created in an older version of Unreal Engine then you need to upgrade your project to Unreal 4.27. To do this,</p> <ol> <li>Open .uproject file and look for the line <code>\"EngineAssociation\"</code> and make sure it reads like <code>\"EngineAssociation\": \"4.27\"</code>.</li> <li>Delete <code>Plugins/AutonomySim</code> folder in your Unreal project's folder.</li> <li>Go to your AutonomySim repo folder and copy <code>Unreal\\Plugins</code> folder to your Unreal project's folder.</li> <li>Copy .cmd (or .sh for Linux) from <code>Unreal\\Environments\\Blocks</code> to your project's folder.</li> <li>Run <code>clean.cmd</code> (or <code>clean.sh</code> for Linux) followed by <code>GenerateProjectFiles.cmd</code> (only for Windows).</li> </ol>"},{"location":"unreal_upgrading.html#faq","title":"FAQ","text":""},{"location":"unreal_upgrading.html#i-have-an-unreal-project-that-is-older-than-416-how-do-i-upgrade-it","title":"I have an <code>Unreal</code> project that is older than 4.16. How do I upgrade it?","text":""},{"location":"unreal_upgrading.html#option-1-recreate-the-project","title":"Option 1: Recreate the Project","text":"<p>If your project doesn't have any code or assets other than environment you downloaded then you can also simply recreate the project in Unreal 4.27 Editor and then copy Plugins folder from <code>AutonomySim/Unreal/Plugins</code>.</p>"},{"location":"unreal_upgrading.html#option-2-modify-the-files","title":"Option 2: Modify the Files","text":"<p>Unreal versions newer than Unreal 4.15 has breaking changes. So you need to modify your .Build.cs and .Target.cs which you can find in the <code>Source</code> folder of your Unreal project. So what are those changes? Below is the gist of it but you should really refer to Unreal's official 4.16 transition post.</p>"},{"location":"unreal_upgrading.html#update-the-project-targetcs-file","title":"Update the project <code>*.Target.cs</code> file","text":"<ol> <li> <p>Change the contructor from, <code>public MyProjectTarget(TargetInfo Target)</code> to <code>public MyProjectTarget(TargetInfo Target) : base(Target)</code></p> </li> <li> <p>Remove <code>SetupBinaries</code> method if you have one and instead add following line in contructor above: <code>ExtraModuleNames.AddRange(new string[] { \"MyProject\" });</code></p> </li> </ol>"},{"location":"unreal_upgrading.html#update-the-project-buildcs-file","title":"Update the project <code>*.Build.cs</code> file","text":"<p>Change the constructor from <code>public MyProject(TargetInfo Target)</code> to <code>public MyProject(ReadOnlyTargetRules Target) : base(Target)</code>.</p>"},{"location":"unreal_upgrading.html#last","title":"Last","text":"<p>Follow above steps to continue the upgrade. The warning box might show only \"Open Copy\" button. Don't click that. Instead, click on More Options which will reveal more buttons. Choose <code>Convert-In-Place option</code>.</p> <p>Caution</p> <p>Always keep backup of your project first! If you don't have anything nasty, in place conversion should go through and you are now on the new version of Unreal.</p>"},{"location":"usage_rover.html","title":"Rover Usage","text":"<p>By default, <code>AutonomySim</code> prompts the user for the vehicle to use. You can easily change this by setting SimMode. For example, if you want to use car instead then just set the SimMode in your settings.json which you can find in your <code>~/Documents/AutonomySim</code> folder, like this:</p> <pre><code>{\n  \"SettingsVersion\": 1.2,\n  \"SimMode\": \"Car\"\n}\n</code></pre> <p>Now when you restart AutonomySim, you should see the car spawned automatically.</p>"},{"location":"usage_rover.html#manual-driving","title":"Manual Driving","text":"<p>Please use the keyboard arrow keys to drive manually. Spacebar for the handbrake. In manual drive mode, gears are set in \"auto\".</p>"},{"location":"usage_rover.html#using-apis","title":"Using APIs","text":"<p>You can control the car, get state and images by calling APIs in variety of client languages including C++ and Python. Please see APIs doc for more details.</p>"},{"location":"usage_rover.html#changing-views","title":"Changing Views","text":"<p>By default camera will chase the car from the back. You can get the FPV view by pressing <code>F</code> key and switch back to chasing from back view by pressing <code>/</code> key. More keyboard shortcuts can be seen by pressing F1.</p>"},{"location":"usage_rover.html#cameras","title":"Cameras","text":"<p>By default car is installed with 5 cameras: center, left and right, driver and reverse. You can chose the images from these camera by specifying the name.</p>"},{"location":"userbase.html","title":"Past and Current Users","text":""},{"location":"userbase.html#would-you-like-to-see-your-own-group-or-project-here","title":"Would you like to see your own group or project here?","text":"<p>Just add a GitHub issue with quick details and link to your website. </p> <ul> <li>Nervosys</li> <li>Microsoft</li> <li>NASA Ames Research Center \u2013 Systems Analysis Office</li> <li>Astrobotic</li> <li>GRASP Lab, Univ of Pennsylvania</li> <li>Department of Aeronautics and Astronautics, Stanford University</li> <li>Formula Technion</li> <li>Ghent University</li> <li>ICARUS</li> <li>UC, Santa Barbara</li> <li>WISE Lab, Univ of Waterloo</li> <li>HAMS project, MSR India</li> <li>Washington and Lee University</li> <li>University of Oklahoma</li> <li>Robotics Institute, Carnegie Mellon University</li> <li>Texas A&amp;M</li> <li>Robotics and Perception Group, University of Zurich</li> <li>National University of Ireland, Galway (NUIG)</li> <li>Soda Mobility Technologies</li> <li>University of Cambridge</li> <li>Skoods - AI Autonomous cars competition</li> <li>Teledyne Scientific</li> <li>BladeStack Systems</li> <li>Unizar (Universidad de Zaragoza)</li> <li>ClearSky LLC</li> <li>Myned AI</li> <li>STPLS3D - University of Southern California Institute for Creative Technologies</li> <li>Central Michigan University</li> <li>Scaled Foundations</li> <li>Codex Labs</li> </ul>"},{"location":"vehicles_multiple.html","title":"Multiple Vehicles","text":"<p>Since release 1.2, <code>AutonomySim</code> supports multiple vehicles. This capability allows you to create multiple vehicles easily and use APIs to control them.</p>"},{"location":"vehicles_multiple.html#creating-multiple-vehicles","title":"Creating Multiple Vehicles","text":"<p>It's as easy as specifying them in settings.json. The <code>Vehicles</code> element allows you to specify list of vehicles you want to create along with their initial positions and orientations. The positions are specified in north-east-down (NED) coordinates in SI units with origin set at <code>Player Start</code> component in the Unreal environment. The orientation is specified as <code>Yaw</code>, <code>Pitch</code> and <code>Roll</code> in degrees.</p>"},{"location":"vehicles_multiple.html#creating-multiple-cars","title":"Creating Multiple Cars","text":"<pre><code>{\n    \"SettingsVersion\": 1.2,\n    \"SimMode\": \"Car\",\n\n    \"Vehicles\": {\n        \"Car1\": {\n          \"VehicleType\": \"PhysXCar\",\n          \"X\": 4, \"Y\": 0, \"Z\": -2\n        },\n        \"Car2\": {\n          \"VehicleType\": \"PhysXCar\",\n          \"X\": -4, \"Y\": 0, \"Z\": -2,\n      \"Yaw\": 90\n        }\n  }\n}\n</code></pre>"},{"location":"vehicles_multiple.html#creating-multiple-drones","title":"Creating Multiple Drones","text":"<pre><code>{\n    \"SettingsVersion\": 1.2,\n    \"SimMode\": \"Multirotor\",\n\n    \"Vehicles\": {\n        \"Drone1\": {\n          \"VehicleType\": \"SimpleFlight\",\n          \"X\": 4, \"Y\": 0, \"Z\": -2,\n      \"Yaw\": -180\n        },\n        \"Drone2\": {\n          \"VehicleType\": \"SimpleFlight\",\n          \"X\": 8, \"Y\": 0, \"Z\": -2\n        }\n\n    }\n}\n</code></pre>"},{"location":"vehicles_multiple.html#using-apis-for-multiple-vehicles","title":"Using APIs for Multiple Vehicles","text":"<p>The new APIs since AutonomySim 1.2 allows you to specify <code>vehicle_name</code>. This name corresponds to keys in json settings (for example, Car1 or Drone2 above).</p> <p>Example code for rovers</p> <p>Example code for drones</p> <p>Using APIs for multi-vehicles requires specifying the <code>vehicle_name</code>, which needs to be hardcoded in the script or requires parsing of the settings file. There's also a simple API <code>listVehicles()</code> which returns a list (vector in C++) of strings containing names of the current vehicles. For example, with the above settings for 2 Cars -</p> <pre><code>&gt;&gt;&gt; client.listVehicles()\n['Car1', 'Car2']\n</code></pre>"},{"location":"vehicles_multiple.html#demo","title":"Demo","text":""},{"location":"vehicles_multiple.html#creating-vehicles-at-runtime-through-api","title":"Creating vehicles at runtime through API","text":"<p>In the latest main branch of AutonomySim, the <code>simAddVehicle</code> API can be used to create vehicles at runtime. This is useful to create many such vehicles without needing to specify them in the settings. There are some limitations of this currently, described below -</p> <p><code>simAddVehicle</code> takes in the following arguments:</p> <ul> <li><code>vehicle_name</code>: Name of the vehicle to be created, this should be unique for each vehicle including any exisiting ones defined in the settings.json</li> <li><code>vehicle_type</code>: Type of vehicle, e.g. \"simpleflight\". Currently only SimpleFlight, PhysXCar, ComputerVision are supported, in their respective SimModes.                   Other vehicle types including PX4 and ArduPilot-related aren't supported</li> <li><code>pose</code>: Initial pose of the vehicle</li> <li><code>pawn_path</code>: Vehicle blueprint path, default empty wbich uses the default blueprint for the vehicle type</li> </ul> <p>Returns: <code>bool</code> Whether vehicle was created</p> <p>The usual APIs can be used to control and interact with the vehicle once created, with the <code>vehicle_name</code> parameter. Specifying other settings such as additional cameras, etc. isn't possible currently, a future enhancement could be passing JSON string of settings for the vehicle. It also works with the <code>listVehicles()</code> API described above, so the vehicles spawned would be included in the list.</p> <p>For some examples, check out HelloSpawnedDrones.cpp -</p> <p></p> <p>And runtime_car.py -</p> <p></p>"},{"location":"voxel_grid.html","title":"Voxel Grid","text":"<p><code>AutonomySim</code> provides a feature that constructs ground truth voxel grids of the world directly from <code>Unreal Engine</code>. A voxel grid is a representation of the occupancy of a given world/map, by discretizing into cells of a certain size; and recording a voxel if that particular location is occupied. </p>"},{"location":"voxel_grid.html#algorithm-and-usage","title":"Algorithm and Usage","text":"<p>The logic for constructing the voxel grid is in <code>WorldSimApi.cpp-&gt;createVoxelGrid()</code>. For now, the assumption is that the voxel grid is a cube - and the API call from Python is of the structure:</p> <pre><code>simCreateVoxelGrid(self, position, x, y, z, res, of)\n\nposition (Vector3r): Global position around which voxel grid is centered in m\nx, y, z (float): Size of each voxel grid dimension in m\nres (float): Resolution of voxel grid in m\nof (str): Name of output file to save voxel grid as\n</code></pre> <p>Within <code>createVoxelGrid()</code>, the main Unreal Engine function that returns occupancy is OverlapBlockingTestByChannel.</p> <pre><code>OverlapBlockingTestByChannel(position, rotation, ECollisionChannel, FCollisionShape, params);\n</code></pre> <p>This function is called on the positions of all the 'cells' we wish to discretize the map into, and the returned occupancy result is collected into an array <code>voxel_grid_</code>. The indexing of the cell occupancy values follows the convention of the binvox format. </p> <pre><code>for (float i = 0; i &lt; ncells_x; i++) {\n    for (float k = 0; k &lt; ncells_z; k++) {\n        for (float j = 0; j &lt; ncells_y; j++) {\n            int idx = i + ncells_x * (k + ncells_z * j);\n            FVector position = FVector((i - ncells_x /2) * scale_cm, (j - ncells_y /2) * scale_cm, (k - ncells_z /2) * scale_cm) + position_in_UE_frame;\n            voxel_grid_[idx] = simmode_-&gt;GetWorld()-&gt;OverlapBlockingTestByChannel(position, FQuat::Identity, ECollisionChannel::ECC_Pawn, FCollisionShape::MakeBox(FVector(scale_cm /2)), params);\n        }\n    }\n}\n</code></pre> <p>The occupancy of the map is calculated iteratively over all discretized cells, which can make it an intensive operation depending on the resolution of the cells, and the total size of the area being measured. If the user's map of interest does not change much, it is possible to run the voxel grid operation once on this map, and save the voxel grid and reuse it. For performance, or with dynamic environments, we recommend running the voxel grid generation for a small area around the robot; and subsequently use it for local planning purposes.</p> <p>The voxel grids are stored in the binvox format which can then be converted by the user into an octomap .bt or any other relevant, desired format. Subsequently, these voxel grids/octomaps can be used within mapping/planning. One nifty little utility to visualize a created binvox files is viewvox. Similarly, <code>binvox2bt</code> can convert the binvox to an octomap file.</p>"},{"location":"voxel_grid.html#example-in-blocks","title":"Example in Blocks","text":""},{"location":"voxel_grid.html#converion-to-octomap-format-visualized-in-rviz","title":"Converion to Octomap format (visualized in rviz)","text":"<p>As an example, a voxel grid can be constructed as follows, once the Blocks environment is up and running:</p> <pre><code>import AutonomySim\nc = AutonomySim.VehicleClient()\ncenter = AutonomySim.Vector3r(0, 0, 0)\noutput_path = os.path.join(os.getcwd(), \"map.binvox\")\nc.simCreateVoxelGrid(center, 100, 100, 100, 0.5, output_path)\n</code></pre> <p>And visualized through <code>viewvox map.binvox</code>.</p>"}]}